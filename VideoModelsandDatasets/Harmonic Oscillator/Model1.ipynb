{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.integrate import solve_ivp\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonic Oscillator Dynamics\n",
    "def harmonic_oscillator(t, y, k, m):\n",
    "    x, v = y\n",
    "    dydt = [v, -(k / m) * x]\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Workflow\n",
    "n_samples = 100\n",
    "output_dir = \"oscillator_dataset\"\n",
    "dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset\n",
    "def generate_dataset(n_samples, dt, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    dataset = []\n",
    "\n",
    "    for i in tqdm(range(n_samples), desc=\"Generating dataset\"):\n",
    "        k = np.random.uniform(1.0, 10.0)  # Spring constant\n",
    "        m = np.random.uniform(0.5, 5.0)   # Mass\n",
    "        y0 = np.random.uniform(-2.0, 2.0, 2)  # Initial position and velocity\n",
    "        t_span = (0, 30)  # Extended time span for 3x movement\n",
    "        t_eval = np.linspace(t_span[0], t_span[1], int(30 / dt))  # More frames\n",
    "\n",
    "        sol = solve_ivp(harmonic_oscillator, t_span, y0, t_eval=t_eval, args=(k, m), method='RK45')\n",
    "        data = sol.y.T  # Positions and velocities over time\n",
    "\n",
    "        gif_path = os.path.join(output_dir, f\"oscillator_{i}.gif\")\n",
    "        generate_gif(data[:, 0], gif_path)\n",
    "\n",
    "        dataset.append((data, k, m, gif_path))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GIFs\n",
    "def generate_gif(positions, save_path):\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    line, = ax.plot([], [], 'o-', lw=2)\n",
    "\n",
    "    def update(frame):\n",
    "        line.set_data([positions[frame], 0], [0, 0])\n",
    "        return line,\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(positions), blit=True, interval=50)\n",
    "    ani.save(save_path, fps=20, writer='pillow')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 100/100 [2:40:10<00:00, 96.10s/it]  \n"
     ]
    }
   ],
   "source": [
    "dataset = generate_dataset(n_samples, dt, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class HarmonicOscillatorDataset(Dataset):\n",
    "    def __init__(self, dataset, seq_len, pred_len):\n",
    "        self.dataset = dataset\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, k, m, gif_path = self.dataset[idx]\n",
    "        positions = data[:, 0]\n",
    "\n",
    "        input_seq = positions[:self.seq_len]\n",
    "        target_seq = positions[self.seq_len:self.seq_len + self.pred_len]\n",
    "\n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32), gif_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 270\n",
    "pred_len = int(0.2 * seq_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and dataloaders\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = HarmonicOscillatorDataset(train_data, seq_len, pred_len)\n",
    "val_dataset = HarmonicOscillatorDataset(val_data, seq_len, pred_len)\n",
    "test_dataset = HarmonicOscillatorDataset(test_data, seq_len, pred_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_dim = 1\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class LSTMOscillatorPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, pred_len):\n",
    "        super(LSTMOscillatorPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        outputs, _ = self.lstm(x)\n",
    "        # Predict a sequence of length `pred_len`\n",
    "        predictions = []\n",
    "        for _ in range(self.pred_len):\n",
    "            last_output = outputs[:, -1, :].unsqueeze(1)  # Get the last hidden state\n",
    "            next_output = self.fc(last_output)\n",
    "            predictions.append(next_output.squeeze(1))\n",
    "            outputs = torch.cat([outputs, last_output], dim=1)  # Append prediction to sequence\n",
    "        return torch.stack(predictions, dim=1)  # Return as sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs = inputs.unsqueeze(-1)  # Add feature dimension\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Compare sequences\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, _ in val_loader:\n",
    "                inputs = inputs.unsqueeze(-1)  # Add feature dimension\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.unsqueeze(1))  # Compare sequences\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction GIFs\n",
    "def generate_prediction_gif(model, test_loader, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inputs, targets, gif_path) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs).squeeze(0).numpy()\n",
    "\n",
    "        true_positions = targets.numpy()\n",
    "        input_positions = inputs.squeeze(0).numpy()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 2))\n",
    "        ax.set_xlim(-2.5, 2.5)\n",
    "        ax.set_ylim(-0.5, 0.5)\n",
    "        true_line, = ax.plot([], [], 'o-', lw=2, label='True Motion')\n",
    "        pred_line, = ax.plot([], [], 'o-', lw=2, linestyle='--', label='Predicted Motion')\n",
    "\n",
    "        def update(frame):\n",
    "            if frame < len(input_positions):\n",
    "                true_line.set_data([input_positions[frame], 0], [0, 0])\n",
    "            else:\n",
    "                idx = frame - len(input_positions)\n",
    "                true_line.set_data([true_positions[idx], 0], [0, 0])\n",
    "                pred_line.set_data([predictions[idx], 0], [0, 0])\n",
    "            return true_line, pred_line\n",
    "\n",
    "        ani = FuncAnimation(fig, update, frames=len(input_positions) + len(true_positions), blit=True, interval=50)\n",
    "        ani.save(os.path.join(output_dir, f\"prediction_{i}.gif\"), fps=20, writer='imagemagick')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = LSTMOscillatorPredictor(input_dim, hidden_dim, output_dim, num_layers, pred_len)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/3 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 1, 54])) that is different to the input size (torch.Size([32, 54, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/20:  67%|██████▋   | 2/3 [00:00<00:00,  3.29it/s]C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([16, 1, 54])) that is different to the input size (torch.Size([16, 54, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/20: 100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([10, 1, 54])) that is different to the input size (torch.Size([10, 54, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.4155, Val Loss: 1.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 3/3 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.3020, Val Loss: 1.5695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 3/3 [00:00<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.1138, Val Loss: 1.4572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 3/3 [00:00<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.1590, Val Loss: 1.3085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 3/3 [00:00<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.0518, Val Loss: 1.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 3/3 [00:00<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.8521, Val Loss: 0.7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 3/3 [00:00<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.6418, Val Loss: 0.4891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 3/3 [00:00<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.5457, Val Loss: 0.3982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 3/3 [00:00<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.4896, Val Loss: 0.2668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 3/3 [00:00<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.4460, Val Loss: 0.2420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 3/3 [00:00<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.4586, Val Loss: 0.2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 3/3 [00:00<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.3790, Val Loss: 0.2919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 3/3 [00:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.3392, Val Loss: 0.3169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 3/3 [00:00<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 0.3174, Val Loss: 0.3405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 3/3 [00:00<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 0.3180, Val Loss: 0.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 0.3139, Val Loss: 0.3364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 3/3 [00:00<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 0.2992, Val Loss: 0.3358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 3/3 [00:00<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 0.2914, Val Loss: 0.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 3/3 [00:00<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 0.2831, Val Loss: 0.3303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 3/3 [00:00<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 0.2910, Val Loss: 0.3172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 1, got 270",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_prediction_gif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted_gifs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mgenerate_prediction_gif\u001b[1;34m(model, test_loader, output_dir)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets, gif_path) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     10\u001b[0m     true_positions \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m     input_positions \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mLSTMOscillatorPredictor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Predict a sequence of length `pred_len`\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:1100\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m   1094\u001b[0m         max_batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m   1098\u001b[0m     )\n\u001b[0;32m   1099\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[1;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[0;32m    999\u001b[0m ):\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1010\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 1, got 270"
     ]
    }
   ],
   "source": [
    "generate_prediction_gif(model, test_loader, output_dir=\"predicted_gifs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_harmonic_oscillator_predictions(example, model, pred_len, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize harmonic oscillator predictions vs. ground truth for the prediction part.\n",
    "\n",
    "    Args:\n",
    "        example: A single data point from the dataset (inputs, targets, gif_path).\n",
    "        model: Trained PyTorch model for predictions.\n",
    "        pred_len: Expected length of the predicted sequence.\n",
    "        save_path: Path to save the animation as a GIF (optional).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    inputs, targets, _ = example\n",
    "    inputs = inputs.unsqueeze(0).unsqueeze(-1)  # Add batch and feature dimensions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(inputs).squeeze(0).squeeze(-1).numpy()\n",
    "\n",
    "    # Extract true and predicted positions for the prediction phase\n",
    "    true_positions = targets.numpy()[:pred_len]\n",
    "    pred_positions = predictions[:pred_len]\n",
    "\n",
    "    # Debugging: Check the values\n",
    "    print(f\"True Positions: {true_positions}\")\n",
    "    print(f\"Predicted Positions: {pred_positions}\")\n",
    "\n",
    "    # Ensure lengths match\n",
    "    actual_pred_len = min(len(true_positions), len(pred_positions))\n",
    "    true_positions = true_positions[:actual_pred_len]\n",
    "    pred_positions = pred_positions[:actual_pred_len]\n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Create initial plots\n",
    "    true_line, = ax.plot([], [], 'o-', lw=2, label='Ground Truth', color='blue')\n",
    "    pred_line, = ax.plot([], [], 'o--', lw=2, label='Prediction', color='orange')\n",
    "    ax.legend()\n",
    "\n",
    "    def update(frame):\n",
    "        # Update the ground truth and prediction lines\n",
    "        true_line.set_data([true_positions[frame], 0], [0, 0])\n",
    "        pred_line.set_data([pred_positions[frame], 0], [0, 0])\n",
    "        return true_line, pred_line\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig, update, frames=actual_pred_len, blit=True, interval=50\n",
    "    )\n",
    "\n",
    "    if save_path:\n",
    "        ani.save(save_path, fps=20, writer='pillow')\n",
    "        print(f\"Animation saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positions: [1.681392  1.6973087 1.7130045 1.7284772 1.7437248 1.758745  1.7735355\n",
      " 1.7880944 1.8024197 1.8165091 1.830361  1.8439733 1.8573439 1.8704714\n",
      " 1.8833536 1.8959888 1.9083755 1.9205117 1.932396  1.9440267 1.9554023\n",
      " 1.966521  1.9773816 1.9879826 1.9983226 2.0084002 2.018214  2.0277627\n",
      " 2.0370452 2.0460603 2.054807  2.0632834 2.0714893 2.0794234 2.0870845\n",
      " 2.0944717 2.1015842 2.108421  2.1149814 2.1212645 2.1272693 2.1329956\n",
      " 2.1384423 2.143609  2.148495  2.1530995 2.1574225 2.1614633 2.1652212\n",
      " 2.168696  2.1718874 2.1747952 2.1774187 2.179758 ]\n",
      "Predicted Positions: [1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042\n",
      " 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042\n",
      " 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042\n",
      " 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042\n",
      " 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042\n",
      " 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042\n",
      " 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042\n",
      " 1.1603042 1.1603042 1.1603042 1.1603042 1.1603042]\n",
      "Animation saved at harmonic_oscillator_prediction_vs_ground_truth.gif\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize predictions for one example\n",
    "example = test_dataset[0]\n",
    "visualize_harmonic_oscillator_predictions(\n",
    "    example, model, pred_len, save_path=\"harmonic_oscillator_prediction_vs_ground_truth.gif\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
