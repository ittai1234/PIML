{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "G = 9.81  # Gravity (m/s^2)\n",
    "BOUNCE_COEFF = 0.9  # Coefficient of restitution (energy retention on bounce)\n",
    "dt = 0.01\n",
    "mass = 1.0\n",
    "radius = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_balls = 100\n",
    "seq_len = 160\n",
    "pred_len = 40\n",
    "box_size = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 16\n",
    "hidden_dim = 64\n",
    "sparse_dim = 8\n",
    "fourier_modes = 16\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "alpha, beta, gamma = 0.1, 0.1, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collision Handling\n",
    "def handle_collisions(state, radius, box_size, bounce_coeff):\n",
    "    \"\"\"\n",
    "    Correct position and velocity of the ball when it collides with a boundary.\n",
    "    \"\"\"\n",
    "    x, vx, y, vy = state\n",
    "\n",
    "    # Check and handle collisions with the left and right walls\n",
    "    if x - radius < -box_size:  # Left wall\n",
    "        x = -box_size + radius\n",
    "        vx = -bounce_coeff * vx  # Reverse x velocity\n",
    "    elif x + radius > box_size:  # Right wall\n",
    "        x = box_size - radius\n",
    "        vx = -bounce_coeff * vx  # Reverse x velocity\n",
    "\n",
    "    # Check and handle collisions with the floor and ceiling\n",
    "    if y - radius < -box_size:  # Floor\n",
    "        y = -box_size + radius\n",
    "        vy = -bounce_coeff * vy  # Reverse y velocity\n",
    "    elif y + radius > box_size:  # Ceiling\n",
    "        y = box_size - radius\n",
    "        vy = -bounce_coeff * vy  # Reverse y velocity\n",
    "\n",
    "    return np.array([x, vx, y, vy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle Dynamics\n",
    "def particle_dynamics_with_collisions(t, y, mass, radius, box_size, drag_coeff, bounce_coeff):\n",
    "    \"\"\"\n",
    "    Simulate the dynamics of a ball with gravity, air resistance, and proper collision handling.\n",
    "    \"\"\"\n",
    "    # Unpack state variables\n",
    "    x, vx, y_pos, vy = y\n",
    "    dydt = np.zeros_like(y)\n",
    "\n",
    "    # Update positions\n",
    "    dydt[0] = vx  # dx/dt = vx\n",
    "    dydt[2] = vy  # dy/dt = vy\n",
    "\n",
    "    # Forces: Gravity and Air Resistance\n",
    "    speed = np.sqrt(vx**2 + vy**2)  # Compute speed for drag calculation\n",
    "    drag_force_x = -drag_coeff * speed * vx  # Drag force in x-direction\n",
    "    drag_force_y = -drag_coeff * speed * vy  # Drag force in y-direction\n",
    "\n",
    "    dydt[1] = drag_force_x / mass  # dvx/dt = Drag force in x / mass\n",
    "    dydt[3] = (-G + drag_force_y / mass)  # dvy/dt = Gravity + drag force in y / mass\n",
    "\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GIFs\n",
    "def generate_ball_gif(trajectory, radius, box_size, save_path):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-box_size, box_size)\n",
    "    ax.set_ylim(-box_size, box_size)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ball = plt.Circle((trajectory[0, 0], trajectory[0, 2]), radius, fc='blue')\n",
    "    ax.add_patch(ball)\n",
    "\n",
    "    def update(frame):\n",
    "        ball.set_center((trajectory[frame, 0], trajectory[frame, 2]))\n",
    "        return ball,\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(trajectory), blit=True, interval=50)\n",
    "    ani.save(save_path, fps=20, writer='imagemagick')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Generation\n",
    "def generate_ball_dataset(n_balls, dt, seq_len, pred_len, box_size=5.0, output_dir=\"Ball Dataset\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trajectories = []\n",
    "    for i in tqdm(range(n_balls), desc=\"Generating dataset\"):\n",
    "        mass = np.random.uniform(0.5, 5.0)\n",
    "        radius = np.random.uniform(0.05, 0.2)\n",
    "        drag_coeff = 0.1\n",
    "        bounce_coeff = 0.9\n",
    "        init_pos = np.random.uniform(-box_size + radius, box_size - radius, 2)\n",
    "        init_vel = np.random.uniform(-5.0, 5.0, 2)\n",
    "        y0 = np.concatenate([init_pos, init_vel])\n",
    "        t_eval = np.linspace(0, (seq_len + pred_len) * dt, int(seq_len + pred_len))\n",
    "        trajectory = []\n",
    "        state = y0\n",
    "        for t in t_eval:\n",
    "            sol = solve_ivp(\n",
    "                particle_dynamics_with_collisions,\n",
    "                (t, t + dt),\n",
    "                state,\n",
    "                args=(mass, radius, box_size, drag_coeff, bounce_coeff),\n",
    "                method=\"RK45\",\n",
    "                t_eval=[t + dt]\n",
    "            )\n",
    "            if sol.y.size == 0:\n",
    "                break\n",
    "            state = sol.y.flatten()\n",
    "            state = handle_collisions(state, radius, box_size, bounce_coeff)\n",
    "            trajectory.append(state)\n",
    "\n",
    "        trajectories.append(np.array(trajectory))\n",
    "        gif_path = os.path.join(output_dir, f\"ball_{i}.gif\")\n",
    "        generate_ball_gif(np.array(trajectory), radius, box_size, gif_path)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   0%|          | 0/100 [00:00<?, ?it/s]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   1%|          | 1/100 [00:08<13:32,  8.21s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   2%|▏         | 2/100 [00:16<13:08,  8.05s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   3%|▎         | 3/100 [00:23<12:49,  7.93s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   4%|▍         | 4/100 [00:32<12:51,  8.03s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   5%|▌         | 5/100 [00:40<12:42,  8.03s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   6%|▌         | 6/100 [00:48<12:36,  8.05s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   7%|▋         | 7/100 [00:56<12:31,  8.08s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   8%|▊         | 8/100 [01:03<12:09,  7.93s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   9%|▉         | 9/100 [01:12<12:07,  8.00s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  10%|█         | 10/100 [01:20<11:56,  7.96s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  11%|█         | 11/100 [01:28<12:06,  8.16s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  12%|█▏        | 12/100 [01:36<11:53,  8.10s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  13%|█▎        | 13/100 [01:44<11:50,  8.17s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  14%|█▍        | 14/100 [01:53<11:50,  8.27s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  15%|█▌        | 15/100 [02:01<11:39,  8.22s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  16%|█▌        | 16/100 [02:10<11:38,  8.31s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  17%|█▋        | 17/100 [02:18<11:32,  8.34s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  18%|█▊        | 18/100 [02:27<11:29,  8.41s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  19%|█▉        | 19/100 [02:35<11:18,  8.38s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  20%|██        | 20/100 [02:43<10:59,  8.25s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  21%|██        | 21/100 [02:51<10:59,  8.34s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  22%|██▏       | 22/100 [03:00<11:02,  8.50s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  23%|██▎       | 23/100 [03:09<11:10,  8.70s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  24%|██▍       | 24/100 [03:17<10:45,  8.50s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  25%|██▌       | 25/100 [03:25<10:26,  8.35s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  26%|██▌       | 26/100 [03:33<10:02,  8.15s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  27%|██▋       | 27/100 [03:41<09:47,  8.04s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  28%|██▊       | 28/100 [03:49<09:31,  7.94s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  29%|██▉       | 29/100 [03:56<09:21,  7.91s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  30%|███       | 30/100 [04:04<09:12,  7.89s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  31%|███       | 31/100 [04:12<08:57,  7.79s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  32%|███▏      | 32/100 [04:20<08:52,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  33%|███▎      | 33/100 [04:28<08:44,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  34%|███▍      | 34/100 [04:36<08:41,  7.90s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  35%|███▌      | 35/100 [04:43<08:29,  7.85s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  36%|███▌      | 36/100 [04:51<08:17,  7.77s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  37%|███▋      | 37/100 [04:59<08:06,  7.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  38%|███▊      | 38/100 [05:07<08:07,  7.86s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  39%|███▉      | 39/100 [05:15<07:58,  7.84s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  40%|████      | 40/100 [05:22<07:50,  7.85s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  41%|████      | 41/100 [05:30<07:41,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  42%|████▏     | 42/100 [05:38<07:34,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  43%|████▎     | 43/100 [05:46<07:21,  7.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  44%|████▍     | 44/100 [05:53<07:13,  7.74s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  45%|████▌     | 45/100 [06:01<07:06,  7.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  46%|████▌     | 46/100 [06:09<06:56,  7.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  47%|████▋     | 47/100 [06:16<06:48,  7.71s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  48%|████▊     | 48/100 [06:24<06:38,  7.67s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  49%|████▉     | 49/100 [06:32<06:32,  7.69s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  50%|█████     | 50/100 [06:39<06:23,  7.68s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  51%|█████     | 51/100 [06:47<06:17,  7.70s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  52%|█████▏    | 52/100 [06:55<06:11,  7.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  53%|█████▎    | 53/100 [07:03<06:11,  7.89s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  54%|█████▍    | 54/100 [07:11<06:00,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  55%|█████▌    | 55/100 [07:19<05:53,  7.85s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  56%|█████▌    | 56/100 [07:27<05:43,  7.82s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  57%|█████▋    | 57/100 [07:34<05:33,  7.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  58%|█████▊    | 58/100 [07:42<05:23,  7.70s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  59%|█████▉    | 59/100 [07:50<05:18,  7.78s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  60%|██████    | 60/100 [07:57<05:10,  7.77s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  61%|██████    | 61/100 [08:05<05:02,  7.77s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  62%|██████▏   | 62/100 [08:13<04:56,  7.81s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  63%|██████▎   | 63/100 [08:21<04:49,  7.82s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  64%|██████▍   | 64/100 [08:29<04:41,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  65%|██████▌   | 65/100 [08:36<04:32,  7.79s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  66%|██████▌   | 66/100 [08:44<04:26,  7.84s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  67%|██████▋   | 67/100 [08:52<04:19,  7.87s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  68%|██████▊   | 68/100 [09:01<04:18,  8.07s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  69%|██████▉   | 69/100 [09:09<04:08,  8.00s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  70%|███████   | 70/100 [09:17<03:58,  7.95s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  71%|███████   | 71/100 [09:25<03:51,  7.99s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  72%|███████▏  | 72/100 [09:32<03:41,  7.90s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  73%|███████▎  | 73/100 [09:40<03:31,  7.85s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  74%|███████▍  | 74/100 [09:48<03:23,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  75%|███████▌  | 75/100 [09:56<03:15,  7.83s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  76%|███████▌  | 76/100 [10:03<03:06,  7.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  77%|███████▋  | 77/100 [10:11<02:58,  7.78s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  78%|███████▊  | 78/100 [10:19<02:54,  7.93s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  79%|███████▉  | 79/100 [10:27<02:45,  7.89s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  80%|████████  | 80/100 [10:35<02:35,  7.77s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  81%|████████  | 81/100 [10:42<02:26,  7.71s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  82%|████████▏ | 82/100 [10:50<02:19,  7.73s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  83%|████████▎ | 83/100 [10:58<02:12,  7.77s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  84%|████████▍ | 84/100 [11:06<02:03,  7.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  85%|████████▌ | 85/100 [11:13<01:55,  7.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  86%|████████▌ | 86/100 [11:21<01:47,  7.71s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  87%|████████▋ | 87/100 [11:29<01:39,  7.68s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  88%|████████▊ | 88/100 [11:36<01:32,  7.68s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  89%|████████▉ | 89/100 [11:44<01:24,  7.65s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  90%|█████████ | 90/100 [11:52<01:17,  7.71s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  91%|█████████ | 91/100 [11:59<01:08,  7.65s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  92%|█████████▏| 92/100 [12:07<01:00,  7.61s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  93%|█████████▎| 93/100 [12:14<00:53,  7.66s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  94%|█████████▍| 94/100 [12:22<00:46,  7.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  95%|█████████▌| 95/100 [12:30<00:38,  7.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  96%|█████████▌| 96/100 [12:38<00:31,  7.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  97%|█████████▋| 97/100 [12:46<00:23,  7.73s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  98%|█████████▊| 98/100 [12:53<00:15,  7.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  99%|█████████▉| 99/100 [13:01<00:07,  7.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset: 100%|██████████| 100/100 [13:09<00:00,  7.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "trajectories = generate_ball_dataset(n_balls, dt, seq_len + pred_len, box_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class BallTrajectoryDataset(Dataset):\n",
    "    def __init__(self, trajectories, seq_len, pred_len):\n",
    "        self.trajectories = trajectories\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        input_seq = trajectory[:self.seq_len]\n",
    "        target_seq = trajectory[self.seq_len:self.seq_len + self.pred_len]\n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, targets, folder, radius, box_size, mode=\"test\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    real_folder = os.path.join(folder, \"real_movement\")\n",
    "    predicted_folder = os.path.join(folder, \"predicted_movement\")\n",
    "    os.makedirs(real_folder, exist_ok=True)\n",
    "    os.makedirs(predicted_folder, exist_ok=True)\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        save_trajectory_gif(targets[idx][-len(targets[idx]) // 5:], radius, box_size, os.path.join(real_folder, f\"{mode}_real_{idx}.gif\"), \"True Trajectory\")\n",
    "        save_trajectory_gif(predictions[idx][-len(predictions[idx]) // 5:], radius, box_size, os.path.join(predicted_folder, f\"{mode}_predicted_{idx}.gif\"), \"Predicted Trajectory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Trajectory GIFs\n",
    "def save_trajectory_gif(trajectory, radius, box_size, save_path, title=\"Trajectory\"):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-box_size, box_size)\n",
    "    ax.set_ylim(-box_size, box_size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ball = plt.Circle((trajectory[0, 0], trajectory[0, 2]), radius, fc='blue')\n",
    "    ax.add_patch(ball)\n",
    "\n",
    "    def update(frame):\n",
    "        ball.set_center((trajectory[frame, 0], trajectory[frame, 2]))\n",
    "        return ball,\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(trajectory), blit=True, interval=50)\n",
    "    ani.save(save_path, fps=20, writer='imagemagick')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Best and Worst Predictions\n",
    "def save_best_worst_predictions(predictions, targets, losses, folder, radius, box_size, mode):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    real_folder = os.path.join(folder, \"real_movement\")\n",
    "    predicted_folder = os.path.join(folder, \"predicted_movement\")\n",
    "    os.makedirs(real_folder, exist_ok=True)\n",
    "    os.makedirs(predicted_folder, exist_ok=True)\n",
    "\n",
    "    sorted_indices = np.argsort(losses)\n",
    "    best_indices = sorted_indices[:5]\n",
    "    worst_indices = sorted_indices[-5:]\n",
    "\n",
    "    for idx in best_indices:\n",
    "        save_trajectory_gif(targets[idx][-len(targets[idx]) // 5:], radius, box_size, os.path.join(real_folder, f\"{mode}_best_real_{idx}.gif\"), \"True Trajectory (Best)\")\n",
    "        save_trajectory_gif(predictions[idx][-len(predictions[idx]) // 5:], radius, box_size, os.path.join(predicted_folder, f\"{mode}_best_predicted_{idx}.gif\"), \"Predicted Trajectory (Best)\")\n",
    "\n",
    "    for idx in worst_indices:\n",
    "        save_trajectory_gif(targets[idx][-len(targets[idx]) // 5:], radius, box_size, os.path.join(real_folder, f\"{mode}_worst_real_{idx}.gif\"), \"True Trajectory (Worst)\")\n",
    "        save_trajectory_gif(predictions[idx][-len(predictions[idx]) // 5:], radius, box_size, os.path.join(predicted_folder, f\"{mode}_worst_predicted_{idx}.gif\"), \"Predicted Trajectory (Worst)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save All Test Predictions\n",
    "def save_all_test_predictions(predictions, targets, folder, radius, box_size):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    real_folder = os.path.join(folder, \"real_movement\")\n",
    "    predicted_folder = os.path.join(folder, \"predicted_movement\")\n",
    "    os.makedirs(real_folder, exist_ok=True)\n",
    "    os.makedirs(predicted_folder, exist_ok=True)\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        save_trajectory_gif(targets[idx][-len(targets[idx]) // 5:], radius, box_size, os.path.join(real_folder, f\"test_real_{idx}.gif\"), \"True Trajectory\")\n",
    "        save_trajectory_gif(predictions[idx][-len(predictions[idx]) // 5:], radius, box_size, os.path.join(predicted_folder, f\"test_predicted_{idx}.gif\"), \"Predicted Trajectory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Model\n",
    "class SINDyKoopmanFNO(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim, sparse_dim, fourier_modes, pred_len):\n",
    "        super(SINDyKoopmanFNO, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "        )\n",
    "        self.koopman = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        self.sindy_dynamics = nn.Linear(latent_dim, sparse_dim, bias=False)\n",
    "        self.fourier_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim, fourier_modes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fourier_modes, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)  # z: [batch_size, seq_len, latent_dim]\n",
    "        predictions = []\n",
    "        z_current = z[:, 0, :]\n",
    "        z_next_list = []\n",
    "        dzdt_list = []\n",
    "\n",
    "        for _ in range(self.pred_len):\n",
    "            z_next = self.koopman(z_current)  # Koopman dynamics\n",
    "            dzdt = self.sindy_dynamics(z_next)  # Sparse dynamics\n",
    "            z_fourier = self.fourier_layer(z_next)  # Fourier transform\n",
    "            z_combined = z_next + z_fourier  # Combine latent dynamics\n",
    "            x_reconstructed = self.decoder(z_combined)  # Decode back to input\n",
    "            predictions.append(x_reconstructed.unsqueeze(1))\n",
    "            z_next_list.append(z_next.unsqueeze(1))\n",
    "            dzdt_list.append(dzdt.unsqueeze(1))\n",
    "            z_current = z_next\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=1)  # [batch_size, pred_len, input_dim]\n",
    "        z_next_all = torch.cat(z_next_list, dim=1)  # [batch_size, pred_len, latent_dim]\n",
    "        dzdt_all = torch.cat(dzdt_list, dim=1)  # [batch_size, pred_len, sparse_dim]\n",
    "        return predictions, z_next_all, dzdt_all, z[:, :self.pred_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def compute_energy(state, mass, g):\n",
    "    vx, vy = state[:, :, 1], state[:, :, 3]\n",
    "    y = state[:, :, 2]\n",
    "    kinetic_energy = 0.5 * mass * (vx**2 + vy**2)\n",
    "    potential_energy = mass * g * y\n",
    "    return kinetic_energy + potential_energy\n",
    "\n",
    "def energy_loss(predicted_state, true_state, mass, g):\n",
    "    predicted_energy = compute_energy(predicted_state, mass, g)\n",
    "    true_energy = compute_energy(true_state, mass, g)\n",
    "    return torch.mean((predicted_energy - true_energy)**2)\n",
    "\n",
    "def combined_loss(x_reconstructed, x_target, z_next, dzdt, z_pred, mass, g, alpha=0.1, beta=0.1, gamma=0.1):\n",
    "    reconstruction_loss = torch.mean((x_reconstructed - x_target) ** 2)\n",
    "    koopman_loss = torch.mean((z_next - z_pred) ** 2)\n",
    "    \n",
    "    # Map z_pred to sparse_dim for comparison with dzdt\n",
    "    z_pred_sparse = dzdt.new_zeros(dzdt.shape)  # Match shape\n",
    "    for i in range(dzdt.size(1)):  # Loop over sequence length\n",
    "        z_pred_sparse[:, i, :] = dzdt[:, i, :]  # Direct mapping for simplicity\n",
    "    \n",
    "    sparse_loss = torch.mean((dzdt - z_pred_sparse) ** 2)\n",
    "    energy_loss_value = energy_loss(x_reconstructed, x_target, mass, g)\n",
    "    return reconstruction_loss + alpha * energy_loss_value + beta * koopman_loss + gamma * sparse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Splitting\n",
    "def split_dataset(trajectories, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    total = train_ratio + val_ratio + test_ratio\n",
    "    train_ratio /= total\n",
    "    val_ratio /= total\n",
    "    test_ratio /= total\n",
    "\n",
    "    train_val_data, test_data = train_test_split(trajectories, test_size=test_ratio, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=val_ratio / (train_ratio + val_ratio), random_state=42)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(trajectories, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)\n",
    "train_dataset = BallTrajectoryDataset(train_set, seq_len, pred_len)\n",
    "val_dataset = BallTrajectoryDataset(val_set, seq_len, pred_len)\n",
    "test_dataset = BallTrajectoryDataset(test_set, seq_len, pred_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation\n",
    "def train_and_save_predictions(model, train_loader, val_loader, optimizer, mass, g, num_epochs, alpha, beta, gamma, radius, box_size):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss, total_train_acc = 0.0, 0.0\n",
    "        total_val_loss, total_val_acc = 0.0, 0.0\n",
    "        train_predictions, train_targets, train_losses = [], [], []\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_reconstructed, z_next, dzdt, z = model(inputs)\n",
    "            z_pred = model.koopman(z)\n",
    "            loss = combined_loss(x_reconstructed, targets, z_next, dzdt, z_pred, mass, g, alpha, beta, gamma)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            train_acc = 1 - (torch.norm(x_reconstructed - targets) / torch.norm(targets))\n",
    "            total_train_acc += train_acc.item()\n",
    "\n",
    "            train_predictions.append(x_reconstructed.detach().cpu().numpy())\n",
    "            train_targets.append(targets.detach().cpu().numpy())\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_predictions, val_targets, val_losses = [], [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                x_reconstructed, z_next, dzdt, z = model(inputs)\n",
    "                z_pred = model.koopman(z)\n",
    "                loss = combined_loss(x_reconstructed, targets, z_next, dzdt, z_pred, mass, g, alpha, beta, gamma)\n",
    "                total_val_loss += loss.item()\n",
    "                val_acc = 1 - (torch.norm(x_reconstructed - targets) / torch.norm(targets))\n",
    "                total_val_acc += val_acc.item()\n",
    "\n",
    "                val_predictions.append(x_reconstructed.detach().cpu().numpy())\n",
    "                val_targets.append(targets.detach().cpu().numpy())\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_train_loss / len(train_loader):.4f}, Train Accuracy: {total_train_acc / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {total_val_loss / len(val_loader):.4f}, Val Accuracy: {total_val_acc / len(val_loader):.4f}\")\n",
    "\n",
    "        if epoch == num_epochs - 1:\n",
    "            save_best_worst_predictions(np.concatenate(train_predictions), np.concatenate(train_targets), train_losses, \"training_predictions\", radius, box_size, \"train\")\n",
    "            save_best_worst_predictions(np.concatenate(val_predictions), np.concatenate(val_targets), val_losses, \"val_predictions\", radius, box_size, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = SINDyKoopmanFNO(input_dim=4, latent_dim=latent_dim, hidden_dim=hidden_dim, sparse_dim=sparse_dim, fourier_modes=fourier_modes, pred_len=pred_len)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 7.3872, Train Accuracy: 0.2195, Val Loss: 10.6601, Val Accuracy: 0.2075\n",
      "Epoch 2/1000, Train Loss: 6.9944, Train Accuracy: 0.2062, Val Loss: 11.0481, Val Accuracy: 0.2119\n",
      "Epoch 3/1000, Train Loss: 7.5607, Train Accuracy: 0.1995, Val Loss: 10.5526, Val Accuracy: 0.2106\n",
      "Epoch 4/1000, Train Loss: 9.1961, Train Accuracy: 0.1916, Val Loss: 11.1055, Val Accuracy: 0.2152\n",
      "Epoch 5/1000, Train Loss: 8.9021, Train Accuracy: 0.1906, Val Loss: 10.5433, Val Accuracy: 0.2108\n",
      "Epoch 6/1000, Train Loss: 7.2979, Train Accuracy: 0.2033, Val Loss: 10.2374, Val Accuracy: 0.2133\n",
      "Epoch 7/1000, Train Loss: 7.5197, Train Accuracy: 0.2100, Val Loss: 10.3335, Val Accuracy: 0.2140\n",
      "Epoch 8/1000, Train Loss: 7.5334, Train Accuracy: 0.2009, Val Loss: 10.2805, Val Accuracy: 0.2139\n",
      "Epoch 9/1000, Train Loss: 7.4942, Train Accuracy: 0.2025, Val Loss: 10.6869, Val Accuracy: 0.2166\n",
      "Epoch 10/1000, Train Loss: 8.0718, Train Accuracy: 0.1947, Val Loss: 10.4337, Val Accuracy: 0.2138\n",
      "Epoch 11/1000, Train Loss: 7.8093, Train Accuracy: 0.2003, Val Loss: 10.2513, Val Accuracy: 0.2179\n",
      "Epoch 12/1000, Train Loss: 7.3637, Train Accuracy: 0.1894, Val Loss: 10.2564, Val Accuracy: 0.2182\n",
      "Epoch 13/1000, Train Loss: 6.9687, Train Accuracy: 0.2246, Val Loss: 10.1907, Val Accuracy: 0.2172\n",
      "Epoch 14/1000, Train Loss: 7.4378, Train Accuracy: 0.1971, Val Loss: 10.3176, Val Accuracy: 0.2175\n",
      "Epoch 15/1000, Train Loss: 7.1677, Train Accuracy: 0.2138, Val Loss: 10.2485, Val Accuracy: 0.2177\n",
      "Epoch 16/1000, Train Loss: 7.2834, Train Accuracy: 0.2123, Val Loss: 10.4228, Val Accuracy: 0.2185\n",
      "Epoch 17/1000, Train Loss: 7.2511, Train Accuracy: 0.2164, Val Loss: 10.3089, Val Accuracy: 0.2162\n",
      "Epoch 18/1000, Train Loss: 6.8873, Train Accuracy: 0.2305, Val Loss: 10.3052, Val Accuracy: 0.2165\n",
      "Epoch 19/1000, Train Loss: 7.5204, Train Accuracy: 0.2044, Val Loss: 10.4457, Val Accuracy: 0.2159\n",
      "Epoch 20/1000, Train Loss: 6.9564, Train Accuracy: 0.2126, Val Loss: 10.3544, Val Accuracy: 0.2154\n",
      "Epoch 21/1000, Train Loss: 7.0599, Train Accuracy: 0.2155, Val Loss: 10.4019, Val Accuracy: 0.2133\n",
      "Epoch 22/1000, Train Loss: 6.8570, Train Accuracy: 0.2125, Val Loss: 10.3354, Val Accuracy: 0.2112\n",
      "Epoch 23/1000, Train Loss: 8.3212, Train Accuracy: 0.1814, Val Loss: 10.5556, Val Accuracy: 0.2145\n",
      "Epoch 24/1000, Train Loss: 7.1611, Train Accuracy: 0.2066, Val Loss: 10.2629, Val Accuracy: 0.2136\n",
      "Epoch 25/1000, Train Loss: 7.3173, Train Accuracy: 0.2089, Val Loss: 10.5167, Val Accuracy: 0.2144\n",
      "Epoch 26/1000, Train Loss: 8.5085, Train Accuracy: 0.1832, Val Loss: 10.4934, Val Accuracy: 0.2119\n",
      "Epoch 27/1000, Train Loss: 7.0069, Train Accuracy: 0.2011, Val Loss: 10.4420, Val Accuracy: 0.2114\n",
      "Epoch 28/1000, Train Loss: 6.9728, Train Accuracy: 0.2446, Val Loss: 10.4126, Val Accuracy: 0.2128\n",
      "Epoch 29/1000, Train Loss: 7.2560, Train Accuracy: 0.2226, Val Loss: 10.3981, Val Accuracy: 0.2142\n",
      "Epoch 30/1000, Train Loss: 6.9358, Train Accuracy: 0.2276, Val Loss: 10.3969, Val Accuracy: 0.2121\n",
      "Epoch 31/1000, Train Loss: 7.6166, Train Accuracy: 0.2022, Val Loss: 10.3595, Val Accuracy: 0.2130\n",
      "Epoch 32/1000, Train Loss: 7.2882, Train Accuracy: 0.2167, Val Loss: 10.3908, Val Accuracy: 0.2128\n",
      "Epoch 33/1000, Train Loss: 7.8864, Train Accuracy: 0.2011, Val Loss: 10.4349, Val Accuracy: 0.2123\n",
      "Epoch 34/1000, Train Loss: 7.3358, Train Accuracy: 0.2076, Val Loss: 10.5910, Val Accuracy: 0.2112\n",
      "Epoch 35/1000, Train Loss: 6.6524, Train Accuracy: 0.2248, Val Loss: 10.3169, Val Accuracy: 0.2092\n",
      "Epoch 36/1000, Train Loss: 8.1624, Train Accuracy: 0.1849, Val Loss: 10.5184, Val Accuracy: 0.2090\n",
      "Epoch 37/1000, Train Loss: 7.2518, Train Accuracy: 0.1993, Val Loss: 10.2904, Val Accuracy: 0.2098\n",
      "Epoch 38/1000, Train Loss: 6.9388, Train Accuracy: 0.2304, Val Loss: 10.4816, Val Accuracy: 0.2127\n",
      "Epoch 39/1000, Train Loss: 6.7979, Train Accuracy: 0.2096, Val Loss: 10.4373, Val Accuracy: 0.2120\n",
      "Epoch 40/1000, Train Loss: 6.9235, Train Accuracy: 0.2080, Val Loss: 10.3953, Val Accuracy: 0.2166\n",
      "Epoch 41/1000, Train Loss: 6.8422, Train Accuracy: 0.1922, Val Loss: 10.4222, Val Accuracy: 0.2180\n",
      "Epoch 42/1000, Train Loss: 7.2680, Train Accuracy: 0.2032, Val Loss: 10.6820, Val Accuracy: 0.2187\n",
      "Epoch 43/1000, Train Loss: 6.8769, Train Accuracy: 0.2119, Val Loss: 10.4040, Val Accuracy: 0.2168\n",
      "Epoch 44/1000, Train Loss: 6.9866, Train Accuracy: 0.2025, Val Loss: 10.4154, Val Accuracy: 0.2150\n",
      "Epoch 45/1000, Train Loss: 7.0147, Train Accuracy: 0.2220, Val Loss: 10.3975, Val Accuracy: 0.2142\n",
      "Epoch 46/1000, Train Loss: 6.9323, Train Accuracy: 0.2212, Val Loss: 10.4641, Val Accuracy: 0.2137\n",
      "Epoch 47/1000, Train Loss: 7.0201, Train Accuracy: 0.2019, Val Loss: 10.4863, Val Accuracy: 0.2100\n",
      "Epoch 48/1000, Train Loss: 6.6007, Train Accuracy: 0.2069, Val Loss: 10.6839, Val Accuracy: 0.2062\n",
      "Epoch 49/1000, Train Loss: 6.8800, Train Accuracy: 0.2228, Val Loss: 10.5241, Val Accuracy: 0.2067\n",
      "Epoch 50/1000, Train Loss: 7.0840, Train Accuracy: 0.2129, Val Loss: 10.6603, Val Accuracy: 0.2092\n",
      "Epoch 51/1000, Train Loss: 8.2236, Train Accuracy: 0.1732, Val Loss: 10.4449, Val Accuracy: 0.2086\n",
      "Epoch 52/1000, Train Loss: 7.1925, Train Accuracy: 0.2185, Val Loss: 10.4582, Val Accuracy: 0.2098\n",
      "Epoch 53/1000, Train Loss: 6.8724, Train Accuracy: 0.2416, Val Loss: 10.4678, Val Accuracy: 0.2098\n",
      "Epoch 54/1000, Train Loss: 6.8975, Train Accuracy: 0.2115, Val Loss: 10.4819, Val Accuracy: 0.2082\n",
      "Epoch 55/1000, Train Loss: 6.7737, Train Accuracy: 0.2082, Val Loss: 11.0166, Val Accuracy: 0.2118\n",
      "Epoch 56/1000, Train Loss: 6.6899, Train Accuracy: 0.2298, Val Loss: 10.3222, Val Accuracy: 0.2137\n",
      "Epoch 57/1000, Train Loss: 6.9020, Train Accuracy: 0.2153, Val Loss: 10.4953, Val Accuracy: 0.2148\n",
      "Epoch 58/1000, Train Loss: 6.7002, Train Accuracy: 0.2318, Val Loss: 10.3688, Val Accuracy: 0.2117\n",
      "Epoch 59/1000, Train Loss: 6.9685, Train Accuracy: 0.2080, Val Loss: 10.5344, Val Accuracy: 0.2135\n",
      "Epoch 60/1000, Train Loss: 7.0247, Train Accuracy: 0.2045, Val Loss: 10.3353, Val Accuracy: 0.2124\n",
      "Epoch 61/1000, Train Loss: 6.6340, Train Accuracy: 0.1968, Val Loss: 10.5630, Val Accuracy: 0.2139\n",
      "Epoch 62/1000, Train Loss: 7.4172, Train Accuracy: 0.2108, Val Loss: 10.3581, Val Accuracy: 0.2167\n",
      "Epoch 63/1000, Train Loss: 6.8319, Train Accuracy: 0.2018, Val Loss: 10.6154, Val Accuracy: 0.2161\n",
      "Epoch 64/1000, Train Loss: 8.2207, Train Accuracy: 0.1960, Val Loss: 10.4794, Val Accuracy: 0.2160\n",
      "Epoch 65/1000, Train Loss: 7.1309, Train Accuracy: 0.2058, Val Loss: 10.5891, Val Accuracy: 0.2108\n",
      "Epoch 66/1000, Train Loss: 6.9893, Train Accuracy: 0.2101, Val Loss: 10.4270, Val Accuracy: 0.2094\n",
      "Epoch 67/1000, Train Loss: 6.7989, Train Accuracy: 0.2146, Val Loss: 10.4579, Val Accuracy: 0.2095\n",
      "Epoch 68/1000, Train Loss: 6.8066, Train Accuracy: 0.2198, Val Loss: 10.4065, Val Accuracy: 0.2081\n",
      "Epoch 69/1000, Train Loss: 6.7238, Train Accuracy: 0.2148, Val Loss: 10.4119, Val Accuracy: 0.2107\n",
      "Epoch 70/1000, Train Loss: 7.7226, Train Accuracy: 0.1907, Val Loss: 10.3978, Val Accuracy: 0.2120\n",
      "Epoch 71/1000, Train Loss: 6.8164, Train Accuracy: 0.2193, Val Loss: 10.5187, Val Accuracy: 0.2142\n",
      "Epoch 72/1000, Train Loss: 6.5454, Train Accuracy: 0.2344, Val Loss: 10.3680, Val Accuracy: 0.2142\n",
      "Epoch 73/1000, Train Loss: 7.0431, Train Accuracy: 0.2152, Val Loss: 10.4023, Val Accuracy: 0.2135\n",
      "Epoch 74/1000, Train Loss: 6.9799, Train Accuracy: 0.1987, Val Loss: 10.4145, Val Accuracy: 0.2140\n",
      "Epoch 75/1000, Train Loss: 7.9935, Train Accuracy: 0.1963, Val Loss: 10.3118, Val Accuracy: 0.2153\n",
      "Epoch 76/1000, Train Loss: 7.0792, Train Accuracy: 0.2075, Val Loss: 10.4501, Val Accuracy: 0.2146\n",
      "Epoch 77/1000, Train Loss: 6.5150, Train Accuracy: 0.2133, Val Loss: 10.5172, Val Accuracy: 0.2143\n",
      "Epoch 78/1000, Train Loss: 6.6353, Train Accuracy: 0.2233, Val Loss: 10.4606, Val Accuracy: 0.2131\n",
      "Epoch 79/1000, Train Loss: 6.9656, Train Accuracy: 0.2106, Val Loss: 10.3656, Val Accuracy: 0.2111\n",
      "Epoch 80/1000, Train Loss: 6.7867, Train Accuracy: 0.2208, Val Loss: 10.4965, Val Accuracy: 0.2079\n",
      "Epoch 81/1000, Train Loss: 9.0057, Train Accuracy: 0.1975, Val Loss: 10.5598, Val Accuracy: 0.2086\n",
      "Epoch 82/1000, Train Loss: 6.8440, Train Accuracy: 0.2256, Val Loss: 10.4585, Val Accuracy: 0.2103\n",
      "Epoch 83/1000, Train Loss: 6.8270, Train Accuracy: 0.2289, Val Loss: 10.4704, Val Accuracy: 0.2099\n",
      "Epoch 84/1000, Train Loss: 6.7047, Train Accuracy: 0.2281, Val Loss: 10.5318, Val Accuracy: 0.2065\n",
      "Epoch 85/1000, Train Loss: 7.9655, Train Accuracy: 0.1974, Val Loss: 11.2327, Val Accuracy: 0.2003\n",
      "Epoch 86/1000, Train Loss: 7.0270, Train Accuracy: 0.2184, Val Loss: 10.7772, Val Accuracy: 0.2007\n",
      "Epoch 87/1000, Train Loss: 6.8152, Train Accuracy: 0.2062, Val Loss: 10.9551, Val Accuracy: 0.2004\n",
      "Epoch 88/1000, Train Loss: 6.6984, Train Accuracy: 0.2415, Val Loss: 10.5612, Val Accuracy: 0.2056\n",
      "Epoch 89/1000, Train Loss: 6.7281, Train Accuracy: 0.2290, Val Loss: 10.9814, Val Accuracy: 0.2022\n",
      "Epoch 90/1000, Train Loss: 7.0664, Train Accuracy: 0.2148, Val Loss: 10.7501, Val Accuracy: 0.2014\n",
      "Epoch 91/1000, Train Loss: 6.8961, Train Accuracy: 0.2282, Val Loss: 10.8155, Val Accuracy: 0.2055\n",
      "Epoch 92/1000, Train Loss: 6.5031, Train Accuracy: 0.2061, Val Loss: 10.5541, Val Accuracy: 0.2071\n",
      "Epoch 93/1000, Train Loss: 6.4924, Train Accuracy: 0.1986, Val Loss: 10.7428, Val Accuracy: 0.2073\n",
      "Epoch 94/1000, Train Loss: 7.0235, Train Accuracy: 0.2113, Val Loss: 10.5844, Val Accuracy: 0.2108\n",
      "Epoch 95/1000, Train Loss: 6.6066, Train Accuracy: 0.2245, Val Loss: 10.5499, Val Accuracy: 0.2126\n",
      "Epoch 96/1000, Train Loss: 6.7850, Train Accuracy: 0.2316, Val Loss: 10.5986, Val Accuracy: 0.2124\n",
      "Epoch 97/1000, Train Loss: 6.3733, Train Accuracy: 0.2378, Val Loss: 10.6443, Val Accuracy: 0.2124\n",
      "Epoch 98/1000, Train Loss: 6.8250, Train Accuracy: 0.2146, Val Loss: 10.5859, Val Accuracy: 0.2085\n",
      "Epoch 99/1000, Train Loss: 6.8249, Train Accuracy: 0.2163, Val Loss: 10.6722, Val Accuracy: 0.2046\n",
      "Epoch 100/1000, Train Loss: 6.7133, Train Accuracy: 0.2107, Val Loss: 10.4842, Val Accuracy: 0.2069\n",
      "Epoch 101/1000, Train Loss: 6.3382, Train Accuracy: 0.2220, Val Loss: 11.2420, Val Accuracy: 0.2058\n",
      "Epoch 102/1000, Train Loss: 7.0578, Train Accuracy: 0.2105, Val Loss: 10.5192, Val Accuracy: 0.2106\n",
      "Epoch 103/1000, Train Loss: 7.6919, Train Accuracy: 0.2031, Val Loss: 10.8363, Val Accuracy: 0.2090\n",
      "Epoch 104/1000, Train Loss: 6.6288, Train Accuracy: 0.2149, Val Loss: 10.6319, Val Accuracy: 0.2126\n",
      "Epoch 105/1000, Train Loss: 6.4194, Train Accuracy: 0.2365, Val Loss: 10.7629, Val Accuracy: 0.2118\n",
      "Epoch 106/1000, Train Loss: 6.2138, Train Accuracy: 0.2503, Val Loss: 10.9283, Val Accuracy: 0.2062\n",
      "Epoch 107/1000, Train Loss: 6.6373, Train Accuracy: 0.2139, Val Loss: 11.1210, Val Accuracy: 0.1935\n",
      "Epoch 108/1000, Train Loss: 6.3296, Train Accuracy: 0.2233, Val Loss: 10.9282, Val Accuracy: 0.1928\n",
      "Epoch 109/1000, Train Loss: 6.6827, Train Accuracy: 0.2192, Val Loss: 11.1777, Val Accuracy: 0.1947\n",
      "Epoch 110/1000, Train Loss: 6.7413, Train Accuracy: 0.2276, Val Loss: 10.7479, Val Accuracy: 0.2048\n",
      "Epoch 111/1000, Train Loss: 6.7672, Train Accuracy: 0.2327, Val Loss: 10.9692, Val Accuracy: 0.2042\n",
      "Epoch 112/1000, Train Loss: 6.4220, Train Accuracy: 0.2192, Val Loss: 10.7218, Val Accuracy: 0.2072\n",
      "Epoch 113/1000, Train Loss: 7.8191, Train Accuracy: 0.2024, Val Loss: 11.0483, Val Accuracy: 0.2061\n",
      "Epoch 114/1000, Train Loss: 6.5411, Train Accuracy: 0.2375, Val Loss: 10.8231, Val Accuracy: 0.2072\n",
      "Epoch 115/1000, Train Loss: 6.4980, Train Accuracy: 0.2119, Val Loss: 10.6908, Val Accuracy: 0.2061\n",
      "Epoch 116/1000, Train Loss: 7.6271, Train Accuracy: 0.1961, Val Loss: 10.9054, Val Accuracy: 0.2021\n",
      "Epoch 117/1000, Train Loss: 6.8044, Train Accuracy: 0.2148, Val Loss: 10.7313, Val Accuracy: 0.2023\n",
      "Epoch 118/1000, Train Loss: 6.2526, Train Accuracy: 0.2395, Val Loss: 10.8688, Val Accuracy: 0.2025\n",
      "Epoch 119/1000, Train Loss: 7.0583, Train Accuracy: 0.2070, Val Loss: 10.5963, Val Accuracy: 0.2085\n",
      "Epoch 120/1000, Train Loss: 6.7818, Train Accuracy: 0.2288, Val Loss: 10.6145, Val Accuracy: 0.2107\n",
      "Epoch 121/1000, Train Loss: 7.8033, Train Accuracy: 0.2088, Val Loss: 10.6125, Val Accuracy: 0.2081\n",
      "Epoch 122/1000, Train Loss: 6.8594, Train Accuracy: 0.2210, Val Loss: 10.6922, Val Accuracy: 0.2063\n",
      "Epoch 123/1000, Train Loss: 7.0582, Train Accuracy: 0.2228, Val Loss: 10.7250, Val Accuracy: 0.2052\n",
      "Epoch 124/1000, Train Loss: 8.9671, Train Accuracy: 0.2098, Val Loss: 10.6478, Val Accuracy: 0.2072\n",
      "Epoch 125/1000, Train Loss: 6.9343, Train Accuracy: 0.2237, Val Loss: 11.9876, Val Accuracy: 0.1947\n",
      "Epoch 126/1000, Train Loss: 6.9050, Train Accuracy: 0.2129, Val Loss: 11.3269, Val Accuracy: 0.2046\n",
      "Epoch 127/1000, Train Loss: 7.1601, Train Accuracy: 0.2236, Val Loss: 10.9900, Val Accuracy: 0.2035\n",
      "Epoch 128/1000, Train Loss: 6.5562, Train Accuracy: 0.2231, Val Loss: 10.7191, Val Accuracy: 0.2030\n",
      "Epoch 129/1000, Train Loss: 6.4535, Train Accuracy: 0.2277, Val Loss: 10.8123, Val Accuracy: 0.2028\n",
      "Epoch 130/1000, Train Loss: 6.4372, Train Accuracy: 0.2287, Val Loss: 10.9008, Val Accuracy: 0.2066\n",
      "Epoch 131/1000, Train Loss: 6.0967, Train Accuracy: 0.2439, Val Loss: 10.7055, Val Accuracy: 0.2063\n",
      "Epoch 132/1000, Train Loss: 7.5024, Train Accuracy: 0.2073, Val Loss: 10.8576, Val Accuracy: 0.1994\n",
      "Epoch 133/1000, Train Loss: 6.1325, Train Accuracy: 0.2182, Val Loss: 10.9303, Val Accuracy: 0.1949\n",
      "Epoch 134/1000, Train Loss: 6.0067, Train Accuracy: 0.2660, Val Loss: 10.9444, Val Accuracy: 0.1967\n",
      "Epoch 135/1000, Train Loss: 6.1591, Train Accuracy: 0.2190, Val Loss: 10.6888, Val Accuracy: 0.2048\n",
      "Epoch 136/1000, Train Loss: 6.1177, Train Accuracy: 0.2269, Val Loss: 10.6996, Val Accuracy: 0.2082\n",
      "Epoch 137/1000, Train Loss: 6.4970, Train Accuracy: 0.2347, Val Loss: 10.7617, Val Accuracy: 0.2042\n",
      "Epoch 138/1000, Train Loss: 6.4015, Train Accuracy: 0.2194, Val Loss: 10.7486, Val Accuracy: 0.1996\n",
      "Epoch 139/1000, Train Loss: 6.2876, Train Accuracy: 0.2479, Val Loss: 10.9379, Val Accuracy: 0.1925\n",
      "Epoch 140/1000, Train Loss: 6.0952, Train Accuracy: 0.2554, Val Loss: 11.0995, Val Accuracy: 0.1891\n",
      "Epoch 141/1000, Train Loss: 6.0524, Train Accuracy: 0.2400, Val Loss: 10.9507, Val Accuracy: 0.1896\n",
      "Epoch 142/1000, Train Loss: 6.4930, Train Accuracy: 0.2387, Val Loss: 10.6457, Val Accuracy: 0.2003\n",
      "Epoch 143/1000, Train Loss: 6.5316, Train Accuracy: 0.2167, Val Loss: 10.7957, Val Accuracy: 0.2004\n",
      "Epoch 144/1000, Train Loss: 7.1802, Train Accuracy: 0.2150, Val Loss: 10.8385, Val Accuracy: 0.2019\n",
      "Epoch 145/1000, Train Loss: 6.5067, Train Accuracy: 0.2324, Val Loss: 11.0291, Val Accuracy: 0.1940\n",
      "Epoch 146/1000, Train Loss: 6.7517, Train Accuracy: 0.2236, Val Loss: 10.7220, Val Accuracy: 0.1958\n",
      "Epoch 147/1000, Train Loss: 6.8891, Train Accuracy: 0.2282, Val Loss: 10.7767, Val Accuracy: 0.1963\n",
      "Epoch 148/1000, Train Loss: 6.3289, Train Accuracy: 0.2299, Val Loss: 11.8627, Val Accuracy: 0.1799\n",
      "Epoch 149/1000, Train Loss: 6.9131, Train Accuracy: 0.2255, Val Loss: 11.5989, Val Accuracy: 0.1970\n",
      "Epoch 150/1000, Train Loss: 8.0318, Train Accuracy: 0.2083, Val Loss: 11.2249, Val Accuracy: 0.1912\n",
      "Epoch 151/1000, Train Loss: 6.8676, Train Accuracy: 0.2393, Val Loss: 10.9227, Val Accuracy: 0.1947\n",
      "Epoch 152/1000, Train Loss: 6.4581, Train Accuracy: 0.2263, Val Loss: 11.7001, Val Accuracy: 0.1912\n",
      "Epoch 153/1000, Train Loss: 6.4654, Train Accuracy: 0.2273, Val Loss: 10.7697, Val Accuracy: 0.2069\n",
      "Epoch 154/1000, Train Loss: 6.4250, Train Accuracy: 0.2477, Val Loss: 10.6778, Val Accuracy: 0.2096\n",
      "Epoch 155/1000, Train Loss: 6.2087, Train Accuracy: 0.2468, Val Loss: 11.5915, Val Accuracy: 0.2017\n",
      "Epoch 156/1000, Train Loss: 7.7627, Train Accuracy: 0.2165, Val Loss: 11.0248, Val Accuracy: 0.2003\n",
      "Epoch 157/1000, Train Loss: 6.4438, Train Accuracy: 0.2455, Val Loss: 11.0934, Val Accuracy: 0.1881\n",
      "Epoch 158/1000, Train Loss: 6.2589, Train Accuracy: 0.2289, Val Loss: 11.5323, Val Accuracy: 0.1764\n",
      "Epoch 159/1000, Train Loss: 7.0125, Train Accuracy: 0.2043, Val Loss: 11.1750, Val Accuracy: 0.1986\n",
      "Epoch 160/1000, Train Loss: 6.3692, Train Accuracy: 0.2290, Val Loss: 11.5191, Val Accuracy: 0.1945\n",
      "Epoch 161/1000, Train Loss: 5.9362, Train Accuracy: 0.2487, Val Loss: 11.1526, Val Accuracy: 0.1931\n",
      "Epoch 162/1000, Train Loss: 7.5271, Train Accuracy: 0.2178, Val Loss: 11.3958, Val Accuracy: 0.1869\n",
      "Epoch 163/1000, Train Loss: 7.4481, Train Accuracy: 0.2137, Val Loss: 11.1746, Val Accuracy: 0.1884\n",
      "Epoch 164/1000, Train Loss: 5.9638, Train Accuracy: 0.2549, Val Loss: 11.1584, Val Accuracy: 0.1889\n",
      "Epoch 165/1000, Train Loss: 6.3544, Train Accuracy: 0.2288, Val Loss: 11.3310, Val Accuracy: 0.1860\n",
      "Epoch 166/1000, Train Loss: 5.7481, Train Accuracy: 0.2712, Val Loss: 11.2317, Val Accuracy: 0.1867\n",
      "Epoch 167/1000, Train Loss: 6.0933, Train Accuracy: 0.2476, Val Loss: 11.9083, Val Accuracy: 0.1759\n",
      "Epoch 168/1000, Train Loss: 6.1745, Train Accuracy: 0.2405, Val Loss: 11.2695, Val Accuracy: 0.1875\n",
      "Epoch 169/1000, Train Loss: 6.3802, Train Accuracy: 0.2399, Val Loss: 11.9737, Val Accuracy: 0.1747\n",
      "Epoch 170/1000, Train Loss: 6.0315, Train Accuracy: 0.2400, Val Loss: 11.3571, Val Accuracy: 0.1886\n",
      "Epoch 171/1000, Train Loss: 6.0168, Train Accuracy: 0.2465, Val Loss: 11.6169, Val Accuracy: 0.1837\n",
      "Epoch 172/1000, Train Loss: 5.9268, Train Accuracy: 0.2491, Val Loss: 11.2137, Val Accuracy: 0.1875\n",
      "Epoch 173/1000, Train Loss: 6.1902, Train Accuracy: 0.2406, Val Loss: 11.4968, Val Accuracy: 0.1837\n",
      "Epoch 174/1000, Train Loss: 5.6501, Train Accuracy: 0.2433, Val Loss: 11.2478, Val Accuracy: 0.1888\n",
      "Epoch 175/1000, Train Loss: 6.1286, Train Accuracy: 0.2499, Val Loss: 11.4992, Val Accuracy: 0.1839\n",
      "Epoch 176/1000, Train Loss: 5.9544, Train Accuracy: 0.2472, Val Loss: 11.3277, Val Accuracy: 0.1933\n",
      "Epoch 177/1000, Train Loss: 5.9368, Train Accuracy: 0.2475, Val Loss: 11.3069, Val Accuracy: 0.1842\n",
      "Epoch 178/1000, Train Loss: 5.7943, Train Accuracy: 0.2369, Val Loss: 11.3147, Val Accuracy: 0.1841\n",
      "Epoch 179/1000, Train Loss: 5.7185, Train Accuracy: 0.2457, Val Loss: 11.3668, Val Accuracy: 0.1824\n",
      "Epoch 180/1000, Train Loss: 6.2075, Train Accuracy: 0.2381, Val Loss: 11.6948, Val Accuracy: 0.1739\n",
      "Epoch 181/1000, Train Loss: 5.8456, Train Accuracy: 0.2528, Val Loss: 11.3149, Val Accuracy: 0.1910\n",
      "Epoch 182/1000, Train Loss: 5.7502, Train Accuracy: 0.2052, Val Loss: 11.5117, Val Accuracy: 0.1903\n",
      "Epoch 183/1000, Train Loss: 5.8268, Train Accuracy: 0.2315, Val Loss: 11.3888, Val Accuracy: 0.1892\n",
      "Epoch 184/1000, Train Loss: 6.7426, Train Accuracy: 0.2202, Val Loss: 11.3751, Val Accuracy: 0.1861\n",
      "Epoch 185/1000, Train Loss: 5.7768, Train Accuracy: 0.2498, Val Loss: 11.4395, Val Accuracy: 0.1850\n",
      "Epoch 186/1000, Train Loss: 5.9945, Train Accuracy: 0.2455, Val Loss: 11.1573, Val Accuracy: 0.1957\n",
      "Epoch 187/1000, Train Loss: 6.2337, Train Accuracy: 0.2341, Val Loss: 11.4047, Val Accuracy: 0.1916\n",
      "Epoch 188/1000, Train Loss: 6.3142, Train Accuracy: 0.2719, Val Loss: 11.4679, Val Accuracy: 0.1879\n",
      "Epoch 189/1000, Train Loss: 6.6584, Train Accuracy: 0.2611, Val Loss: 11.4110, Val Accuracy: 0.1833\n",
      "Epoch 190/1000, Train Loss: 6.6675, Train Accuracy: 0.2209, Val Loss: 12.7368, Val Accuracy: 0.1581\n",
      "Epoch 191/1000, Train Loss: 5.8966, Train Accuracy: 0.2682, Val Loss: 11.8260, Val Accuracy: 0.1973\n",
      "Epoch 192/1000, Train Loss: 6.2055, Train Accuracy: 0.2445, Val Loss: 12.6723, Val Accuracy: 0.1855\n",
      "Epoch 193/1000, Train Loss: 6.1271, Train Accuracy: 0.2419, Val Loss: 11.7157, Val Accuracy: 0.1970\n",
      "Epoch 194/1000, Train Loss: 5.8905, Train Accuracy: 0.2215, Val Loss: 11.5708, Val Accuracy: 0.1817\n",
      "Epoch 195/1000, Train Loss: 5.9160, Train Accuracy: 0.2552, Val Loss: 11.3451, Val Accuracy: 0.1803\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_and_save_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_size\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[90], line 12\u001b[0m, in \u001b[0;36mtrain_and_save_predictions\u001b[1;34m(model, train_loader, val_loader, optimizer, mass, g, num_epochs, alpha, beta, gamma, radius, box_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m     x_reconstructed, z_next, dzdt, z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     z_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkoopman(z)\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m combined_loss(x_reconstructed, targets, z_next, dzdt, z_pred, mass, g, alpha, beta, gamma)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[86], line 36\u001b[0m, in \u001b[0;36mSINDyKoopmanFNO.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m z_fourier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_layer(z_next)  \u001b[38;5;66;03m# Fourier transform\u001b[39;00m\n\u001b[0;32m     35\u001b[0m z_combined \u001b[38;5;241m=\u001b[39m z_next \u001b[38;5;241m+\u001b[39m z_fourier  \u001b[38;5;66;03m# Combine latent dynamics\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_combined\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Decode back to input\u001b[39;00m\n\u001b[0;32m     37\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(x_reconstructed\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     38\u001b[0m z_next_list\u001b[38;5;241m.\u001b[39mappend(z_next\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_and_save_predictions(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    mass=mass,\n",
    "    g=G,\n",
    "    num_epochs=num_epochs,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    gamma=gamma,\n",
    "    radius=radius,\n",
    "    box_size=box_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Evaluation\n",
    "def evaluate_and_save_test_predictions(model, test_loader, mass, g, radius, box_size):\n",
    "    model.eval()\n",
    "    test_predictions, test_targets = [], []\n",
    "    total_test_loss, total_test_acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            x_reconstructed, z_next, dzdt, z = model(inputs)\n",
    "            loss = combined_loss(x_reconstructed, targets, z_next, dzdt, z, mass, g)\n",
    "            total_test_loss += loss.item()\n",
    "            test_acc = 1 - (torch.norm(x_reconstructed - targets) / torch.norm(targets))\n",
    "            total_test_acc += test_acc.item()\n",
    "\n",
    "            test_predictions.append(x_reconstructed.detach().cpu().numpy())\n",
    "            test_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    avg_test_acc = total_test_acc / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.4f}\")\n",
    "\n",
    "    save_all_test_predictions(np.concatenate(test_predictions), np.concatenate(test_targets), folder=\"test_predictions\", radius=radius, box_size=box_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_and_save_test_predictions(model, test_loader, mass, G, radius, box_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
