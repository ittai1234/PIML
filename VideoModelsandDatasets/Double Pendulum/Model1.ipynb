{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.integrate import solve_ivp\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Pendulum Dynamics\n",
    "def double_pendulum(t, y, l1, l2, m1, m2, g):\n",
    "    theta1, z1, theta2, z2 = y\n",
    "    delta = theta2 - theta1\n",
    "    denom1 = (m1 + m2) * l1 - m2 * l1 * np.cos(delta) ** 2\n",
    "    denom2 = (l2 / l1) * denom1\n",
    "\n",
    "    dydt = np.zeros_like(y)\n",
    "    dydt[0] = z1\n",
    "    dydt[1] = (\n",
    "        (m2 * l1 * z1 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "         + m2 * g * np.sin(theta2) * np.cos(delta)\n",
    "         + m2 * l2 * z2 ** 2 * np.sin(delta)\n",
    "         - (m1 + m2) * g * np.sin(theta1))\n",
    "        / denom1\n",
    "    )\n",
    "    dydt[2] = z2\n",
    "    dydt[3] = (\n",
    "        (-m2 * l2 * z2 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "         + (m1 + m2) * g * np.sin(theta1) * np.cos(delta)\n",
    "         - (m1 + m2) * l1 * z1 ** 2 * np.sin(delta)\n",
    "         - (m1 + m2) * g * np.sin(theta2))\n",
    "        / denom2\n",
    "    )\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Workflow\n",
    "n_pendulums = 100\n",
    "output_dir = \"video_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset\n",
    "def generate_dataset(n_pendulums, dt, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    dataset = []\n",
    "\n",
    "    for i in tqdm(range(n_pendulums), desc=\"Generating dataset\"):\n",
    "        l1, l2 = np.random.uniform(0.5, 2.0, 2)\n",
    "        m1, m2 = np.random.uniform(0.5, 2.0, 2)\n",
    "        g = 9.81\n",
    "        y0 = np.random.uniform(-np.pi, np.pi, 4)\n",
    "        t_span = (0, 10)\n",
    "        t_eval = np.linspace(t_span[0], t_span[1], int(10 / dt))\n",
    "\n",
    "        sol = solve_ivp(double_pendulum, t_span, y0, t_eval=t_eval, args=(l1, l2, m1, m2, g), method='RK45')\n",
    "        data = np.column_stack((sol.y[0], sol.y[2]))\n",
    "\n",
    "        gif_path = os.path.join(output_dir, f\"pendulum_{i}.gif\")\n",
    "        generate_gif(data, l1, l2, gif_path)\n",
    "\n",
    "        dataset.append((data, l1, l2, m1, m2, g, gif_path))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GIFs\n",
    "def generate_gif(data, l1, l2, save_path):\n",
    "    theta1, theta2 = data[:, 0], data[:, 1]\n",
    "    x1 = l1 * np.sin(theta1)\n",
    "    y1 = -l1 * np.cos(theta1)\n",
    "    x2 = x1 + l2 * np.sin(theta2)\n",
    "    y2 = y1 - l2 * np.cos(theta2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    line, = ax.plot([], [], 'o-', lw=2)\n",
    "\n",
    "    def update(frame):\n",
    "        line.set_data([0, x1[frame], x2[frame]], [0, y1[frame], y2[frame]])\n",
    "        return line,\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(theta1), blit=True, interval=50)\n",
    "    ani.save(save_path, fps=20, writer='imagemagick')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   0%|          | 0/100 [00:00<?, ?it/s]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   1%|          | 1/100 [00:34<57:21, 34.77s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   2%|▏         | 2/100 [01:09<56:44, 34.74s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   3%|▎         | 3/100 [01:43<55:56, 34.61s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   4%|▍         | 4/100 [02:18<55:14, 34.53s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   5%|▌         | 5/100 [02:52<54:38, 34.51s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   6%|▌         | 6/100 [03:27<54:23, 34.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   7%|▋         | 7/100 [04:02<53:46, 34.69s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   8%|▊         | 8/100 [04:37<53:17, 34.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:   9%|▉         | 9/100 [05:12<52:39, 34.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  10%|█         | 10/100 [05:46<51:58, 34.65s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  11%|█         | 11/100 [06:21<51:18, 34.59s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  12%|█▏        | 12/100 [06:55<50:47, 34.64s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  13%|█▎        | 13/100 [07:30<50:22, 34.74s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  14%|█▍        | 14/100 [08:05<49:39, 34.65s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  15%|█▌        | 15/100 [08:39<48:57, 34.56s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  16%|█▌        | 16/100 [09:14<48:21, 34.55s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  17%|█▋        | 17/100 [09:48<47:41, 34.48s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  18%|█▊        | 18/100 [10:22<47:05, 34.46s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  19%|█▉        | 19/100 [10:57<46:32, 34.48s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  20%|██        | 20/100 [11:32<46:07, 34.60s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  21%|██        | 21/100 [12:06<45:25, 34.50s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  22%|██▏       | 22/100 [12:40<44:46, 34.44s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  23%|██▎       | 23/100 [13:15<44:18, 34.53s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  24%|██▍       | 24/100 [13:50<43:44, 34.53s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  25%|██▌       | 25/100 [14:24<43:12, 34.57s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  26%|██▌       | 26/100 [14:59<42:52, 34.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  27%|██▋       | 27/100 [15:34<42:17, 34.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  28%|██▊       | 28/100 [16:09<41:33, 34.63s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  29%|██▉       | 29/100 [16:43<41:00, 34.66s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  30%|███       | 30/100 [17:18<40:20, 34.58s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  31%|███       | 31/100 [17:52<39:48, 34.62s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  32%|███▏      | 32/100 [18:27<39:16, 34.66s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  33%|███▎      | 33/100 [19:02<38:39, 34.62s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  34%|███▍      | 34/100 [19:37<38:10, 34.70s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  35%|███▌      | 35/100 [20:11<37:38, 34.74s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  36%|███▌      | 36/100 [20:46<37:00, 34.70s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  37%|███▋      | 37/100 [21:21<36:25, 34.69s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  38%|███▊      | 38/100 [21:55<35:48, 34.65s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  39%|███▉      | 39/100 [22:30<35:10, 34.61s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  40%|████      | 40/100 [23:04<34:35, 34.59s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  41%|████      | 41/100 [23:39<34:01, 34.60s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  42%|████▏     | 42/100 [24:13<33:25, 34.58s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  43%|████▎     | 43/100 [24:48<32:51, 34.59s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  44%|████▍     | 44/100 [25:23<32:18, 34.62s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  45%|████▌     | 45/100 [25:57<31:43, 34.60s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  46%|████▌     | 46/100 [26:32<31:15, 34.73s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  47%|████▋     | 47/100 [27:07<30:42, 34.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  48%|████▊     | 48/100 [27:42<30:04, 34.70s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  49%|████▉     | 49/100 [28:16<29:29, 34.69s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  50%|█████     | 50/100 [28:51<28:51, 34.64s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  51%|█████     | 51/100 [29:26<28:21, 34.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  52%|█████▏    | 52/100 [30:01<27:57, 34.94s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  53%|█████▎    | 53/100 [30:36<27:18, 34.86s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  54%|█████▍    | 54/100 [31:11<26:40, 34.79s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  55%|█████▌    | 55/100 [31:45<26:02, 34.72s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  56%|█████▌    | 56/100 [32:19<25:22, 34.61s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  57%|█████▋    | 57/100 [32:54<24:50, 34.66s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  58%|█████▊    | 58/100 [33:29<24:18, 34.73s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  59%|█████▉    | 59/100 [34:04<23:42, 34.70s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  60%|██████    | 60/100 [34:38<23:07, 34.69s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  61%|██████    | 61/100 [35:13<22:31, 34.65s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  62%|██████▏   | 62/100 [35:47<21:50, 34.49s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  63%|██████▎   | 63/100 [36:22<21:17, 34.52s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  64%|██████▍   | 64/100 [36:57<20:46, 34.63s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  65%|██████▌   | 65/100 [37:31<20:13, 34.68s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  66%|██████▌   | 66/100 [38:06<19:41, 34.74s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  67%|██████▋   | 67/100 [38:43<19:31, 35.50s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  68%|██████▊   | 68/100 [39:23<19:38, 36.82s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  69%|██████▉   | 69/100 [40:03<19:30, 37.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  70%|███████   | 70/100 [40:43<19:10, 38.34s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  71%|███████   | 71/100 [41:21<18:27, 38.20s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  72%|███████▏  | 72/100 [41:58<17:41, 37.91s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  73%|███████▎  | 73/100 [42:34<16:44, 37.21s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  74%|███████▍  | 74/100 [43:08<15:44, 36.34s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  75%|███████▌  | 75/100 [43:43<14:56, 35.84s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  76%|███████▌  | 76/100 [44:17<14:10, 35.44s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  77%|███████▋  | 77/100 [44:52<13:29, 35.18s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  78%|███████▊  | 78/100 [45:26<12:50, 35.03s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  79%|███████▉  | 79/100 [46:01<12:12, 34.88s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  80%|████████  | 80/100 [46:36<11:35, 34.79s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  81%|████████  | 81/100 [47:10<10:59, 34.69s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  82%|████████▏ | 82/100 [47:44<10:22, 34.61s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  83%|████████▎ | 83/100 [48:19<09:47, 34.55s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  84%|████████▍ | 84/100 [48:53<09:12, 34.56s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  85%|████████▌ | 85/100 [49:28<08:38, 34.59s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  86%|████████▌ | 86/100 [50:03<08:04, 34.61s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  87%|████████▋ | 87/100 [50:37<07:30, 34.63s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  88%|████████▊ | 88/100 [51:12<06:55, 34.64s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  89%|████████▉ | 89/100 [51:47<06:22, 34.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  90%|█████████ | 90/100 [52:22<05:47, 34.76s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  91%|█████████ | 91/100 [52:58<05:15, 35.02s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  92%|█████████▏| 92/100 [53:31<04:37, 34.63s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  93%|█████████▎| 93/100 [54:07<04:04, 34.94s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  94%|█████████▍| 94/100 [54:42<03:29, 34.93s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  95%|█████████▌| 95/100 [55:16<02:54, 34.84s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  96%|█████████▌| 96/100 [55:51<02:19, 34.77s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  97%|█████████▋| 97/100 [56:26<01:44, 34.75s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  98%|█████████▊| 98/100 [57:00<01:09, 34.67s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset:  99%|█████████▉| 99/100 [57:35<00:34, 34.67s/it]MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "Generating dataset: 100%|██████████| 100/100 [58:09<00:00, 34.90s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_dataset(n_pendulums, dt=0.01, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "class DoublePendulumDataset(Dataset):\n",
    "    def __init__(self, dataset, seq_len, pred_len):\n",
    "        self.dataset = dataset\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, l1, l2, m1, m2, g, gif_path = self.dataset[idx]\n",
    "\n",
    "        gif = Image.open(gif_path)\n",
    "        frames = []\n",
    "        try:\n",
    "            while True:\n",
    "                frame = gif.convert(\"RGB\")\n",
    "                frames.append(np.array(frame))\n",
    "                gif.seek(gif.tell() + 1)\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "        positions = np.column_stack([data[:, 0], data[:, 1]])\n",
    "\n",
    "        input_seq = positions[:self.seq_len]\n",
    "        target_seq = positions[self.seq_len:self.seq_len + self.pred_len]\n",
    "\n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32), gif_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 90\n",
    "pred_len = 10\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = DoublePendulumDataset(train_data, seq_len, pred_len)\n",
    "val_dataset = DoublePendulumDataset(val_data, seq_len, pred_len)\n",
    "test_dataset = DoublePendulumDataset(test_data, seq_len, pred_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class LSTMPendulumPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMPendulumPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, _ in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction GIFs\n",
    "def generate_prediction_gif(model, test_loader, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    true_motion_dir = os.path.join(output_dir, \"true_motion\")\n",
    "    pred_motion_dir = os.path.join(output_dir, \"predicted_motion\")\n",
    "    os.makedirs(true_motion_dir, exist_ok=True)\n",
    "    os.makedirs(pred_motion_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    def compute_positions(theta):\n",
    "        x1 = np.sin(theta[0])\n",
    "        y1 = -np.cos(theta[0])\n",
    "        x2 = x1 + np.sin(theta[1])\n",
    "        y2 = y1 - np.cos(theta[1])\n",
    "        return np.array([x1, y1, x2, y2])\n",
    "\n",
    "    for i, (inputs, targets, _) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            # Generate sequential predictions\n",
    "            inputs_seq = inputs.squeeze(0).numpy()\n",
    "            predictions = []\n",
    "            current_input = inputs_seq[0]  # Initial state\n",
    "\n",
    "            # Debug: Initial input shape\n",
    "            print(f\"Sample {i}: Initial input shape: {current_input.shape}\")\n",
    "\n",
    "            for _ in range(targets.shape[1]):  # Number of time steps\n",
    "                # Reshape current_input to match model input shape\n",
    "                model_input = torch.tensor(current_input).unsqueeze(0).unsqueeze(1)  # Shape (1, 1, input_size)\n",
    "                current_pred = model(model_input).squeeze(0).numpy()  # Shape (output_size,)\n",
    "                \n",
    "                # Debug: Log shapes before concatenation\n",
    "                print(f\"Current input shape: {current_input.shape}, Current prediction shape: {current_pred.shape}\")\n",
    "\n",
    "                current_pred = current_pred.flatten()  # Ensure 1D shape for concatenation\n",
    "                current_input = np.concatenate([current_input[2:], current_pred])  # Shift and append\n",
    "                predictions.append(current_pred)\n",
    "\n",
    "            predictions = np.array(predictions).T  # Shape (2, N)\n",
    "\n",
    "        # Reshape true positions\n",
    "        true_positions = targets.squeeze(0).numpy().reshape(2, -1)\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        print(f\"Sample {i}: Reshaped True positions shape {true_positions.shape}, Reshaped Predicted positions shape {predictions.shape}\")\n",
    "\n",
    "        # Compute positional data\n",
    "        true_pos = compute_positions(true_positions)\n",
    "        pred_pos = compute_positions(predictions)\n",
    "\n",
    "        num_frames = min(true_pos.shape[1], pred_pos.shape[1])\n",
    "\n",
    "        def create_gif(data, save_path, label, style):\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            ax.set_xlim(-2.5, 2.5)\n",
    "            ax.set_ylim(-2.5, 2.5)\n",
    "            ax.set_aspect('equal')\n",
    "            line, = ax.plot([], [], style, lw=2, label=label)\n",
    "            ax.legend()\n",
    "\n",
    "            def update(frame):\n",
    "                x = [0, data[0, frame], data[2, frame]]\n",
    "                y = [0, data[1, frame], data[3, frame]]\n",
    "                line.set_data(x, y)\n",
    "                return line,\n",
    "\n",
    "            ani = FuncAnimation(fig, update, frames=num_frames, blit=True, interval=50)\n",
    "            ani.save(save_path, fps=20, writer='pillow')\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Create True Motion GIF\n",
    "        true_gif_path = os.path.join(true_motion_dir, f\"true_motion_{i}.gif\")\n",
    "        create_gif(true_pos, true_gif_path, \"True Motion\", \"o-\")\n",
    "\n",
    "        # Create Predicted Motion GIF\n",
    "        pred_gif_path = os.path.join(pred_motion_dir, f\"predicted_motion_{i}.gif\")\n",
    "        create_gif(pred_pos, pred_gif_path, \"Predicted Motion\", \"o--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMPendulumPredictor(input_dim, hidden_dim, output_dim, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/3 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 10, 2])) that is different to the input size (torch.Size([32, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/20:  67%|██████▋   | 2/3 [03:00<01:29, 89.91s/it]C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([16, 10, 2])) that is different to the input size (torch.Size([16, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/20: 100%|██████████| 3/3 [03:45<00:00, 75.03s/it]\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([10, 10, 2])) that is different to the input size (torch.Size([10, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.3736090660095215, Val Loss: 4.52817964553833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 3/3 [03:45<00:00, 75.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 5.318526983261108, Val Loss: 4.28372049331665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 3/3 [03:45<00:00, 75.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 4.801838556925456, Val Loss: 3.9791100025177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 3/3 [03:45<00:00, 75.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 4.321553389231364, Val Loss: 3.564699411392212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 3/3 [03:47<00:00, 75.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 4.190829038619995, Val Loss: 3.003627300262451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 3/3 [04:02<00:00, 80.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 5.177499890327454, Val Loss: 2.373523473739624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 3/3 [03:56<00:00, 78.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 3.458063840866089, Val Loss: 1.9403414726257324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 3/3 [03:57<00:00, 79.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 3.6725242932637534, Val Loss: 1.731871485710144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 3/3 [03:49<00:00, 76.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 3.348624308904012, Val Loss: 1.5580623149871826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 3/3 [03:46<00:00, 75.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 3.335639794667562, Val Loss: 1.381689190864563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 3/3 [03:45<00:00, 75.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 2.703630884488424, Val Loss: 1.2240482568740845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 3/3 [03:51<00:00, 77.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 2.8694640398025513, Val Loss: 1.0643788576126099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 3/3 [04:15<00:00, 85.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 2.139625906944275, Val Loss: 0.9126798510551453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 3/3 [04:09<00:00, 83.17s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 2.628370761871338, Val Loss: 0.7586759924888611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 3/3 [04:11<00:00, 83.70s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 1.857475479443868, Val Loss: 0.6270207166671753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 3/3 [03:57<00:00, 79.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 1.7895455757776897, Val Loss: 0.5433318614959717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 3/3 [04:00<00:00, 80.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 1.545601765314738, Val Loss: 0.4671122431755066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 3/3 [04:01<00:00, 80.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 1.3085393210252125, Val Loss: 0.3936749994754791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 3/3 [03:55<00:00, 78.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 1.3139463663101196, Val Loss: 0.3121100664138794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 3/3 [03:49<00:00, 76.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 1.5473692814509075, Val Loss: 0.27944207191467285\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 0: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 1: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 1: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 2: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 2: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 3: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 3: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 4: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 4: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 5: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 5: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 6: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 6: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 7: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 7: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 8: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 8: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n",
      "Sample 9: Initial input shape: (2,)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Current input shape: (2,), Current prediction shape: (1, 2)\n",
      "Sample 9: Reshaped True positions shape (2, 10), Reshaped Predicted positions shape (2, 10)\n"
     ]
    }
   ],
   "source": [
    "generate_prediction_gif(model, test_loader, output_dir=\"predicted_gifs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
