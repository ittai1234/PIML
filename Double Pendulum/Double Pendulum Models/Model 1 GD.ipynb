{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.integrate import solve_ivp\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import pysindy as ps\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define double pendulum dynamics\n",
    "def double_pendulum(t, y, l1, l2, m1, m2, g):\n",
    "    theta1, z1, theta2, z2 = y\n",
    "    delta = theta2 - theta1\n",
    "    denom1 = (m1 + m2) * l1 - m2 * l1 * np.cos(delta) ** 2\n",
    "    denom2 = (l2 / l1) * denom1\n",
    "\n",
    "    dydt = np.zeros_like(y)\n",
    "    dydt[0] = z1\n",
    "    dydt[1] = (\n",
    "        (m2 * l1 * z1 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "         + m2 * g * np.sin(theta2) * np.cos(delta)\n",
    "         + m2 * l2 * z2 ** 2 * np.sin(delta)\n",
    "         - (m1 + m2) * g * np.sin(theta1))\n",
    "        / denom1\n",
    "    )\n",
    "    dydt[2] = z2\n",
    "    dydt[3] = (\n",
    "        (-m2 * l2 * z2 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "         + (m1 + m2) * g * np.sin(theta1) * np.cos(delta)\n",
    "         - (m1 + m2) * l1 * z1 ** 2 * np.sin(delta)\n",
    "         - (m1 + m2) * g * np.sin(theta2))\n",
    "        / denom2\n",
    "    )\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pendulums = 2000  # data set sizae\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset with second derivatives\n",
    "for _ in range(n_pendulums):\n",
    "    l1, l2 = np.random.uniform(0.5, 2.0, 2)\n",
    "    m1, m2 = np.random.uniform(0.5, 2.0, 2)\n",
    "    g = 9.81\n",
    "    y0 = np.random.uniform(-np.pi, np.pi, 4)\n",
    "    t_span = (0, 10)\n",
    "    t_eval = np.linspace(t_span[0], t_span[1], 200)  # Finer temporal resolution\n",
    "    sol = solve_ivp(double_pendulum, t_span, y0, t_eval=t_eval, args=(l1, l2, m1, m2, g))\n",
    "    \n",
    "    theta1, theta1_dot, theta2, theta2_dot = sol.y\n",
    "    theta1_ddot = np.gradient(theta1_dot, t_eval)\n",
    "    theta2_ddot = np.gradient(theta2_dot, t_eval)\n",
    "    \n",
    "    X = np.vstack([theta1, theta2, theta1_dot, theta2_dot]).T  # Inputs: angles and angular velocities\n",
    "    y = np.vstack([theta1_ddot, theta2_ddot]).T               # Targets: angular accelerations\n",
    "    data.append((X, y, l1, l2, m1, m2, g))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_double_pendulum(example, save_path=None):\n",
    "    X, _, l1, l2, _, _, _ = example  # Skip 'y'\n",
    "    theta1, theta2 = X[:, 0], X[:, 1]\n",
    "\n",
    "    # Compute positions\n",
    "    x1 = l1 * np.sin(theta1)\n",
    "    y1 = -l1 * np.cos(theta1)\n",
    "    x2 = x1 + l2 * np.sin(theta2)\n",
    "    y2 = y1 - l2 * np.cos(theta2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    line, = ax.plot([], [], 'o-', lw=2)\n",
    "\n",
    "    def update(frame):\n",
    "        line.set_data([0, x1[frame], x2[frame]], [0, y1[frame], y2[frame]])\n",
    "        return line,\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(theta1), blit=True, interval=50)\n",
    "\n",
    "    if save_path:\n",
    "        ani.save(save_path, fps=20, writer='imagemagick')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Visualize two examples\n",
    "visualize_double_pendulum(data[0], save_path='double_pendulum1.gif')\n",
    "visualize_double_pendulum(data[1], save_path='double_pendulum2.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = np.vstack([example[0] for example in train_data])\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_data = [(scaler.transform(example[0]), *example[1:]) for example in train_data]\n",
    "val_data = [(scaler.transform(example[0]), *example[1:]) for example in val_data]\n",
    "test_data = [(scaler.transform(example[0]), *example[1:]) for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open('extended_double_pendulum_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump({'train': train_data, 'val': val_data, 'test': test_data}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers, dropout_rate=0.2):\n",
    "        super(PINN, self).__init__()\n",
    "        self.input_layer = nn.Linear(layers[0], layers[1])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(1, len(layers) - 2):\n",
    "            self.hidden_layers.append(nn.Sequential(\n",
    "                nn.Linear(layers[i], layers[i + 1]),\n",
    "                nn.BatchNorm1d(layers[i + 1]),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ))\n",
    "        self.output_layer = nn.Linear(layers[-2], layers[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss functions\n",
    "def log_cosh_loss(y_pred, y_true):\n",
    "    diff = y_pred - y_true\n",
    "    return torch.mean(torch.log(torch.cosh(diff)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(y_pred, y_true, params_batch, X_batch):\n",
    "    l1, l2, m1, m2, g = params_batch\n",
    "    theta1, theta2, theta1_dot, theta2_dot = torch.split(X_batch, 1, dim=1)\n",
    "    theta1_ddot, theta2_ddot = torch.split(y_pred, 1, dim=1)\n",
    "\n",
    "    delta = theta2 - theta1\n",
    "    physics_term1 = (\n",
    "        (m2 * l1 * theta1_dot**2 * torch.sin(delta) * torch.cos(delta)\n",
    "         + m2 * g * torch.sin(theta2) * torch.cos(delta)\n",
    "         + m2 * l2 * theta2_dot**2 * torch.sin(delta)\n",
    "         - (m1 + m2) * g * torch.sin(theta1))\n",
    "        / ((m1 + m2) * l1 - m2 * l1 * torch.cos(delta)**2)\n",
    "    )\n",
    "    physics_term2 = (\n",
    "        (-m2 * l2 * theta2_dot**2 * torch.sin(delta) * torch.cos(delta)\n",
    "         + (m1 + m2) * g * torch.sin(theta1) * torch.cos(delta)\n",
    "         - (m1 + m2) * l1 * theta1_dot**2 * torch.sin(delta)\n",
    "         - (m1 + m2) * g * torch.sin(theta2))\n",
    "        / ((l2 / l1) * ((m1 + m2) * l1 - m2 * l1 * torch.cos(delta)**2))\n",
    "    )\n",
    "\n",
    "    loss = torch.mean((theta1_ddot - physics_term1)**2 + (theta2_ddot - physics_term2)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('extended_double_pendulum_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['val']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train = torch.tensor(np.vstack([example[0] for example in train_data]), dtype=torch.float32)\n",
    "y_train = torch.tensor(np.vstack([example[1] for example in train_data]), dtype=torch.float32)\n",
    "params_train = torch.tensor([example[2:] for example in train_data], dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(np.vstack([example[0] for example in val_data]), dtype=torch.float32)\n",
    "y_val = torch.tensor(np.vstack([example[1] for example in val_data]), dtype=torch.float32)\n",
    "params_val = torch.tensor([example[2:] for example in val_data], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PINN\n",
    "model = PINN([4, 256, 256, 128, 64, 2])  # Increased depth and residual connections\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accuracy metric for regression\n",
    "def compute_accuracy(y_pred, y_true, threshold=0.1):\n",
    "    # Calculate absolute error\n",
    "    error = torch.abs(y_pred - y_true)\n",
    "    # Count predictions within the threshold\n",
    "    within_threshold = (error < threshold).all(dim=1).float()\n",
    "    # Calculate percentage\n",
    "    accuracy = within_threshold.mean().item() * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 330.0924987792969, Val Loss: 294.4075622558594, Train Accuracy: 0.00%, Val Accuracy: 0.00%\n",
      "Epoch 2, Loss: 329.1801452636719, Val Loss: 294.2883605957031, Train Accuracy: 0.00%, Val Accuracy: 0.00%\n",
      "Epoch 3, Loss: 328.6101989746094, Val Loss: 294.1498718261719, Train Accuracy: 0.01%, Val Accuracy: 0.01%\n",
      "Epoch 4, Loss: 328.1043395996094, Val Loss: 293.9916076660156, Train Accuracy: 0.00%, Val Accuracy: 0.01%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m physics_loss(y_pred, y_train, params_train\u001b[38;5;241m.\u001b[39mT, X_train)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train) + 0.1 * physics_loss(y_pred, y_train, params_train.T, X_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_val)\n",
    "        val_loss = criterion(y_val_pred, y_val)\n",
    "        val_accuracy = compute_accuracy(y_val_pred, y_val)  # Calculate accuracy\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print metrics\n",
    "    train_accuracy = compute_accuracy(y_pred, y_train)  # Calculate training accuracy\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}, \"\n",
    "          f\"Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "test_data = torch.tensor(np.vstack([example[0] for example in test_data]), dtype=torch.float32)\n",
    "y_test = test_data[:, 2:]\n",
    "test_params = torch.tensor([example[1:] for example in test_data], dtype=torch.float32)\n",
    "\n",
    "y_test_pred = model(test_data)\n",
    "test_loss = criterion(y_test_pred, y_test)\n",
    "print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(example, model, scaler, save_path=None):\n",
    "    X, l1, l2, _, _, _ = example\n",
    "    theta1, theta2 = X[:, 0], X[:, 1]\n",
    "\n",
    "    # Predict using the model\n",
    "    X_tensor = torch.tensor(scaler.transform(X), dtype=torch.float32)\n",
    "    y_pred = model(X_tensor).detach().numpy()\n",
    "\n",
    "    theta1_pred, theta2_pred = y_pred[:, 0], y_pred[:, 1]\n",
    "\n",
    "    # Compute positions for ground truth\n",
    "    x1 = l1 * np.sin(theta1)\n",
    "    y1 = -l1 * np.cos(theta1)\n",
    "    x2 = x1 + l2 * np.sin(theta2)\n",
    "    y2 = y1 - l2 * np.cos(theta2)\n",
    "\n",
    "    # Compute positions for predictions\n",
    "    x1_pred = l1 * np.sin(theta1_pred)\n",
    "    y1_pred = -l1 * np.cos(theta1_pred)\n",
    "    x2_pred = x1_pred + l2 * np.sin(theta2_pred)\n",
    "    y2_pred = y1_pred - l2 * np.cos(theta2_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    line_true, = ax.plot([], [], 'o-', lw=2, label='Ground Truth')\n",
    "    line_pred, = ax.plot([], [], 'o--', lw=2, label='Prediction')\n",
    "    ax.legend()\n",
    "\n",
    "    def update(frame):\n",
    "        line_true.set_data([0, x1[frame], x2[frame]], [0, y1[frame], y2[frame]])\n",
    "        line_pred.set_data([0, x1_pred[frame], x2_pred[frame]], [0, y1_pred[frame], y2_pred[frame]])\n",
    "        return line_true, line_pred\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(theta1), blit=True, interval=50)\n",
    "\n",
    "    if save_path:\n",
    "        ani.save(save_path, fps=20, writer='imagemagick')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Visualize prediction vs ground truth for two examples\n",
    "visualize_prediction(test_data[0], model, scaler, save_path='prediction_vs_ground_truth1.gif')\n",
    "visualize_prediction(test_data[1], model, scaler, save_path='prediction_vs_ground_truth2.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sindy(X, t):\n",
    "    # Use custom feature library for trigonometric functions\n",
    "    library = ps.PolynomialLibrary(degree=3) + ps.TrigonometricLibrary([1, 2])\n",
    "\n",
    "    # Use a lower threshold to capture more dynamics\n",
    "    optimizer = ps.STLSQ(threshold=0.05)\n",
    "\n",
    "    # Instantiate and fit SINDy model\n",
    "    sindy_model = ps.SINDy(feature_library=library, optimizer=optimizer, differentiation_method=ps.FiniteDifference())\n",
    "    sindy_model.fit(X, t=t)\n",
    "    return sindy_model\n",
    "\n",
    "# Prepare data for SINDy\n",
    "X_sindy = np.vstack([example[0] for example in train_data])\n",
    "t_sindy = np.linspace(0, 10, X_sindy.shape[0])\n",
    "\n",
    "# Train SINDy model\n",
    "sindy_model = train_sindy(X_sindy, t_sindy)\n",
    "\n",
    "# Display discovered equations\n",
    "sindy_model.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SINDy predictions vs ground truth\n",
    "def visualize_sindy_prediction(example, sindy_model, scaler, save_path=None):\n",
    "    X, l1, l2, _, _, _ = example\n",
    "    theta1, theta2 = X[:, 0], X[:, 1]\n",
    "\n",
    "    # Predict using the SINDy model\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y_pred = sindy_model.simulate(X_scaled[0], t=np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "    theta1_pred, theta2_pred = y_pred[:, 0], y_pred[:, 1]\n",
    "\n",
    "    # Compute positions for ground truth\n",
    "    x1 = l1 * np.sin(theta1)\n",
    "    y1 = -l1 * np.cos(theta1)\n",
    "    x2 = x1 + l2 * np.sin(theta2)\n",
    "    y2 = y1 - l2 * np.cos(theta2)\n",
    "\n",
    "    # Compute positions for SINDy predictions\n",
    "    x1_pred = l1 * np.sin(theta1_pred)\n",
    "    y1_pred = -l1 * np.cos(theta1_pred)\n",
    "    x2_pred = x1_pred + l2 * np.sin(theta2_pred)\n",
    "    y2_pred = y1_pred - l2 * np.cos(theta2_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    line_true, = ax.plot([], [], 'o-', lw=2, label='Ground Truth')\n",
    "    line_pred, = ax.plot([], [], 'o--', lw=2, label='SINDy Prediction')\n",
    "    ax.legend()\n",
    "\n",
    "    def update(frame):\n",
    "        line_true.set_data([0, x1[frame], x2[frame]], [0, y1[frame], y2[frame]])\n",
    "        line_pred.set_data([0, x1_pred[frame], x2_pred[frame]], [0, y1_pred[frame], y2_pred[frame]])\n",
    "        return line_true, line_pred\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(theta1), blit=True, interval=50)\n",
    "\n",
    "    if save_path:\n",
    "        ani.save(save_path, fps=20, writer='imagemagick')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Visualize SINDy prediction vs ground truth for two examples\n",
    "visualize_sindy_prediction(test_data[0], sindy_model, scaler, save_path='sindy_vs_ground_truth1.gif')\n",
    "visualize_sindy_prediction(test_data[1], sindy_model, scaler, save_path='sindy_vs_ground_truth2.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
