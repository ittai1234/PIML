{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.integrate import solve_ivp\n",
    "import pickle\n",
    "import pysindy as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the double pendulum dynamics\n",
    "def double_pendulum(t, y, l1, l2, m1, m2, g):\n",
    "    theta1, z1, theta2, z2 = y\n",
    "    delta = theta2 - theta1\n",
    "\n",
    "    # Equations of motion\n",
    "    denom1 = (m1 + m2) * l1 - m2 * l1 * np.cos(delta) ** 2\n",
    "    denom2 = (l2 / l1) * denom1\n",
    "\n",
    "    dydt = np.zeros_like(y)\n",
    "    dydt[0] = z1\n",
    "    dydt[1] = (\n",
    "        (m2 * l1 * z1 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "        + m2 * g * np.sin(theta2) * np.cos(delta)\n",
    "        + m2 * l2 * z2 ** 2 * np.sin(delta)\n",
    "        - (m1 + m2) * g * np.sin(theta1))\n",
    "        / denom1\n",
    "    )\n",
    "    dydt[2] = z2\n",
    "    dydt[3] = (\n",
    "        (-m2 * l2 * z2 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "        + (m1 + m2) * g * np.sin(theta1) * np.cos(delta)\n",
    "        - (m1 + m2) * l1 * z1 ** 2 * np.sin(delta)\n",
    "        - (m1 + m2) * g * np.sin(theta2))\n",
    "        / denom2\n",
    "    )\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datasets size\n",
    "n_pendulums = 1000\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n_pendulums):\n",
    "    # Randomize parameters\n",
    "    l1, l2 = np.random.uniform(0.5, 2.0, 2)\n",
    "    m1, m2 = np.random.uniform(0.5, 2.0, 2)\n",
    "    g = 9.81\n",
    "    y0 = np.random.uniform(-np.pi, np.pi, 4)\n",
    "\n",
    "    # Simulate dynamics\n",
    "    t_span = (0, 10)\n",
    "    t_eval = np.linspace(t_span[0], t_span[1], 100)\n",
    "    sol = solve_ivp(double_pendulum, t_span, y0, t_eval=t_eval, args=(l1, l2, m1, m2, g))\n",
    "    \n",
    "    # Store data\n",
    "    theta1, theta1_dot, theta2, theta2_dot = sol.y\n",
    "    X = np.vstack([theta1, theta2, theta1_dot, theta2_dot]).T\n",
    "    data.append((X, l1, l2, m1, m2, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation, and test sets\n",
    "train_data = data[:800]\n",
    "val_data = data[800:900]\n",
    "test_data = data[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: Train=800, Validation=100, Test=100 examples.\n"
     ]
    }
   ],
   "source": [
    "# Save data as pickle\n",
    "with open('double_pendulum_split_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump({'train': train_data, 'val': val_data, 'test': test_data}, f)\n",
    "\n",
    "print(f\"Dataset saved: Train={len(train_data)}, Validation={len(val_data)}, Test={len(test_data)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PINN structure\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.hidden_layers = [tf.keras.layers.Dense(units, activation='tanh') for units in layers]\n",
    "        self.output_layer = tf.keras.layers.Dense(2)  # Outputs: [theta1_ddot, theta2_ddot]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics-informed loss\n",
    "def physics_loss(y_pred, y_true, params_batch):\n",
    "    l1, l2, m1, m2, g = params_batch\n",
    "    theta1, theta2, theta1_dot, theta2_dot = tf.split(y_true, 4, axis=1)\n",
    "    theta1_ddot, theta2_ddot = tf.split(y_pred, 2, axis=1)\n",
    "\n",
    "    delta = theta2 - theta1\n",
    "    physics_term1 = (\n",
    "        (m2 * l1 * theta1_dot ** 2 * tf.sin(delta) * tf.cos(delta)\n",
    "         + m2 * g * tf.sin(theta2) * tf.cos(delta)\n",
    "         + m2 * l2 * theta2_dot ** 2 * tf.sin(delta)\n",
    "         - (m1 + m2) * g * tf.sin(theta1))\n",
    "        / ((m1 + m2) * l1 - m2 * l1 * tf.cos(delta) ** 2)\n",
    "    )\n",
    "    physics_term2 = (\n",
    "        (-m2 * l2 * theta2_dot ** 2 * tf.sin(delta) * tf.cos(delta)\n",
    "         + (m1 + m2) * g * tf.sin(theta1) * tf.cos(delta)\n",
    "         - (m1 + m2) * l1 * theta1_dot ** 2 * tf.sin(delta)\n",
    "         - (m1 + m2) * g * tf.sin(theta2))\n",
    "        / ((l2 / l1) * ((m1 + m2) * l1 - m2 * l1 * tf.cos(delta) ** 2))\n",
    "    )\n",
    "\n",
    "    loss = (\n",
    "        tf.reduce_mean((theta1_ddot - physics_term1) ** 2)\n",
    "        + tf.reduce_mean((theta2_ddot - physics_term2) ** 2)\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    # Extract accelerations from y_true\n",
    "    theta1_ddot_true, theta2_ddot_true = tf.split(y_true[:, 2:], 2, axis=1)\n",
    "    y_true_acc = tf.concat([theta1_ddot_true, theta2_ddot_true], axis=1)\n",
    "    \n",
    "    # Compute accuracy using only the accelerations\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true_acc) / (tf.abs(y_true_acc) + 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open('double_pendulum_split_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training, validation, and test data\n",
    "def prepare_data(data):\n",
    "    X = np.vstack([example[0] for example in data]).astype(np.float32)\n",
    "    y = X  # All state variables as targets\n",
    "    params = np.array([example[1:] for example in data], dtype=np.float32).T\n",
    "    return X, y, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, params_train = prepare_data(dataset['train'])\n",
    "X_val, y_val, params_val = prepare_data(dataset['val'])\n",
    "X_test, y_test, params_test = prepare_data(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the PINN\n",
    "pinn = PINN([64, 64, 64])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: nan, Train Accuracy: 4.854622\n",
      "Validation Loss: nan, Validation Accuracy: 4.545644\n",
      "Epoch 2/50\n",
      "Train Loss: nan, Train Accuracy: 9.962829\n",
      "Validation Loss: nan, Validation Accuracy: 11.020790\n",
      "Epoch 3/50\n",
      "Train Loss: nan, Train Accuracy: 11.500507\n",
      "Validation Loss: nan, Validation Accuracy: 11.364056\n",
      "Epoch 4/50\n",
      "Train Loss: nan, Train Accuracy: 14.723288\n",
      "Validation Loss: nan, Validation Accuracy: 15.541641\n",
      "Epoch 5/50\n",
      "Train Loss: nan, Train Accuracy: 14.853122\n",
      "Validation Loss: nan, Validation Accuracy: 14.848695\n",
      "Epoch 6/50\n",
      "Train Loss: nan, Train Accuracy: 15.645617\n",
      "Validation Loss: nan, Validation Accuracy: 13.691628\n",
      "Epoch 7/50\n",
      "Train Loss: nan, Train Accuracy: 16.069534\n",
      "Validation Loss: nan, Validation Accuracy: 14.775350\n",
      "Epoch 8/50\n",
      "Train Loss: nan, Train Accuracy: 17.146450\n",
      "Validation Loss: nan, Validation Accuracy: 15.701552\n",
      "Epoch 9/50\n",
      "Train Loss: nan, Train Accuracy: 16.774979\n",
      "Validation Loss: nan, Validation Accuracy: 14.582520\n",
      "Epoch 10/50\n",
      "Train Loss: nan, Train Accuracy: 18.560049\n",
      "Validation Loss: nan, Validation Accuracy: 16.860842\n",
      "Epoch 11/50\n",
      "Train Loss: nan, Train Accuracy: 17.949978\n",
      "Validation Loss: nan, Validation Accuracy: 15.198090\n",
      "Epoch 12/50\n",
      "Train Loss: nan, Train Accuracy: 22.020086\n",
      "Validation Loss: nan, Validation Accuracy: 18.745064\n",
      "Epoch 13/50\n",
      "Train Loss: nan, Train Accuracy: 24.713673\n",
      "Validation Loss: nan, Validation Accuracy: 22.463894\n",
      "Epoch 14/50\n",
      "Train Loss: nan, Train Accuracy: 21.556686\n",
      "Validation Loss: nan, Validation Accuracy: 17.717787\n",
      "Epoch 15/50\n",
      "Train Loss: nan, Train Accuracy: 17.636646\n",
      "Validation Loss: nan, Validation Accuracy: 14.249744\n",
      "Epoch 16/50\n",
      "Train Loss: nan, Train Accuracy: 18.455595\n",
      "Validation Loss: nan, Validation Accuracy: 15.073420\n",
      "Epoch 17/50\n",
      "Train Loss: nan, Train Accuracy: 17.014694\n",
      "Validation Loss: nan, Validation Accuracy: 13.370574\n",
      "Epoch 18/50\n",
      "Train Loss: nan, Train Accuracy: 21.508184\n",
      "Validation Loss: nan, Validation Accuracy: 18.047424\n",
      "Epoch 19/50\n",
      "Train Loss: nan, Train Accuracy: 21.906530\n",
      "Validation Loss: nan, Validation Accuracy: 18.845959\n",
      "Epoch 20/50\n",
      "Train Loss: nan, Train Accuracy: 27.069832\n",
      "Validation Loss: nan, Validation Accuracy: 22.699348\n",
      "Epoch 21/50\n",
      "Train Loss: nan, Train Accuracy: 26.145479\n",
      "Validation Loss: nan, Validation Accuracy: 22.000031\n",
      "Epoch 22/50\n",
      "Train Loss: nan, Train Accuracy: 23.800558\n",
      "Validation Loss: nan, Validation Accuracy: 20.181898\n",
      "Epoch 23/50\n",
      "Train Loss: nan, Train Accuracy: 24.000542\n",
      "Validation Loss: nan, Validation Accuracy: 20.264048\n",
      "Epoch 24/50\n",
      "Train Loss: nan, Train Accuracy: 24.941088\n",
      "Validation Loss: nan, Validation Accuracy: 20.302872\n",
      "Epoch 25/50\n",
      "Train Loss: nan, Train Accuracy: 23.389692\n",
      "Validation Loss: nan, Validation Accuracy: 23.465458\n",
      "Epoch 26/50\n",
      "Train Loss: nan, Train Accuracy: 26.590998\n",
      "Validation Loss: nan, Validation Accuracy: 22.078377\n",
      "Epoch 27/50\n",
      "Train Loss: nan, Train Accuracy: 21.463678\n",
      "Validation Loss: nan, Validation Accuracy: 18.348549\n",
      "Epoch 28/50\n",
      "Train Loss: nan, Train Accuracy: 24.275770\n",
      "Validation Loss: nan, Validation Accuracy: 21.382067\n",
      "Epoch 29/50\n",
      "Train Loss: nan, Train Accuracy: 22.573591\n",
      "Validation Loss: nan, Validation Accuracy: 20.688559\n",
      "Epoch 30/50\n",
      "Train Loss: nan, Train Accuracy: 24.930290\n",
      "Validation Loss: nan, Validation Accuracy: 21.740959\n",
      "Epoch 31/50\n",
      "Train Loss: nan, Train Accuracy: 29.812807\n",
      "Validation Loss: nan, Validation Accuracy: 26.903889\n",
      "Epoch 32/50\n",
      "Train Loss: nan, Train Accuracy: 29.409643\n",
      "Validation Loss: nan, Validation Accuracy: 25.523735\n",
      "Epoch 33/50\n",
      "Train Loss: nan, Train Accuracy: 33.260136\n",
      "Validation Loss: nan, Validation Accuracy: 28.531755\n",
      "Epoch 34/50\n",
      "Train Loss: nan, Train Accuracy: 36.977314\n",
      "Validation Loss: nan, Validation Accuracy: 31.158609\n",
      "Epoch 35/50\n",
      "Train Loss: nan, Train Accuracy: 30.036453\n",
      "Validation Loss: nan, Validation Accuracy: 26.239410\n",
      "Epoch 36/50\n",
      "Train Loss: nan, Train Accuracy: 30.393749\n",
      "Validation Loss: nan, Validation Accuracy: 29.388428\n",
      "Epoch 37/50\n",
      "Train Loss: nan, Train Accuracy: 31.831301\n",
      "Validation Loss: nan, Validation Accuracy: 28.741098\n",
      "Epoch 38/50\n",
      "Train Loss: nan, Train Accuracy: 31.415516\n",
      "Validation Loss: nan, Validation Accuracy: 29.868587\n",
      "Epoch 39/50\n",
      "Train Loss: nan, Train Accuracy: 27.385790\n",
      "Validation Loss: nan, Validation Accuracy: 26.387018\n",
      "Epoch 40/50\n",
      "Train Loss: nan, Train Accuracy: 28.177425\n",
      "Validation Loss: nan, Validation Accuracy: 28.553814\n",
      "Epoch 41/50\n",
      "Train Loss: nan, Train Accuracy: 23.470015\n",
      "Validation Loss: nan, Validation Accuracy: 21.950941\n",
      "Epoch 42/50\n",
      "Train Loss: nan, Train Accuracy: 27.923256\n",
      "Validation Loss: nan, Validation Accuracy: 27.183064\n",
      "Epoch 43/50\n",
      "Train Loss: nan, Train Accuracy: 23.946157\n",
      "Validation Loss: nan, Validation Accuracy: 23.033072\n",
      "Epoch 44/50\n",
      "Train Loss: nan, Train Accuracy: 31.003181\n",
      "Validation Loss: nan, Validation Accuracy: 26.535093\n",
      "Epoch 45/50\n",
      "Train Loss: nan, Train Accuracy: 20.897181\n",
      "Validation Loss: nan, Validation Accuracy: 16.760492\n",
      "Epoch 46/50\n",
      "Train Loss: nan, Train Accuracy: 22.695635\n",
      "Validation Loss: nan, Validation Accuracy: 23.288202\n",
      "Epoch 47/50\n",
      "Train Loss: nan, Train Accuracy: 24.806374\n",
      "Validation Loss: nan, Validation Accuracy: 23.735430\n",
      "Epoch 48/50\n",
      "Train Loss: nan, Train Accuracy: 26.283234\n",
      "Validation Loss: nan, Validation Accuracy: 24.073418\n",
      "Epoch 49/50\n",
      "Train Loss: nan, Train Accuracy: 25.085859\n",
      "Validation Loss: nan, Validation Accuracy: 22.613508\n",
      "Epoch 50/50\n",
      "Train Loss: nan, Train Accuracy: 24.757122\n",
      "Validation Loss: nan, Validation Accuracy: 25.536110\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = tf.constant(X_train[i:i + batch_size], dtype=tf.float32)  # Convert to tf.float32\n",
    "        y_batch = tf.constant(y_train[i:i + batch_size], dtype=tf.float32)  # Convert to tf.float32\n",
    "        params_batch = [tf.constant(param[i:i + batch_size], dtype=tf.float32) for param in params_train]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = pinn(X_batch)\n",
    "            loss = physics_loss(y_pred, y_batch, params_batch)\n",
    "\n",
    "        gradients = tape.gradient(loss, pinn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, pinn.trainable_variables))\n",
    "\n",
    "        # Append training loss and accuracy\n",
    "        train_losses.append(loss.numpy())\n",
    "        train_accuracies.append(compute_accuracy(y_pred, y_batch).numpy())\n",
    "\n",
    "    # Validation\n",
    "    for i in range(0, len(X_val), batch_size):\n",
    "        X_batch = X_val[i:i + batch_size]\n",
    "        y_batch = y_val[i:i + batch_size]\n",
    "        params_batch = [tf.constant(param[i:i + batch_size], dtype=tf.float32) for param in params_val]\n",
    "\n",
    "        y_pred = pinn(X_batch)\n",
    "        loss = physics_loss(y_pred, y_batch, params_batch)\n",
    "\n",
    "        val_losses.append(loss.numpy())\n",
    "        val_accuracies.append(compute_accuracy(y_pred, y_batch).numpy())\n",
    "\n",
    "    print(f\"Train Loss: {np.mean(train_losses):.6f}, Train Accuracy: {np.mean(train_accuracies):.6f}\")\n",
    "    print(f\"Validation Loss: {np.mean(val_losses):.6f}, Validation Accuracy: {np.mean(val_accuracies):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 18.302931\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "test_accuracies = []\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test[i:i + batch_size]\n",
    "    y_batch = y_test[i:i + batch_size]\n",
    "    params_batch = [tf.constant(param[i:i + batch_size], dtype=tf.float32) for param in params_test]\n",
    "\n",
    "    y_pred = pinn(X_batch)\n",
    "    test_accuracies.append(compute_accuracy(y_pred, y_batch).numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracies):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('double_pendulum_split_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "def prepare_data(data):\n",
    "    X = np.vstack([example[0] for example in data])\n",
    "    y = X[:, 2:]  # Second derivatives as targets\n",
    "    params = np.array([example[1:] for example in data])\n",
    "    return X, y, params.T\n",
    "\n",
    "X_test, y_test, params_test = prepare_data(dataset['test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINDy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sparse Identification of Nonlinear Dynamics (SINDy)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sindy \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mSINDy()\n\u001b[1;32m----> 3\u001b[0m sindy\u001b[38;5;241m.\u001b[39mfit(X, t\u001b[38;5;241m=\u001b[39m\u001b[43mt\u001b[49m)\n\u001b[0;32m      4\u001b[0m sindy\u001b[38;5;241m.\u001b[39mprint()\n",
      "\u001b[1;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "# Sparse Identification of Nonlinear Dynamics (SINDy)\n",
    "sindy = ps.SINDy()\n",
    "sindy.fit(X, t=t)\n",
    "sindy.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load the PINN model\n",
    "pinn = tf.keras.models.load_model('pinn_double_pendulum_model.h5')\n",
    "print(\"PINN model loaded for SINDy analysis.\")\n",
    "\n",
    "# Generate predictions with the PINN (optional step)\n",
    "y_pred_pinn = pinn(X_test).numpy()  # Predictions for test data\n",
    "\n",
    "# Use SINDy for sparse identification\n",
    "sindy = ps.SINDy()\n",
    "\n",
    "# Fit the SINDy model using the test data\n",
    "sindy.fit(X_test, t=np.linspace(0, 10, 100))\n",
    "print(\"SINDy model trained successfully.\")\n",
    "sindy.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the SINDy model on test data\n",
    "sindy_score = sindy.score(X_test, t=np.linspace(0, 10, 100))\n",
    "print(f\"SINDy Model Score (R²): {sindy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SINDy model\n",
    "import joblib\n",
    "joblib.dump(sindy, 'sindy_double_pendulum.pkl')\n",
    "print(\"SINDy model saved as 'sindy_double_pendulum.pkl'.\")\n",
    "\n",
    "# Compute accuracy between SINDy predictions and PINN outputs\n",
    "test_accuracies = []\n",
    "for i in range(len(X_test)):\n",
    "    sindy_pred = sindy.simulate(X_test[i], t=np.linspace(0, 10, 100))\n",
    "    pinn_accuracy = np.mean(np.abs(sindy_pred - y_pred_pinn[i]) / (np.abs(y_pred_pinn[i]) + 1e-6))\n",
    "    test_accuracies.append(pinn_accuracy)\n",
    "\n",
    "print(f\"SINDy vs. PINN Test Accuracy: {np.mean(test_accuracies):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and visualize ground truth vs predictions\n",
    "test_accuracies = []\n",
    "for i in range(len(X_test)):\n",
    "    sindy_pred = sindy.simulate(X_test[i], t=np.linspace(0, 10, 100))\n",
    "    pinn_accuracy = np.mean(np.abs(sindy_pred - y_pred_pinn[i]) / (np.abs(y_pred_pinn[i]) + 1e-6))\n",
    "    test_accuracies.append(pinn_accuracy)\n",
    "\n",
    "    # Plot ground truth vs predictions for the first 5 examples\n",
    "    if i < 5:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Ground truth\n",
    "        plt.plot(X_test[i], label=\"Ground Truth\", linestyle='-', linewidth=2)\n",
    "\n",
    "        # SINDy predictions\n",
    "        plt.plot(sindy_pred, label=\"SINDy Prediction\", linestyle='--')\n",
    "\n",
    "    \n",
    "\n",
    "print(\"prediction & model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
