{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.integrate import solve_ivp\n",
    "import pickle\n",
    "import pysindy as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the double pendulum dynamics\n",
    "def double_pendulum(t, y, l1, l2, m1, m2, g):\n",
    "    theta1, z1, theta2, z2 = y\n",
    "    delta = theta2 - theta1\n",
    "\n",
    "    # Equations of motion\n",
    "    denom1 = (m1 + m2) * l1 - m2 * l1 * np.cos(delta) ** 2\n",
    "    denom2 = (l2 / l1) * denom1\n",
    "\n",
    "    dydt = np.zeros_like(y)\n",
    "    dydt[0] = z1\n",
    "    dydt[1] = (\n",
    "        (m2 * l1 * z1 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "        + m2 * g * np.sin(theta2) * np.cos(delta)\n",
    "        + m2 * l2 * z2 ** 2 * np.sin(delta)\n",
    "        - (m1 + m2) * g * np.sin(theta1))\n",
    "        / denom1\n",
    "    )\n",
    "    dydt[2] = z2\n",
    "    dydt[3] = (\n",
    "        (-m2 * l2 * z2 ** 2 * np.sin(delta) * np.cos(delta)\n",
    "        + (m1 + m2) * g * np.sin(theta1) * np.cos(delta)\n",
    "        - (m1 + m2) * l1 * z1 ** 2 * np.sin(delta)\n",
    "        - (m1 + m2) * g * np.sin(theta2))\n",
    "        / denom2\n",
    "    )\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datasets size\n",
    "n_pendulums = 1000\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n_pendulums):\n",
    "    # Randomize parameters\n",
    "    l1, l2 = np.random.uniform(0.5, 2.0, 2)\n",
    "    m1, m2 = np.random.uniform(0.5, 2.0, 2)\n",
    "    g = 9.81\n",
    "    y0 = np.random.uniform(-np.pi, np.pi, 4)\n",
    "\n",
    "    # Simulate dynamics\n",
    "    t_span = (0, 10)\n",
    "    t_eval = np.linspace(t_span[0], t_span[1], 100)\n",
    "    sol = solve_ivp(double_pendulum, t_span, y0, t_eval=t_eval, args=(l1, l2, m1, m2, g))\n",
    "    \n",
    "    # Store data\n",
    "    theta1, theta1_dot, theta2, theta2_dot = sol.y\n",
    "    X = np.vstack([theta1, theta2, theta1_dot, theta2_dot]).T\n",
    "    data.append((X, l1, l2, m1, m2, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation, and test sets\n",
    "train_data = data[:800]\n",
    "val_data = data[800:900]\n",
    "test_data = data[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as pickle\n",
    "with open('double_pendulum_split_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump({'train': train_data, 'val': val_data, 'test': test_data}, f)\n",
    "\n",
    "print(f\"Dataset saved: Train={len(train_data)}, Validation={len(val_data)}, Test={len(test_data)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PINN structure\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.hidden_layers = [tf.keras.layers.Dense(units, activation='tanh') for units in layers]\n",
    "        self.output_layer = tf.keras.layers.Dense(2)  # Outputs: [theta1_ddot, theta2_ddot]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics-informed loss\n",
    "def physics_loss(y_pred, y_true, params_batch):\n",
    "    l1, l2, m1, m2, g = params_batch\n",
    "    theta1, theta2, theta1_dot, theta2_dot = tf.split(y_true, 4, axis=1)\n",
    "    theta1_ddot, theta2_ddot = tf.split(y_pred, 2, axis=1)\n",
    "\n",
    "    delta = theta2 - theta1\n",
    "    physics_term1 = (\n",
    "        (m2 * l1 * theta1_dot ** 2 * tf.sin(delta) * tf.cos(delta)\n",
    "         + m2 * g * tf.sin(theta2) * tf.cos(delta)\n",
    "         + m2 * l2 * theta2_dot ** 2 * tf.sin(delta)\n",
    "         - (m1 + m2) * g * tf.sin(theta1))\n",
    "        / ((m1 + m2) * l1 - m2 * l1 * tf.cos(delta) ** 2)\n",
    "    )\n",
    "    physics_term2 = (\n",
    "        (-m2 * l2 * theta2_dot ** 2 * tf.sin(delta) * tf.cos(delta)\n",
    "         + (m1 + m2) * g * tf.sin(theta1) * tf.cos(delta)\n",
    "         - (m1 + m2) * l1 * theta1_dot ** 2 * tf.sin(delta)\n",
    "         - (m1 + m2) * g * tf.sin(theta2))\n",
    "        / ((l2 / l1) * ((m1 + m2) * l1 - m2 * l1 * tf.cos(delta) ** 2))\n",
    "    )\n",
    "\n",
    "    loss = (\n",
    "        tf.reduce_mean((theta1_ddot - physics_term1) ** 2)\n",
    "        + tf.reduce_mean((theta2_ddot - physics_term2) ** 2)\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    # Extract accelerations from y_true\n",
    "    theta1_ddot_true, theta2_ddot_true = tf.split(y_true[:, 2:], 2, axis=1)\n",
    "    y_true_acc = tf.concat([theta1_ddot_true, theta2_ddot_true], axis=1)\n",
    "    \n",
    "    # Compute accuracy using only the accelerations\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true_acc) / (tf.abs(y_true_acc) + 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open('double_pendulum_split_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training, validation, and test data\n",
    "def prepare_data(data):\n",
    "    X = np.vstack([example[0] for example in data]).astype(np.float32)\n",
    "    y = X  # All state variables as targets\n",
    "    params = np.array([example[1:] for example in data], dtype=np.float32).T\n",
    "    return X, y, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, params_train = prepare_data(dataset['train'])\n",
    "X_val, y_val, params_val = prepare_data(dataset['val'])\n",
    "X_test, y_test, params_test = prepare_data(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the PINN\n",
    "pinn = PINN([64, 64, 64])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = tf.constant(X_train[i:i + batch_size], dtype=tf.float32)  # Convert to tf.float32\n",
    "        y_batch = tf.constant(y_train[i:i + batch_size], dtype=tf.float32)  # Convert to tf.float32\n",
    "        params_batch = [tf.constant(param[i:i + batch_size], dtype=tf.float32) for param in params_train]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = pinn(X_batch)\n",
    "            loss = physics_loss(y_pred, y_batch, params_batch)\n",
    "\n",
    "        gradients = tape.gradient(loss, pinn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, pinn.trainable_variables))\n",
    "\n",
    "        # Append training loss and accuracy\n",
    "        train_losses.append(loss.numpy())\n",
    "        train_accuracies.append(compute_accuracy(y_pred, y_batch).numpy())\n",
    "\n",
    "    # Validation\n",
    "    for i in range(0, len(X_val), batch_size):\n",
    "        X_batch = X_val[i:i + batch_size]\n",
    "        y_batch = y_val[i:i + batch_size]\n",
    "        params_batch = [tf.constant(param[i:i + batch_size], dtype=tf.float32) for param in params_val]\n",
    "\n",
    "        y_pred = pinn(X_batch)\n",
    "        loss = physics_loss(y_pred, y_batch, params_batch)\n",
    "\n",
    "        val_losses.append(loss.numpy())\n",
    "        val_accuracies.append(compute_accuracy(y_pred, y_batch).numpy())\n",
    "\n",
    "    print(f\"Train Loss: {np.mean(train_losses):.6f}, Train Accuracy: {np.mean(train_accuracies):.6f}\")\n",
    "    print(f\"Validation Loss: {np.mean(val_losses):.6f}, Validation Accuracy: {np.mean(val_accuracies):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "test_accuracies = []\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test[i:i + batch_size]\n",
    "    y_batch = y_test[i:i + batch_size]\n",
    "    params_batch = [tf.constant(param[i:i + batch_size], dtype=tf.float32) for param in params_test]\n",
    "\n",
    "    y_pred = pinn(X_batch)\n",
    "    test_accuracies.append(compute_accuracy(y_pred, y_batch).numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracies):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('double_pendulum_split_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "def prepare_data(data):\n",
    "    X = np.vstack([example[0] for example in data])\n",
    "    y = X[:, 2:]  # Second derivatives as targets\n",
    "    params = np.array([example[1:] for example in data])\n",
    "    return X, y, params.T\n",
    "\n",
    "X_test, y_test, params_test = prepare_data(dataset['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Identification of Nonlinear Dynamics (SINDy)\n",
    "sindy = ps.SINDy()\n",
    "sindy.fit(X, t=t)\n",
    "sindy.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load the PINN model\n",
    "pinn = tf.keras.models.load_model('pinn_double_pendulum_model.h5')\n",
    "print(\"PINN model loaded for SINDy analysis.\")\n",
    "\n",
    "# Generate predictions with the PINN (optional step)\n",
    "y_pred_pinn = pinn(X_test).numpy()  # Predictions for test data\n",
    "\n",
    "# Use SINDy for sparse identification\n",
    "sindy = ps.SINDy()\n",
    "\n",
    "# Fit the SINDy model using the test data\n",
    "sindy.fit(X_test, t=np.linspace(0, 10, 100))\n",
    "print(\"SINDy model trained successfully.\")\n",
    "sindy.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the SINDy model on test data\n",
    "sindy_score = sindy.score(X_test, t=np.linspace(0, 10, 100))\n",
    "print(f\"SINDy Model Score (RÂ²): {sindy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SINDy model\n",
    "import joblib\n",
    "joblib.dump(sindy, 'sindy_double_pendulum.pkl')\n",
    "print(\"SINDy model saved as 'sindy_double_pendulum.pkl'.\")\n",
    "\n",
    "# Compute accuracy between SINDy predictions and PINN outputs\n",
    "test_accuracies = []\n",
    "for i in range(len(X_test)):\n",
    "    sindy_pred = sindy.simulate(X_test[i], t=np.linspace(0, 10, 100))\n",
    "    pinn_accuracy = np.mean(np.abs(sindy_pred - y_pred_pinn[i]) / (np.abs(y_pred_pinn[i]) + 1e-6))\n",
    "    test_accuracies.append(pinn_accuracy)\n",
    "\n",
    "print(f\"SINDy vs. PINN Test Accuracy: {np.mean(test_accuracies):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and visualize ground truth vs predictions\n",
    "test_accuracies = []\n",
    "for i in range(len(X_test)):\n",
    "    sindy_pred = sindy.simulate(X_test[i], t=np.linspace(0, 10, 100))\n",
    "    pinn_accuracy = np.mean(np.abs(sindy_pred - y_pred_pinn[i]) / (np.abs(y_pred_pinn[i]) + 1e-6))\n",
    "    test_accuracies.append(pinn_accuracy)\n",
    "\n",
    "    # Plot ground truth vs predictions for the first 5 examples\n",
    "    if i < 5:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Ground truth\n",
    "        plt.plot(X_test[i], label=\"Ground Truth\", linestyle='-', linewidth=2)\n",
    "\n",
    "        # SINDy predictions\n",
    "        plt.plot(sindy_pred, label=\"SINDy Prediction\", linestyle='--')\n",
    "\n",
    "    \n",
    "\n",
    "print(\"prediction & model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
