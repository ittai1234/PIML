{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from scipy.integrate import odeint\n",
    "from scipy.special import binom\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pendulum_data(n_ics, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates a dataset where each input consists of a sequence of images\n",
    "    representing a pendulum's motion over time.\n",
    "\n",
    "    Args:\n",
    "        n_ics (int): Number of different pendulum trajectories (initial conditions).\n",
    "        sequence_length (int): Number of time steps in each sequence.\n",
    "\n",
    "    Returns:\n",
    "        data (dict): Dictionary containing sequences of images, velocities,\n",
    "                     accelerations, and latent variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define pendulum dynamics: θ'' = -sin(θ)\n",
    "    f = lambda z, t: [z[1], -np.sin(z[0])]\n",
    "\n",
    "    # Define time grid\n",
    "    t = np.arange(0, 10, .02)  # Total time steps\n",
    "    T = len(t)  # Number of total time steps\n",
    "\n",
    "    # Pre-allocate arrays for state variables\n",
    "    z = np.zeros((n_ics, T, 2))  # (n_ics, time_steps, [θ, ω])\n",
    "    dz = np.zeros(z.shape)       # (n_ics, time_steps, [dθ/dt, dω/dt])\n",
    "\n",
    "    # Define random initial conditions for the pendulum\n",
    "    z1range = np.array([-np.pi, np.pi])\n",
    "    z2range = np.array([-1.8, 1.8])\n",
    "\n",
    "    i = 0\n",
    "    while i < n_ics:\n",
    "        z0 = np.array([\n",
    "            (z1range[1] - z1range[0]) * np.random.rand() + z1range[0],  # Random θ\n",
    "            (z2range[1] - z2range[0]) * np.random.rand() + z2range[0]   # Random ω\n",
    "        ])\n",
    "\n",
    "        # Energy constraint check (to ensure valid oscillation behavior)\n",
    "        if np.abs(z0[1]**2 / 2. - np.cos(z0[0])) > 0.99:\n",
    "            continue\n",
    "\n",
    "        # Solve pendulum dynamics\n",
    "        z[i] = odeint(f, z0, t)\n",
    "        dz[i] = np.array([f(z[i, j], t[j]) for j in range(T)])\n",
    "        i += 1\n",
    "\n",
    "    # Convert pendulum motion into image sequences\n",
    "    x, dx, ddx = pendulum_to_movie(z, dz)\n",
    "\n",
    "    # Reshape into sequence format\n",
    "    num_sequences = T - sequence_length + 1\n",
    "    dataset = {\n",
    "        'x': np.array([x[:, i:i+sequence_length] for i in range(num_sequences)]),\n",
    "        'dx': np.array([dx[:, i:i+sequence_length] for i in range(num_sequences)]),\n",
    "        'ddx': np.array([ddx[:, i:i+sequence_length] for i in range(num_sequences)]),\n",
    "        'z': np.array([z[:, i:i+sequence_length, 0] for i in range(num_sequences)]),\n",
    "        'dz': np.array([z[:, i:i+sequence_length, 1] for i in range(num_sequences)])\n",
    "    }\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def pendulum_to_movie(z, dz):\n",
    "    \"\"\"\n",
    "    Converts pendulum motion into images.\n",
    "\n",
    "    Args:\n",
    "        z (numpy.ndarray): The pendulum state variables (θ, ω).\n",
    "        dz (numpy.ndarray): The time derivatives of the state variables.\n",
    "\n",
    "    Returns:\n",
    "        x, dx, ddx (numpy.ndarray): Image sequences representing position, velocity, and acceleration.\n",
    "    \"\"\"\n",
    "\n",
    "    n_ics, T, _ = z.shape  # Number of initial conditions and time steps\n",
    "    n = 51  # Image size\n",
    "\n",
    "    # Define a 2D mesh grid for creating images\n",
    "    y1, y2 = np.meshgrid(np.linspace(-1.5, 1.5, n), np.linspace(1.5, -1.5, n))\n",
    "\n",
    "    # Functions to generate images\n",
    "    create_image = lambda theta: np.exp(-((y1 - np.cos(theta - np.pi/2))**2 + (y2 - np.sin(theta - np.pi/2))**2) / .05)\n",
    "    argument_derivative = lambda theta, dtheta: -1/.05 * (\n",
    "        2 * (y1 - np.cos(theta - np.pi/2)) * np.sin(theta - np.pi/2) * dtheta +\n",
    "        2 * (y2 - np.sin(theta - np.pi/2)) * (-np.cos(theta - np.pi/2)) * dtheta\n",
    "    )\n",
    "\n",
    "    # Allocate memory for images\n",
    "    x = np.zeros((n_ics, T, n, n))\n",
    "    dx = np.zeros((n_ics, T, n, n))\n",
    "    ddx = np.zeros((n_ics, T, n, n))\n",
    "\n",
    "    for i in range(n_ics):\n",
    "        for j in range(T):\n",
    "            x[i, j] = create_image(z[i, j, 0])\n",
    "            dx[i, j] = create_image(z[i, j, 0]) * argument_derivative(z[i, j, 0], dz[i, j, 0])\n",
    "            ddx[i, j] = create_image(z[i, j, 0]) * (argument_derivative(z[i, j, 0], dz[i, j, 0])**2)\n",
    "\n",
    "    return x, dx, ddx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict): Dictionary containing 'x', 'dx', 'ddx', 'z', 'dz' sequences.\n",
    "        transform (callable, optional): Optional transform to apply to samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.x = data['x']      # Shape: (num_sequences, n_ics, seq_len, 51, 51)\n",
    "        self.dx = data['dx']\n",
    "        self.ddx = data['ddx']\n",
    "        self.z = data['z']      # Latent representation (θ)\n",
    "        self.dz = data['dz']    # Latent velocity (ω)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Reshape data: (num_sequences, n_ics, seq_len, height, width) → (num_samples, seq_len, 1, height, width)\n",
    "        self.x = self.x.reshape(-1, self.x.shape[2], 1, self.x.shape[3], self.x.shape[4])\n",
    "        self.dx = self.dx.reshape(-1, self.dx.shape[2], 1, self.dx.shape[3], self.dx.shape[4])\n",
    "        self.ddx = self.ddx.reshape(-1, self.ddx.shape[2], 1, self.ddx.shape[3], self.ddx.shape[4])\n",
    "        self.z = self.z.reshape(-1, self.z.shape[2], 1)\n",
    "        self.dz = self.dz.reshape(-1, self.dz.shape[2], 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'x': torch.tensor(self.x[idx], dtype=torch.float32),   # Shape: (seq_len, 1, 51, 51)\n",
    "            'dx': torch.tensor(self.dx[idx], dtype=torch.float32),\n",
    "            'ddx': torch.tensor(self.ddx[idx], dtype=torch.float32),\n",
    "            'z': torch.tensor(self.z[idx], dtype=torch.float32),   # Shape: (seq_len, 1)\n",
    "            'dz': torch.tensor(self.dz[idx], dtype=torch.float32)  # Shape: (seq_len, 1)\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def library_size(n, poly_order, use_sine=False, include_constant=True):\n",
    "    \"\"\"\n",
    "    Compute the size of the SINDy library for a given number of variables and polynomial order.\n",
    "    \"\"\"\n",
    "    l = 0\n",
    "    for k in range(poly_order + 1):\n",
    "        l += int(binom(n + k - 1, k))  # Binomial coefficient for polynomial terms\n",
    "    if use_sine:\n",
    "        l += n  # Add sine terms\n",
    "    if not include_constant:\n",
    "        l -= 1  # Remove constant term if needed\n",
    "    return l\n",
    "\n",
    "\n",
    "def sindy_library(X, poly_order, include_sine=False):\n",
    "    \"\"\"\n",
    "    Build the SINDy library for a sequence-based dataset.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Input data of shape (num_samples, num_features).\n",
    "        poly_order (int): Maximum polynomial order to include.\n",
    "        include_sine (bool): Whether to include sine terms.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The constructed SINDy library.\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # Number of samples, number of features\n",
    "    l = library_size(n, poly_order, include_sine, True)  # Compute library size\n",
    "    library = np.ones((m, l))  # Initialize library with constant term\n",
    "    index = 1\n",
    "\n",
    "    # First-order terms\n",
    "    for i in range(n):\n",
    "        library[:, index] = X[:, i]\n",
    "        index += 1\n",
    "\n",
    "    # Higher-order polynomial terms\n",
    "    if poly_order > 1:\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                library[:, index] = X[:, i] * X[:, j]\n",
    "                index += 1\n",
    "\n",
    "    if poly_order > 2:\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                for k in range(j, n):\n",
    "                    library[:, index] = X[:, i] * X[:, j] * X[:, k]\n",
    "                    index += 1\n",
    "\n",
    "    if poly_order > 3:\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                for k in range(j, n):\n",
    "                    for q in range(k, n):\n",
    "                        library[:, index] = X[:, i] * X[:, j] * X[:, k] * X[:, q]\n",
    "                        index += 1\n",
    "\n",
    "    # Sine terms\n",
    "    if include_sine:\n",
    "        for i in range(n):\n",
    "            library[:, index] = np.sin(X[:, i])\n",
    "            index += 1\n",
    "\n",
    "    return library\n",
    "\n",
    "\n",
    "def sindy_fit(RHS, LHS, coefficient_threshold):\n",
    "    \"\"\"\n",
    "    Solve the sparse regression problem to discover governing equations.\n",
    "\n",
    "    Args:\n",
    "        RHS (numpy.ndarray): The SINDy library matrix.\n",
    "        LHS (numpy.ndarray): The derivatives to fit.\n",
    "        coefficient_threshold (float): The threshold for coefficient pruning.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The discovered sparse coefficient matrix.\n",
    "    \"\"\"\n",
    "    m, n = LHS.shape\n",
    "    Xi = np.linalg.lstsq(RHS, LHS, rcond=None)[0]  # Initial least squares fit\n",
    "\n",
    "    # Sequential Thresholding\n",
    "    for k in range(10):\n",
    "        small_inds = (np.abs(Xi) < coefficient_threshold)\n",
    "        Xi[small_inds] = 0  # Zero out small coefficients\n",
    "        for i in range(n):\n",
    "            big_inds = ~small_inds[:, i]\n",
    "            if np.where(big_inds)[0].size == 0:\n",
    "                continue\n",
    "            Xi[big_inds, i] = np.linalg.lstsq(RHS[:, big_inds], LHS[:, i], rcond=None)[0]\n",
    "\n",
    "    return Xi\n",
    "\n",
    "\n",
    "def sindy_simulate(x0, t, Xi, poly_order, include_sine):\n",
    "    \"\"\"\n",
    "    Simulate a discovered SINDy model forward in time.\n",
    "\n",
    "    Args:\n",
    "        x0 (numpy.ndarray): Initial state.\n",
    "        t (numpy.ndarray): Time vector.\n",
    "        Xi (numpy.ndarray): Discovered SINDy coefficients.\n",
    "        poly_order (int): Polynomial order used in SINDy.\n",
    "        include_sine (bool): Whether sine terms were included.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Simulated state trajectory.\n",
    "    \"\"\"\n",
    "    m = t.size\n",
    "    n = x0.size\n",
    "    f = lambda x, t: np.dot(sindy_library(np.array(x).reshape((1, n)), poly_order, include_sine), Xi).reshape((n,))\n",
    "\n",
    "    x = odeint(f, x0, t)\n",
    "    return x\n",
    "\n",
    "\n",
    "def sindy_library_sequence(X_seq, poly_order, include_sine=False):\n",
    "    \"\"\"\n",
    "    Build the SINDy library for a **sequence** of time-series data.\n",
    "\n",
    "    Args:\n",
    "        X_seq (numpy.ndarray): Input data of shape (num_sequences, sequence_length, num_features).\n",
    "        poly_order (int): Maximum polynomial order to include.\n",
    "        include_sine (bool): Whether to include sine terms.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The constructed SINDy library.\n",
    "    \"\"\"\n",
    "    num_sequences, sequence_length, num_features = X_seq.shape\n",
    "\n",
    "    # Reshape into a 2D format\n",
    "    X_flat = X_seq.reshape(num_sequences * sequence_length, num_features)\n",
    "\n",
    "    # Construct the library\n",
    "    library_flat = sindy_library(X_flat, poly_order, include_sine)\n",
    "\n",
    "    # Reshape back to sequence format\n",
    "    library = library_flat.reshape(num_sequences, sequence_length, -1)\n",
    "\n",
    "    return library\n",
    "\n",
    "\n",
    "def sindy_fit_sequence(RHS_seq, LHS_seq, coefficient_threshold):\n",
    "    \"\"\"\n",
    "    Solve the sparse regression problem for **sequence-based data**.\n",
    "\n",
    "    Args:\n",
    "        RHS_seq (numpy.ndarray): The SINDy library matrix (sequences).\n",
    "        LHS_seq (numpy.ndarray): The derivatives to fit (sequences).\n",
    "        coefficient_threshold (float): The threshold for coefficient pruning.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The discovered sparse coefficient matrix.\n",
    "    \"\"\"\n",
    "    num_sequences, sequence_length, library_size = RHS_seq.shape\n",
    "    _, _, num_features = LHS_seq.shape\n",
    "\n",
    "    # Reshape into 2D format\n",
    "    RHS_flat = RHS_seq.reshape(num_sequences * sequence_length, library_size)\n",
    "    LHS_flat = LHS_seq.reshape(num_sequences * sequence_length, num_features)\n",
    "\n",
    "    # Solve sparse regression\n",
    "    Xi = sindy_fit(RHS_flat, LHS_flat, coefficient_threshold)\n",
    "\n",
    "    return Xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CNN + LSTM Encoder ====================\n",
    "class CNNLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, latent_dim=1):\n",
    "        super(CNNLSTMEncoder, self).__init__()\n",
    "\n",
    "        # CNN Encoder\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1),  # (B, 16, 26, 26)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # (B, 32, 13, 13)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # (B, 64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (B, 128, 4, 4)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1)  # Flatten spatial dimensions\n",
    "        )\n",
    "\n",
    "        # Compute CNN output size dynamically\n",
    "        test_input = torch.zeros(1, input_channels, 51, 51)\n",
    "        cnn_output_size = self.encoder_cnn(test_input).shape[1]  # Get flattened CNN output size\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm_encoder = nn.LSTM(input_size=cnn_output_size, hidden_size=latent_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.shape\n",
    "\n",
    "        # Flatten batch & sequence dimensions\n",
    "        x = x.view(batch_size * seq_len, C, H, W)\n",
    "        encoded_features = self.encoder_cnn(x)  # Shape: (batch_size * seq_len, cnn_output_size)\n",
    "        encoded_features = encoded_features.view(batch_size, seq_len, -1)  # Restore sequence structure\n",
    "\n",
    "        # LSTM Encoder\n",
    "        _, (z, _) = self.lstm_encoder(encoded_features)  # Get final LSTM hidden state\n",
    "        z = z.squeeze(0)  # Remove extra dimension from LSTM\n",
    "\n",
    "        return z  # Latent representation\n",
    "\n",
    "# ==================== CNN + LSTM Decoder ====================\n",
    "class CNNLSTMDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1, input_channels=1):\n",
    "        super(CNNLSTMDecoder, self).__init__()\n",
    "\n",
    "        # CNN Decoder input size (same as CNN Encoder output size)\n",
    "        self.cnn_output_size = 128 * 4 * 4  # Must match CNN encoder's last layer\n",
    "\n",
    "        # LSTM Decoder\n",
    "        self.lstm_decoder = nn.LSTM(input_size=latent_dim, hidden_size=self.cnn_output_size, batch_first=True)\n",
    "\n",
    "        # CNN Decoder\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.Unflatten(2, (128, 4, 4)),  # Reshape for transposed convolutions\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 32, 13, 13)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 16, 26, 26)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 1, 51, 51)\n",
    "            nn.Sigmoid()  # Normalize output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        # Repeat latent representation across time steps\n",
    "        repeated_z = z.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # LSTM Decoder\n",
    "        decoded_features, _ = self.lstm_decoder(repeated_z)\n",
    "\n",
    "        # CNN Decoder\n",
    "        decoded_features = decoded_features.view(batch_size * seq_len, self.cnn_output_size, 1, 1)\n",
    "        x_reconstructed = self.decoder_cnn(decoded_features)  # Shape: (batch_size * seq_len, 1, 51, 51)\n",
    "        x_reconstructed = x_reconstructed.view(batch_size, seq_len, 1, 51, 51)  # Restore sequence structure\n",
    "\n",
    "        return x_reconstructed\n",
    "\n",
    "# ==================== Full CNN + LSTM Autoencoder ====================\n",
    "class CNNLSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, latent_dim=1):\n",
    "        super(CNNLSTMAutoencoder, self).__init__()\n",
    "        self.encoder = CNNLSTMEncoder(input_channels, latent_dim)\n",
    "        self.decoder = CNNLSTMDecoder(latent_dim, input_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _, _, _ = x.shape\n",
    "        z = self.encoder(x)  # Encode sequence into latent space\n",
    "        x_reconstructed = self.decoder(z, seq_len)  # Decode latent space back to images\n",
    "        return x_reconstructed, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(z, ddz, lambda_phys=0.1):\n",
    "    \"\"\"\n",
    "    Physics-informed loss enforcing the pendulum equation: ddot{z} = -sin(z).\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): Latent variable sequence (batch_size, seq_len, latent_dim)\n",
    "        ddz (torch.Tensor): Second derivative of latent variable (batch_size, seq_len, latent_dim)\n",
    "        lambda_phys (float): Weighting for physics loss\n",
    "\n",
    "    Returns:\n",
    "        physics_loss (torch.Tensor): Physics constraint loss\n",
    "    \"\"\"\n",
    "\n",
    "    physics_residual = ddz + torch.sin(z)  # Difference from expected physics\n",
    "    physics_loss = lambda_phys * torch.mean(physics_residual ** 2)\n",
    "\n",
    "    return physics_loss\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def noether_loss(z, dz, Xi, lambda_noether=0.05):\n",
    "    # Compute energy\n",
    "    kinetic_energy = 0.5 * dz**2\n",
    "    potential_energy = 1 - torch.cos(z)\n",
    "    total_energy = kinetic_energy + potential_energy\n",
    "\n",
    "    # Use SINDy to find conservation laws\n",
    "    Theta = sindy_library(z, dz, poly_order=3, include_sine=True)\n",
    "    sindy_energy = Theta @ Xi  # Predicted dynamics\n",
    "\n",
    "    # Enforce that learned dynamics must respect conservation laws\n",
    "    return lambda_noether * torch.mean((sindy_energy - total_energy) ** 2)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sindy_loss(x, x_recon, z, dz, ddz, Theta, Xi, lambda_phys=0.1, lambda1=5e-4, lambda2=5e-5, lambda3=1e-5):\n",
    "    \"\"\"\n",
    "    Computes the full loss:\n",
    "    1. Reconstruction loss\n",
    "    2. SINDy loss in x-dot\n",
    "    3. SINDy loss in z-dot\n",
    "    4. SINDy sparsity regularization\n",
    "    5. Physics loss enforcing ddot{z} = -sin(z)\n",
    "\n",
    "    Args:\n",
    "        x, x_recon: Original and reconstructed sequences.\n",
    "        z, dz, ddz: Latent variables and their derivatives.\n",
    "        Theta: SINDy library matrix.\n",
    "        Xi: SINDy coefficient matrix.\n",
    "        lambda_phys: Physics constraint weight.\n",
    "        lambda1, lambda2, lambda3: Other loss weights.\n",
    "\n",
    "    Returns:\n",
    "        total_loss: Combined loss.\n",
    "    \"\"\"\n",
    "    recon_loss = torch.mean((x - x_recon) ** 2)\n",
    "    sindy_x_loss = torch.mean((dz - torch.matmul(Theta, Xi)) ** 2)\n",
    "    sindy_z_loss = torch.mean((torch.autograd.grad(z, x, grad_outputs=torch.ones_like(z), create_graph=True)[0] - torch.matmul(Theta, Xi)) ** 2)\n",
    "    sindy_reg_loss = torch.mean(torch.abs(Xi))\n",
    "\n",
    "    # Add physics loss\n",
    "    phys_loss = physics_loss(z, ddz, lambda_phys)\n",
    "\n",
    "    total_loss = recon_loss + lambda1 * sindy_x_loss + lambda2 * sindy_z_loss + lambda3 * sindy_reg_loss + phys_loss\n",
    "    return total_loss, recon_loss, sindy_x_loss, sindy_z_loss, sindy_reg_loss, phys_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sindy_library(z, dz, poly_order, include_sine=False):\n",
    "    \"\"\"\n",
    "    Constructs the second-order SINDy library.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): Latent variables (batch_size, seq_len, latent_dim)\n",
    "        dz (torch.Tensor): First derivatives (batch_size, seq_len, latent_dim)\n",
    "        poly_order (int): Maximum polynomial order to include\n",
    "        include_sine (bool): Whether to include sine terms\n",
    "\n",
    "    Returns:\n",
    "        Theta (torch.Tensor): Constructed SINDy library (batch_size, seq_len, num_library_terms)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, latent_dim = z.shape\n",
    "    z_combined = torch.cat([z, dz], dim=-1)  # Combine z and dz\n",
    "\n",
    "    library = [torch.ones((batch_size, seq_len, 1), device=z.device)]  # Constant term\n",
    "\n",
    "    # First-order terms\n",
    "    for i in range(2 * latent_dim):\n",
    "        library.append(z_combined[:, :, i:i+1])\n",
    "\n",
    "    # Higher-order polynomial terms\n",
    "    if poly_order > 1:\n",
    "        for i in range(2 * latent_dim):\n",
    "            for j in range(i, 2 * latent_dim):\n",
    "                library.append(z_combined[:, :, i:i+1] * z_combined[:, :, j:j+1])\n",
    "\n",
    "    if poly_order > 2:\n",
    "        for i in range(2 * latent_dim):\n",
    "            for j in range(i, 2 * latent_dim):\n",
    "                for k in range(j, 2 * latent_dim):\n",
    "                    library.append(z_combined[:, :, i:i+1] * z_combined[:, :, j:j+1] * z_combined[:, :, k:k+1])\n",
    "\n",
    "    # Optional sine terms\n",
    "    if include_sine:\n",
    "        for i in range(2 * latent_dim):\n",
    "            library.append(torch.sin(z_combined[:, :, i:i+1]))\n",
    "\n",
    "    # Stack all terms into a single tensor\n",
    "    Theta = torch.cat(library, dim=-1)\n",
    "    return Theta  # Shape: (batch_size, seq_len, num_library_terms)\n",
    "\n",
    "def z_derivative(z, dt):\n",
    "    \"\"\"\n",
    "    Computes first and second time derivatives of the latent space variable z.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): Latent representation (batch_size, seq_len, latent_dim)\n",
    "        dt (float): Time step between frames\n",
    "\n",
    "    Returns:\n",
    "        dz (torch.Tensor): First derivative (batch_size, seq_len, latent_dim)\n",
    "        ddz (torch.Tensor): Second derivative (batch_size, seq_len, latent_dim)\n",
    "    \"\"\"\n",
    "    dz = torch.zeros_like(z)\n",
    "    ddz = torch.zeros_like(z)\n",
    "\n",
    "    # Compute first derivative using central difference\n",
    "    dz[:, 1:-1, :] = (z[:, 2:, :] - z[:, :-2, :]) / (2 * dt)\n",
    "    dz[:, 0, :] = (z[:, 1, :] - z[:, 0, :]) / dt  # Forward difference for first point\n",
    "    dz[:, -1, :] = (z[:, -1, :] - z[:, -2, :]) / dt  # Backward difference for last point\n",
    "\n",
    "    # Compute second derivative using central difference\n",
    "    ddz[:, 1:-1, :] = (z[:, 2:, :] - 2 * z[:, 1:-1, :] + z[:, :-2, :]) / (dt**2)\n",
    "    ddz[:, 0, :] = (z[:, 1, :] - 2 * z[:, 0, :] + z[:, 1, :]) / (dt**2)  # Approximation at first point\n",
    "    ddz[:, -1, :] = (z[:, -2, :] - 2 * z[:, -1, :] + z[:, -2, :]) / (dt**2)  # Approximation at last point\n",
    "\n",
    "    return dz, ddz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(autoencoder, training_loader, val_loader, params, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Train the CNN + LSTM Autoencoder with SINDy loss.\n",
    "\n",
    "    Args:\n",
    "        autoencoder (nn.Module): The autoencoder model (CNN + LSTM).\n",
    "        training_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        params (dict): Training parameters.\n",
    "        device (str): Device to use (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        results_dict (dict): Dictionary containing final loss values and trained SINDy coefficients.\n",
    "    \"\"\"\n",
    "    # Move model to device (GPU or CPU)\n",
    "    autoencoder = autoencoder.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "    # Initialize SINDy coefficients (random initialization)\n",
    "    library_dim = params['library_dim']\n",
    "    latent_dim = params['latent_dim']\n",
    "    Xi = torch.randn((library_dim, latent_dim), requires_grad=True, device=device)  # Learnable coefficients\n",
    "    Xi_optimizer = optim.Adam([Xi], lr=1e-3)\n",
    "\n",
    "    # Loss tracking\n",
    "    validation_losses = []\n",
    "    sindy_model_terms = [torch.sum(params['coefficient_mask']).item()]\n",
    "\n",
    "    print(\"TRAINING STARTED...\")\n",
    "    for epoch in range(params[\"max_epochs\"]):\n",
    "\n",
    "        # Training loop\n",
    "        autoencoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in training_loader:\n",
    "            x = batch['x'].to(device)  # Input sequence (batch_size, seq_len, 1, 51, 51)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            Xi_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through autoencoder\n",
    "            x_recon, z = autoencoder(x)\n",
    "\n",
    "            # Compute time derivatives of z (for SINDy)\n",
    "            dt = params[\"dt\"]\n",
    "            dz, ddz = z_derivative(z, dt)\n",
    "\n",
    "            # Compute SINDy library Theta\n",
    "            Theta = sindy_library(z, dz, params[\"poly_order\"], params[\"include_sine\"])\n",
    "\n",
    "            # Compute full SINDy loss\n",
    "            loss, recon_loss, sindy_x_loss, sindy_z_loss, sindy_reg_loss = sindy_loss(x, x_recon, z, dz, Theta, Xi)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            Xi_optimizer.step()  # Optimize Xi separately\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation step\n",
    "        autoencoder.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch['x'].to(device)\n",
    "\n",
    "                x_recon, z = autoencoder(x)\n",
    "                dz, ddz = z_derivative(z, dt)\n",
    "                Theta = sindy_library(z, dz, params[\"poly_order\"], params[\"include_sine\"])\n",
    "                val_loss, _, _, _, _ = sindy_loss(x, x_recon, z, dz, Theta, Xi)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % params[\"print_frequency\"] == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {total_loss / len(training_loader)}, Validation Loss = {val_loss.item()}\")\n",
    "\n",
    "        validation_losses.append(val_loss.item())\n",
    "\n",
    "        # Apply sequential thresholding every few epochs\n",
    "        if params[\"sequential_thresholding\"] and epoch % params[\"threshold_frequency\"] == 0 and epoch > 0:\n",
    "            params[\"coefficient_mask\"] = (torch.abs(Xi) > params[\"coefficient_threshold\"]).float()\n",
    "            print(f\"Thresholding Applied: {torch.sum(params['coefficient_mask']).item()} active coefficients\")\n",
    "            sindy_model_terms.append(torch.sum(params[\"coefficient_mask\"]).item())\n",
    "\n",
    "    # Store results\n",
    "    results_dict = {\n",
    "        \"num_epochs\": params[\"max_epochs\"],\n",
    "        \"validation_losses\": np.array(validation_losses),\n",
    "        \"sindy_model_terms\": np.array(sindy_model_terms),\n",
    "        \"sindy_coefficients\": Xi.detach().cpu().numpy(),\n",
    "    }\n",
    "\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATASET GENERATION ====================\n",
    "local_path = r\"C:\\Users\\User\\PIML-2\\None Video Models and datasets\\Nonlinear Pendulum\\nonlinearpendulum_dataset.pkl\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "if os.path.exists(local_path):\n",
    "    print(\"Loading dataset from local storage...\")\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "else:\n",
    "    print(\"Generating new dataset and saving to local storage...\")\n",
    "    sequence_length = 40  # Number of time steps per sequence\n",
    "    n_ics = 120  # Number of different pendulum initial conditions\n",
    "    dataset = generate_pendulum_data(n_ics, sequence_length)\n",
    "\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    print(\"Dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== HYPERPARAMETERS ====================\n",
    "params = {\n",
    "    \"latent_dim\": 1,  # Dimension of the latent space (angle representation)\n",
    "    \"model_order\": 2,  # Second-order dynamics (ddz term)\n",
    "    \"poly_order\": 3,  # Polynomial order for SINDy library\n",
    "    \"include_sine\": True,  # Include sine terms in the SINDy library\n",
    "    \"library_dim\": 10,  # Placeholder (computed in SINDy library function)\n",
    "\n",
    "    # Loss Weights\n",
    "    \"loss_weight_decoder\": 1.0,\n",
    "    \"loss_weight_sindy_x\": 5e-4,\n",
    "    \"loss_weight_sindy_z\": 5e-5,\n",
    "    \"loss_weight_sindy_regularization\": 1e-5,\n",
    "    \"loss_weight_physics\": 0.1,  # Physics constraint weight\n",
    "\n",
    "    # Sequential Thresholding\n",
    "    \"sequential_thresholding\": True,\n",
    "    \"coefficient_threshold\": 0.1,\n",
    "    \"threshold_frequency\": 500,\n",
    "\n",
    "    # Training Parameters\n",
    "    \"batch_size\": 3,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"max_epochs\": 5001,\n",
    "    \"print_frequency\": 100,\n",
    "    \"dt\": 0.01,  # Time step for finite differences\n",
    "}\n",
    "\n",
    "# ==================== DATASET & DATALOADERS ====================\n",
    "train_size = int(0.8 * len(dataset))  # 80% train, 20% validation\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# PyTorch dataset & split\n",
    "pendulum_dataset = PendulumDataset(dataset)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(pendulum_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Training Samples: {len(train_dataset)}, Validation Samples: {len(val_dataset)}\")\n",
    "\n",
    "# ==================== AUTOMATIC INPUT DIM DETECTION ====================\n",
    "sample = pendulum_dataset[0][\"x\"]\n",
    "print(f\"Sample Shape: {sample.shape}\")  # Debugging output\n",
    "\n",
    "# Correctly infer input_dim based on dataset format\n",
    "if len(sample.shape) == 2:\n",
    "    params[\"input_dim\"] = sample.shape[-1]  # If dataset is flattened (num_samples, features)\n",
    "else:\n",
    "    params[\"input_dim\"] = sample.shape[1:]  # If dataset is structured as (seq_len, channels, height, width)\n",
    "\n",
    "print(f\"Using input_dim: {params['input_dim']}\")  # Debug check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MODEL INITIALIZATION ====================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "autoencoder = CNNLSTMAutoencoder(input_channels=1, latent_dim=params[\"latent_dim\"]).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "# Initialize SINDy coefficient matrix\n",
    "Xi = torch.randn((params[\"library_dim\"], params[\"latent_dim\"]), requires_grad=True, device=device)\n",
    "Xi_optimizer = torch.optim.Adam([Xi], lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
