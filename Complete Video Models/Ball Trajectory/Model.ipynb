{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from pathlib import Path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical Constants\n",
    "G = 9.81  # Gravity (m/s^2)\n",
    "BOUNCE_COEFF = 0.9  # Coefficient of restitution\n",
    "MASS = 1.0  # Mass of the particle\n",
    "RADIUS = 0.2  # Radius of the ball\n",
    "DRAG_COEFF = 0.1  # Air resistance coefficient\n",
    "box_size = 5.0 # Size of the box\n",
    "\n",
    "# Simulation Parameters\n",
    "n_balls = 100\n",
    "output_dir = \"BallDataset\"\n",
    "dt = 0.01  # Time step\n",
    "seq_len = 160  # Input sequence length\n",
    "pred_len = 40  # Prediction sequence length\n",
    "\n",
    "# Model Parameters\n",
    "LATENT_DIM = 16  # Latent space size\n",
    "HIDDEN_DIM = 64  # Hidden layer size in dynamics\n",
    "LIBRARY_DIM = 30  # Number of candidate functions for SINDy\n",
    "INPUT_SIZE = (64, 64)  # Input image dimensions (H, W)\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-3\n",
    "ALPHA, BETA, GAMMA, DELTA, EPSILON, ZETA = 1.0, 1.0, 1.0, 1e-4, 1.0, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_collisions(state, radius, box_size, bounce_coeff):\n",
    "    x, vx, y, vy = state\n",
    "    if x - radius < -box_size:\n",
    "        x = -box_size + radius\n",
    "        vx = -bounce_coeff * vx\n",
    "    elif x + radius > box_size:\n",
    "        x = box_size - radius\n",
    "        vx = -bounce_coeff * vx\n",
    "    if y - radius < -box_size:\n",
    "        y = -box_size + radius\n",
    "        vy = -bounce_coeff * vy\n",
    "    elif y + radius > box_size:\n",
    "        y = box_size - radius\n",
    "        vy = -bounce_coeff * vy\n",
    "    return np.array([x, vx, y, vy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_dynamics_with_collisions(t, y, mass, radius, box_size, drag_coeff, bounce_coeff):\n",
    "    x, vx, y_pos, vy = y\n",
    "    dydt = np.zeros_like(y)\n",
    "    dydt[0] = vx\n",
    "    dydt[2] = vy\n",
    "    speed = np.sqrt(vx**2 + vy**2)\n",
    "    drag_force_x = -drag_coeff * speed * vx\n",
    "    drag_force_y = -drag_coeff * speed * vy\n",
    "    dydt[1] = drag_force_x / mass\n",
    "    dydt[3] = -G + drag_force_y / mass\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ball_dataset(n_balls, dt, seq_len, pred_len, box_size, radius=0.2, output_dir=\"BallDataset\"):\n",
    "    \"\"\"\n",
    "    Generate a dataset of ball trajectories and save them as GIFs.\n",
    "\n",
    "    Args:\n",
    "        n_balls: Number of trajectories to generate.\n",
    "        dt: Time step for simulation.\n",
    "        seq_len: Length of input sequences.\n",
    "        pred_len: Length of prediction sequences.\n",
    "        box_size: Size of the simulation box.\n",
    "        radius: Radius of the ball.\n",
    "        output_dir: Directory to save the dataset and generated GIFs.\n",
    "\n",
    "    Returns:\n",
    "        dataset: List of tuples containing trajectory data and corresponding GIF paths.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory path is absolute\n",
    "    output_dir = os.path.abspath(output_dir)\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n_balls), desc=\"Generating dataset\"):\n",
    "        # Randomize parameters for the trajectory\n",
    "        mass = np.random.uniform(1, 3)\n",
    "        radius = mass/10\n",
    "        drag_coeff = 0.1\n",
    "        bounce_coeff = 0.9\n",
    "\n",
    "        # Initial conditions for position and velocity\n",
    "        init_pos = np.random.uniform(-box_size + radius, box_size - radius, 2)\n",
    "        init_vel = np.random.uniform(-5.0, 5.0, 2)\n",
    "        y0 = np.concatenate([init_pos, init_vel])\n",
    "\n",
    "        # Time evaluation for the trajectory\n",
    "        t_eval = np.linspace(0, (seq_len + pred_len) * dt, int(seq_len + pred_len))\n",
    "\n",
    "        # Simulate the trajectory\n",
    "        trajectory = []\n",
    "        state = y0\n",
    "        for t in t_eval:\n",
    "            sol = solve_ivp(\n",
    "                particle_dynamics_with_collisions,\n",
    "                (t, t + dt),\n",
    "                state,\n",
    "                args=(mass, radius, box_size, drag_coeff, bounce_coeff),\n",
    "                method=\"RK45\",\n",
    "                t_eval=[t + dt]\n",
    "            )\n",
    "            if sol.y.size == 0:\n",
    "                break\n",
    "            state = sol.y.flatten()\n",
    "            state = handle_collisions(state, radius, box_size, bounce_coeff)\n",
    "            trajectory.append(state)\n",
    "\n",
    "        # Convert trajectory to numpy array\n",
    "        trajectory = np.array(trajectory)\n",
    "\n",
    "        # Path for the corresponding GIF\n",
    "        gif_path = os.path.join(output_dir, f\"ball_{i}.gif\")\n",
    "\n",
    "        # Generate and save the GIF\n",
    "        generate_ball_gif(trajectory, radius, box_size, gif_path)\n",
    "\n",
    "        # Append the trajectory and metadata to the dataset\n",
    "        dataset.append((trajectory, gif_path))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ball_gif(trajectory, radius, box_size, save_path):\n",
    "    \"\"\"\n",
    "    Generate and save a GIF of the ball trajectory.\n",
    "\n",
    "    Args:\n",
    "        trajectory: Numpy array containing the ball's positions and velocities.\n",
    "        radius: Radius of the ball.\n",
    "        box_size: Size of the box.\n",
    "        save_path: File path to save the generated GIF.\n",
    "    \"\"\"\n",
    "    x = trajectory[:, 0]\n",
    "    y = trajectory[:, 2]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-box_size, box_size)\n",
    "    ax.set_ylim(-box_size, box_size)\n",
    "    ax.set_aspect('equal')\n",
    "    ball = plt.Circle((x[0], y[0]), radius, fc='blue')\n",
    "    ax.add_patch(ball)\n",
    "\n",
    "    def update(frame):\n",
    "        ball.set_center((x[frame], y[frame]))\n",
    "        return ball,\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(trajectory), blit=True, interval=50)\n",
    "    ani.save(save_path, fps=20, writer='pillow')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset from directory\n",
    "def load_dataset_from_directory(directory):\n",
    "    \"\"\"\n",
    "    Load a dataset of trajectories from the given directory.\n",
    "\n",
    "    Args:\n",
    "        directory: Path to the directory containing trajectory files (e.g., .npy files).\n",
    "\n",
    "    Returns:\n",
    "        trajectories: List of trajectories loaded from the directory.\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".npy\"):  # Assuming the dataset is saved as .npy files\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            trajectory = np.load(file_path)  # Load .npy file\n",
    "            trajectories.append(trajectory)  # Append to the list of trajectories\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BallTrajectoryDataset class\n",
    "class BallTrajectoryDataset(Dataset):\n",
    "    def __init__(self, trajectories, seq_len, pred_len):\n",
    "        \"\"\"\n",
    "        Dataset class for ball trajectories.\n",
    "\n",
    "        Args:\n",
    "            trajectories: List of trajectories (numpy arrays).\n",
    "            seq_len: Length of input sequence.\n",
    "            pred_len: Length of prediction sequence.\n",
    "        \"\"\"\n",
    "        self.trajectories = trajectories\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of trajectories in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the input and target sequences for the given index.\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the trajectory.\n",
    "\n",
    "        Returns:\n",
    "            input_seq: Input sequence (seq_len, state_dim).\n",
    "            target_seq: Target sequence (pred_len, state_dim).\n",
    "        \"\"\"\n",
    "        trajectory = self.trajectories[idx]  # Get the trajectory at the specified index\n",
    "        input_seq = trajectory[:self.seq_len]  # Slice the input sequence\n",
    "        target_seq = trajectory[self.seq_len:self.seq_len + self.pred_len]  # Slice the target sequence\n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset splitting function\n",
    "def split_dataset(trajectories, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        trajectories: List of trajectories (numpy arrays).\n",
    "        train_ratio: Proportion of data for training.\n",
    "        val_ratio: Proportion of data for validation.\n",
    "        test_ratio: Proportion of data for testing.\n",
    "\n",
    "    Returns:\n",
    "        train_set: Training dataset.\n",
    "        val_set: Validation dataset.\n",
    "        test_set: Testing dataset.\n",
    "    \"\"\"\n",
    "    total = train_ratio + val_ratio + test_ratio\n",
    "    train_ratio /= total\n",
    "    val_ratio /= total\n",
    "    test_ratio /= total\n",
    "\n",
    "    train_val, test_set = train_test_split(trajectories, test_size=test_ratio, random_state=42)\n",
    "    train_set, val_set = train_test_split(train_val, test_size=val_ratio / (train_ratio + val_ratio), random_state=42)\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 trajectories from C:\\Users\\User\\Documents\\GitHub\\PIML\\Complete Video Models\\Ball Trajectory\\BallDataset\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the saved directory\n",
    "dataset_path = r\"C:\\Users\\User\\Documents\\GitHub\\PIML\\Complete Video Models\\Ball Trajectory\\BallDataset\"\n",
    "\n",
    "# Load trajectories\n",
    "trajectories = load_dataset_from_directory(dataset_path)\n",
    "print(f\"Loaded {len(trajectories)} trajectories from {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory C:\\Users\\User\\Documents\\GitHub\\PIML\\Complete Video Models\\Ball Trajectory\\BallDataset exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"Error: Directory {dataset_path} does not exist.\")\n",
    "else:\n",
    "    print(f\"Directory {dataset_path} exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in C:\\Users\\User\\Documents\\GitHub\\PIML\\Complete Video Models\\Ball Trajectory\\BallDataset: ['ball_0.gif', 'ball_1.gif', 'ball_10.gif', 'ball_100.gif', 'ball_101.gif', 'ball_102.gif', 'ball_103.gif', 'ball_104.gif', 'ball_105.gif', 'ball_106.gif', 'ball_107.gif', 'ball_108.gif', 'ball_109.gif', 'ball_11.gif', 'ball_110.gif', 'ball_111.gif', 'ball_112.gif', 'ball_113.gif', 'ball_114.gif', 'ball_115.gif', 'ball_116.gif', 'ball_117.gif', 'ball_118.gif', 'ball_119.gif', 'ball_12.gif', 'ball_120.gif', 'ball_121.gif', 'ball_122.gif', 'ball_123.gif', 'ball_124.gif', 'ball_125.gif', 'ball_126.gif', 'ball_127.gif', 'ball_128.gif', 'ball_129.gif', 'ball_13.gif', 'ball_130.gif', 'ball_131.gif', 'ball_132.gif', 'ball_133.gif', 'ball_134.gif', 'ball_135.gif', 'ball_136.gif', 'ball_137.gif', 'ball_138.gif', 'ball_139.gif', 'ball_14.gif', 'ball_140.gif', 'ball_141.gif', 'ball_142.gif', 'ball_143.gif', 'ball_144.gif', 'ball_145.gif', 'ball_146.gif', 'ball_147.gif', 'ball_148.gif', 'ball_149.gif', 'ball_15.gif', 'ball_150.gif', 'ball_151.gif', 'ball_152.gif', 'ball_153.gif', 'ball_154.gif', 'ball_155.gif', 'ball_156.gif', 'ball_157.gif', 'ball_158.gif', 'ball_159.gif', 'ball_16.gif', 'ball_160.gif', 'ball_161.gif', 'ball_162.gif', 'ball_163.gif', 'ball_164.gif', 'ball_165.gif', 'ball_166.gif', 'ball_167.gif', 'ball_168.gif', 'ball_169.gif', 'ball_17.gif', 'ball_170.gif', 'ball_171.gif', 'ball_172.gif', 'ball_173.gif', 'ball_174.gif', 'ball_175.gif', 'ball_176.gif', 'ball_177.gif', 'ball_178.gif', 'ball_179.gif', 'ball_18.gif', 'ball_180.gif', 'ball_181.gif', 'ball_182.gif', 'ball_183.gif', 'ball_184.gif', 'ball_185.gif', 'ball_186.gif', 'ball_187.gif', 'ball_188.gif', 'ball_189.gif', 'ball_19.gif', 'ball_190.gif', 'ball_191.gif', 'ball_192.gif', 'ball_193.gif', 'ball_194.gif', 'ball_195.gif', 'ball_196.gif', 'ball_197.gif', 'ball_198.gif', 'ball_199.gif', 'ball_2.gif', 'ball_20.gif', 'ball_200.gif', 'ball_201.gif', 'ball_202.gif', 'ball_203.gif', 'ball_204.gif', 'ball_205.gif', 'ball_206.gif', 'ball_207.gif', 'ball_208.gif', 'ball_209.gif', 'ball_21.gif', 'ball_210.gif', 'ball_211.gif', 'ball_212.gif', 'ball_213.gif', 'ball_214.gif', 'ball_215.gif', 'ball_216.gif', 'ball_217.gif', 'ball_218.gif', 'ball_219.gif', 'ball_22.gif', 'ball_220.gif', 'ball_221.gif', 'ball_222.gif', 'ball_223.gif', 'ball_224.gif', 'ball_225.gif', 'ball_226.gif', 'ball_227.gif', 'ball_228.gif', 'ball_229.gif', 'ball_23.gif', 'ball_230.gif', 'ball_231.gif', 'ball_232.gif', 'ball_233.gif', 'ball_234.gif', 'ball_235.gif', 'ball_236.gif', 'ball_237.gif', 'ball_238.gif', 'ball_239.gif', 'ball_24.gif', 'ball_240.gif', 'ball_241.gif', 'ball_242.gif', 'ball_243.gif', 'ball_244.gif', 'ball_245.gif', 'ball_246.gif', 'ball_247.gif', 'ball_248.gif', 'ball_249.gif', 'ball_25.gif', 'ball_250.gif', 'ball_251.gif', 'ball_252.gif', 'ball_253.gif', 'ball_254.gif', 'ball_255.gif', 'ball_256.gif', 'ball_257.gif', 'ball_258.gif', 'ball_259.gif', 'ball_26.gif', 'ball_260.gif', 'ball_261.gif', 'ball_262.gif', 'ball_263.gif', 'ball_264.gif', 'ball_265.gif', 'ball_266.gif', 'ball_267.gif', 'ball_268.gif', 'ball_269.gif', 'ball_27.gif', 'ball_270.gif', 'ball_271.gif', 'ball_272.gif', 'ball_273.gif', 'ball_274.gif', 'ball_275.gif', 'ball_276.gif', 'ball_277.gif', 'ball_278.gif', 'ball_279.gif', 'ball_28.gif', 'ball_280.gif', 'ball_281.gif', 'ball_282.gif', 'ball_283.gif', 'ball_284.gif', 'ball_285.gif', 'ball_286.gif', 'ball_287.gif', 'ball_288.gif', 'ball_289.gif', 'ball_29.gif', 'ball_290.gif', 'ball_291.gif', 'ball_292.gif', 'ball_293.gif', 'ball_294.gif', 'ball_295.gif', 'ball_296.gif', 'ball_297.gif', 'ball_298.gif', 'ball_299.gif', 'ball_3.gif', 'ball_30.gif', 'ball_31.gif', 'ball_32.gif', 'ball_33.gif', 'ball_34.gif', 'ball_35.gif', 'ball_36.gif', 'ball_37.gif', 'ball_38.gif', 'ball_39.gif', 'ball_4.gif', 'ball_40.gif', 'ball_41.gif', 'ball_42.gif', 'ball_43.gif', 'ball_44.gif', 'ball_45.gif', 'ball_46.gif', 'ball_47.gif', 'ball_48.gif', 'ball_49.gif', 'ball_5.gif', 'ball_50.gif', 'ball_51.gif', 'ball_52.gif', 'ball_53.gif', 'ball_54.gif', 'ball_55.gif', 'ball_56.gif', 'ball_57.gif', 'ball_58.gif', 'ball_59.gif', 'ball_6.gif', 'ball_60.gif', 'ball_61.gif', 'ball_62.gif', 'ball_63.gif', 'ball_64.gif', 'ball_65.gif', 'ball_66.gif', 'ball_67.gif', 'ball_68.gif', 'ball_69.gif', 'ball_7.gif', 'ball_70.gif', 'ball_71.gif', 'ball_72.gif', 'ball_73.gif', 'ball_74.gif', 'ball_75.gif', 'ball_76.gif', 'ball_77.gif', 'ball_78.gif', 'ball_79.gif', 'ball_8.gif', 'ball_80.gif', 'ball_81.gif', 'ball_82.gif', 'ball_83.gif', 'ball_84.gif', 'ball_85.gif', 'ball_86.gif', 'ball_87.gif', 'ball_88.gif', 'ball_89.gif', 'ball_9.gif', 'ball_90.gif', 'ball_91.gif', 'ball_92.gif', 'ball_93.gif', 'ball_94.gif', 'ball_95.gif', 'ball_96.gif', 'ball_97.gif', 'ball_98.gif', 'ball_99.gif']\n"
     ]
    }
   ],
   "source": [
    "# List all files in the directory\n",
    "files_in_directory = os.listdir(dataset_path)\n",
    "print(f\"Files in {dataset_path}: {files_in_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.10000000000000002 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split dataset into train, validation, and test sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_set, val_set, test_set \u001b[38;5;241m=\u001b[39m \u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize datasets for each split\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m BallTrajectoryDataset(train_set, seq_len\u001b[38;5;241m=\u001b[39mSEQ_LEN, pred_len\u001b[38;5;241m=\u001b[39mPRED_LEN)\n",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m, in \u001b[0;36msplit_dataset\u001b[1;34m(trajectories, train_ratio, val_ratio, test_ratio)\u001b[0m\n\u001b[0;32m     19\u001b[0m val_ratio \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m total\n\u001b[0;32m     20\u001b[0m test_ratio \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m total\n\u001b[1;32m---> 22\u001b[0m train_val, test_set \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m train_set, val_set \u001b[38;5;241m=\u001b[39m train_test_split(train_val, test_size\u001b[38;5;241m=\u001b[39mval_ratio \u001b[38;5;241m/\u001b[39m (train_ratio \u001b[38;5;241m+\u001b[39m val_ratio), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_set, val_set, test_set\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2785\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2415\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2412\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2419\u001b[0m     )\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.10000000000000002 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Split dataset into train, validation, and test sets\n",
    "train_set, val_set, test_set = split_dataset(trajectories)\n",
    "\n",
    "# Initialize datasets for each split\n",
    "train_dataset = BallTrajectoryDataset(train_set, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "val_dataset = BallTrajectoryDataset(val_set, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "test_dataset = BallTrajectoryDataset(test_set, seq_len=SEQ_LEN, pred_len=PRED_LEN)\n",
    "\n",
    "# Create DataLoader objects for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch to verify\n",
    "for batch_inputs, batch_targets in train_loader:\n",
    "    print(f\"Input batch shape: {batch_inputs.shape}\")\n",
    "    print(f\"Target batch shape: {batch_targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SINDyLayer(nn.Module):\n",
    "    def __init__(self, input_dim, library_dim):\n",
    "        super(SINDyLayer, self).__init__()\n",
    "        self.coefficients = nn.Parameter(torch.randn(library_dim, input_dim))\n",
    "\n",
    "    def forward(self, library, dz_dt):\n",
    "        dz_dt_pred = library @ self.coefficients\n",
    "        sindy_loss = torch.mean((dz_dt - dz_dt_pred) ** 2)\n",
    "        return dz_dt_pred, sindy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNModel(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim, hidden_dim, library_dim):\n",
    "        super(PINNModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * (input_size[0] // 4) * (input_size[1] // 4), latent_dim),\n",
    "        )\n",
    "        self.dynamics = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32 * (input_size[0] // 4) * (input_size[1] // 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (32, input_size[0] // 4, input_size[1] // 4)),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sindy_layer = SINDyLayer(latent_dim, library_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        dz_dt = self.dynamics(z)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, z, dz_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(u_pred, u_true):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction loss.\n",
    "\n",
    "    Args:\n",
    "        u_pred: Predicted states (batch_size, seq_len, state_dim).\n",
    "        u_true: Ground truth states (batch_size, seq_len, state_dim).\n",
    "\n",
    "    Returns:\n",
    "        loss: Mean squared error loss.\n",
    "    \"\"\"\n",
    "    return torch.mean((u_pred - u_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sindy_loss(z, dz_dt, sindy_layer):\n",
    "    \"\"\"\n",
    "    Compute the SINDy loss for sparse dynamics.\n",
    "\n",
    "    Args:\n",
    "        z: Latent variables (batch_size, latent_dim).\n",
    "        dz_dt: Time derivatives of latent variables (batch_size, latent_dim).\n",
    "        sindy_layer: SINDy dynamics layer.\n",
    "\n",
    "    Returns:\n",
    "        loss: SINDy loss value.\n",
    "    \"\"\"\n",
    "    library = sindy_library(z)\n",
    "    dz_dt_pred, loss = sindy_layer(library, dz_dt)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_loss(sindy_layer):\n",
    "    \"\"\"\n",
    "    Compute the sparsity loss for SINDy coefficients.\n",
    "\n",
    "    Args:\n",
    "        sindy_layer: SINDy dynamics layer.\n",
    "\n",
    "    Returns:\n",
    "        loss: L1 norm of the SINDy coefficients.\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.abs(sindy_layer.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy(state, mass, G):\n",
    "    \"\"\"\n",
    "    Compute total energy (kinetic + potential).\n",
    "\n",
    "    Args:\n",
    "        state: Tensor of shape (batch_size, state_dim).\n",
    "        mass: Mass of the particle.\n",
    "        G: Gravitational acceleration.\n",
    "\n",
    "    Returns:\n",
    "        energy: Tensor of total energy for each batch.\n",
    "    \"\"\"\n",
    "    _, vx, y, vy = state.T\n",
    "\n",
    "    # Kinetic energy\n",
    "    kinetic_energy = 0.5 * mass * (vx**2 + vy**2)\n",
    "\n",
    "    # Potential energy\n",
    "    potential_energy = mass * G * y\n",
    "\n",
    "    return kinetic_energy + potential_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_loss(u_pred, u_true, mass, G):\n",
    "    \"\"\"\n",
    "    Compute the energy loss between predicted and true states.\n",
    "\n",
    "    Args:\n",
    "        u_pred: Predicted states (batch_size, seq_len, state_dim).\n",
    "        u_true: True states (batch_size, seq_len, state_dim).\n",
    "        mass: Mass of the particle.\n",
    "        G: Gravitational acceleration.\n",
    "\n",
    "    Returns:\n",
    "        loss: Mean squared error of energy differences.\n",
    "    \"\"\"\n",
    "    predicted_energy = compute_energy(u_pred, mass, G)\n",
    "    true_energy = compute_energy(u_true, mass, G)\n",
    "    return torch.mean((predicted_energy - true_energy) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_loss(predicted_state, true_state, mass, drag_coeff, G):\n",
    "    \"\"\"\n",
    "    Compute the ODE residual loss.\n",
    "\n",
    "    Args:\n",
    "        predicted_state: Predicted states (batch_size, state_dim).\n",
    "        true_state: True states (batch_size, state_dim).\n",
    "        mass: Mass of the particle.\n",
    "        drag_coeff: Drag coefficient.\n",
    "        G: Gravitational acceleration.\n",
    "\n",
    "    Returns:\n",
    "        loss: Residual loss enforcing the governing equations.\n",
    "    \"\"\"\n",
    "    x, vx, y, vy = predicted_state.T\n",
    "    speed = torch.sqrt(vx**2 + vy**2)\n",
    "    drag_force_x = -drag_coeff * speed * vx\n",
    "    drag_force_y = -drag_coeff * speed * vy\n",
    "    dx_dt = vx\n",
    "    dy_dt = vy\n",
    "    dvx_dt = drag_force_x / mass\n",
    "    dvy_dt = -G + drag_force_y / mass\n",
    "    dx_dt_pred, dvx_dt_pred, dy_dt_pred, dvy_dt_pred = true_state.T\n",
    "    return torch.mean((dx_dt - dx_dt_pred)**2 + (dvx_dt - dvx_dt_pred)**2 +\n",
    "                      (dy_dt - dy_dt_pred)**2 + (dvy_dt - dvy_dt_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collision_loss(predicted_state, true_state, radius, box_size, bounce_coeff):\n",
    "    \"\"\"\n",
    "    Compute the collision loss based on boundary interactions.\n",
    "\n",
    "    Args:\n",
    "        predicted_state: Predicted states (batch_size, state_dim).\n",
    "        true_state: True states (batch_size, state_dim).\n",
    "        radius: Radius of the particle.\n",
    "        box_size: Size of the box.\n",
    "        bounce_coeff: Coefficient of restitution for collisions.\n",
    "\n",
    "    Returns:\n",
    "        loss: Collision loss value.\n",
    "    \"\"\"\n",
    "    x, vx, y, vy = predicted_state.T\n",
    "    x_collision = (x - radius < -box_size) | (x + radius > box_size)\n",
    "    y_collision = (y - radius < -box_size) | (y + radius > box_size)\n",
    "    vx_true, vy_true = true_state[:, 1], true_state[:, 3]\n",
    "    loss = torch.mean(x_collision * (vx + bounce_coeff * vx_true)**2) + \\\n",
    "           torch.mean(y_collision * (vy + bounce_coeff * vy_true)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_combined_loss(u_pred, u_true, z, dz_dt, sindy_layer, mass, drag_coeff, G, radius, box_size, bounce_coeff,\n",
    "                       alpha=1.0, beta=1.0, gamma=1.0, delta=1.0, epsilon=1.0, zeta=1.0):\n",
    "    \"\"\"\n",
    "    Compute the full combined loss.\n",
    "\n",
    "    Args:\n",
    "        u_pred: Predicted states.\n",
    "        u_true: True states.\n",
    "        z: Latent variables.\n",
    "        dz_dt: Time derivatives of latent variables.\n",
    "        sindy_layer: SINDy dynamics layer.\n",
    "        mass, drag_coeff, G: Physical parameters.\n",
    "        radius, box_size, bounce_coeff: Collision parameters.\n",
    "        alpha, beta, gamma, delta, epsilon, zeta: Weights for loss components.\n",
    "\n",
    "    Returns:\n",
    "        total_loss: Combined loss value.\n",
    "    \"\"\"\n",
    "    # Compute individual losses\n",
    "    reconstruction = reconstruction_loss(u_pred, u_true)\n",
    "    sindy = sindy_loss(z, dz_dt, sindy_layer)\n",
    "    sparsity = sparsity_loss(sindy_layer)\n",
    "    ode = ode_loss(u_pred, u_true, mass, drag_coeff, G)\n",
    "    collision = collision_loss(u_pred, u_true, radius, box_size, bounce_coeff)\n",
    "    energy = energy_loss(u_pred, u_true, mass, G)\n",
    "\n",
    "    # Total combined loss\n",
    "    return (alpha * reconstruction +\n",
    "            beta * sindy +\n",
    "            gamma * sparsity +\n",
    "            delta * ode +\n",
    "            epsilon * collision +\n",
    "            zeta * energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_validation(model, sindy_layer, train_loader, val_loader, optimizer, epochs, mass, drag_coeff, G,\n",
    "                                radius, box_size, bounce_coeff, alpha, beta, gamma, delta, epsilon, zeta):\n",
    "    \"\"\"\n",
    "    Train the PINN + SINDy model with validation and print accuracy.\n",
    "\n",
    "    Args:\n",
    "        model: PINNModel instance.\n",
    "        sindy_layer: SINDy dynamics layer.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        optimizer: Optimizer for training.\n",
    "        epochs: Number of training epochs.\n",
    "        mass, drag_coeff, G: Physical parameters.\n",
    "        radius, box_size, bounce_coeff: Collision parameters.\n",
    "        alpha, beta, gamma, delta, epsilon, zeta: Weights for different loss components.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    sindy_layer.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "\n",
    "        # Training Phase\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            u_pred, z, dz_dt = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = full_combined_loss(u_pred, targets, z, dz_dt, sindy_layer, mass, drag_coeff, G,\n",
    "                                      radius, box_size, bounce_coeff, alpha, beta, gamma, delta, epsilon, zeta)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            batch_accuracy = 1 - (torch.norm(u_pred - targets) / torch.norm(targets))\n",
    "            train_accuracy += batch_accuracy.item()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                # Forward pass\n",
    "                u_pred, z, dz_dt = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = full_combined_loss(u_pred, targets, z, dz_dt, sindy_layer, mass, drag_coeff, G,\n",
    "                                          radius, box_size, bounce_coeff, alpha, beta, gamma, delta, epsilon, zeta)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                batch_accuracy = 1 - (torch.norm(u_pred - targets) / torch.norm(targets))\n",
    "                val_accuracy += batch_accuracy.item()\n",
    "\n",
    "        # Print losses and accuracies for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "              f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy / len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy / len(val_loader):.4f}\")\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, test_loader, mass, drag_coeff, G, radius, box_size, bounce_coeff,\n",
    "                           alpha, beta, gamma, delta, epsilon, zeta):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set and compute accuracy.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PINNModel instance.\n",
    "        test_loader: DataLoader for test data.\n",
    "        mass, drag_coeff, G: Physical parameters.\n",
    "        radius, box_size, bounce_coeff: Collision parameters.\n",
    "        alpha, beta, gamma, delta, epsilon, zeta: Weights for loss components.\n",
    "\n",
    "    Returns:\n",
    "        test_loss: Average test loss.\n",
    "        test_accuracy: Average test accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            # Forward pass\n",
    "            u_pred, z, dz_dt = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = full_combined_loss(u_pred, targets, z, dz_dt, model.sindy_layer, mass, drag_coeff, G,\n",
    "                                      radius, box_size, bounce_coeff, alpha, beta, gamma, delta, epsilon, zeta)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            batch_accuracy = 1 - (torch.norm(u_pred - targets) / torch.norm(targets))\n",
    "            test_accuracy += batch_accuracy.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    avg_test_accuracy = test_accuracy / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_accuracy:.4f}\")\n",
    "    return avg_test_loss, avg_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = PINNModel(input_size=INPUT_SIZE, latent_dim=LATENT_DIM, hidden_dim=HIDDEN_DIM, library_dim=LIBRARY_DIM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model with validation\n",
    "train_model_with_validation(model, model.sindy_layer, train_loader, val_loader, optimizer, NUM_EPOCHS, MASS,\n",
    "                            DRAG_COEFF, G, RADIUS, BOX_SIZE, BOUNCE_COEFF, ALPHA, BETA, GAMMA, DELTA, EPSILON, ZETA)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = evaluate_model_on_test(model, test_loader, MASS, DRAG_COEFF, G, RADIUS, BOX_SIZE, BOUNCE_COEFF,\n",
    "                                                  ALPHA, BETA, GAMMA, DELTA, EPSILON, ZETA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
