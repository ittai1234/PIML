{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.integrate import solve_ivp\n",
    "import pickle\n",
    "import pysindy as ps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "from torchdiffeq import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.fft import fft, ifft\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import MultiTaskLassoCV, LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from typing import Tuple, Any, Iterable, Dict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lorenz_data(timesteps, dt, initial_conditions, sigma=10, beta=8/3, rho=28):\n",
    "    \"\"\"\n",
    "    Generate Lorenz system data.\n",
    "\n",
    "    :param timesteps: Number of timesteps to simulate.\n",
    "    :param dt: Time step size.\n",
    "    :param initial_conditions: Initial conditions (x0, y0, z0).\n",
    "    :param sigma: Lorenz system parameter.\n",
    "    :param beta: Lorenz system parameter.\n",
    "    :param rho: Lorenz system parameter.\n",
    "    :return: Numpy array of shape (timesteps, 3).\n",
    "    \"\"\"\n",
    "    def lorenz(t, state):\n",
    "        x, y, z = state\n",
    "        dxdt = sigma * (y - x)\n",
    "        dydt = x * (rho - z) - y\n",
    "        dzdt = x * y - beta * z\n",
    "        return [dxdt, dydt, dzdt]\n",
    "\n",
    "    t_eval = np.linspace(0, dt * timesteps, timesteps)\n",
    "    sol = solve_ivp(lorenz, [0, dt * timesteps], initial_conditions, t_eval=t_eval, method='RK45')\n",
    "    return sol.y.T  # Transpose to shape (timesteps, 3)\n",
    "\n",
    "# Generate dataset parameters\n",
    "timesteps = 7500  # Extended sequence length for better visualization\n",
    "n_samples = 20000  # Total number of sequences\n",
    "dt = 0.01  # Time step size\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "initial_conditions_list = np.random.uniform(-10, 10, size=(n_samples, 3))  # Initial conditions closer to attractor\n",
    "\n",
    "data = np.array([generate_lorenz_data(timesteps, dt, ic) for ic in initial_conditions_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_data = data[:16000]\n",
    "val_data = data[16000:18000]\n",
    "test_data = data[18000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExamples from the dataset:\")\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "for i in range(10):\n",
    "    example = data[i]\n",
    "    print(example)\n",
    "    ax = fig.add_subplot(2, 5, i + 1, projection='3d')\n",
    "    ax.plot(example[:, 0], example[:, 1], example[:, 2])\n",
    "    ax.set_title(f\"Example {i + 1}\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def prepare_dataloader(data, batch_size):\n",
    "    inputs = torch.tensor(data[:, :-1, :], dtype=torch.float32)  # All but last timestep as input\n",
    "    targets = torch.tensor(data[:, -1, :], dtype=torch.float32)  # Last timestep as target\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = prepare_dataloader(train_data, batch_size)\n",
    "val_loader = prepare_dataloader(val_data, batch_size)\n",
    "test_loader = prepare_dataloader(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderRNN(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, seq_len, rnn_hidden_dim, rnn_layers, output_dim):\n",
    "        super(AutoencoderRNN, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Encoder: Compress the input features\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # RNN: LSTM for sequence modeling\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=latent_dim,\n",
    "            hidden_size=rnn_hidden_dim,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Decoder: Map the latent representation to the output\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_dim = x.size()\n",
    "\n",
    "        # Apply encoder to each time step\n",
    "        x = x.view(-1, input_dim)  # Flatten for encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(batch_size, seq_len, -1)  # Reshape for RNN\n",
    "\n",
    "        # Pass through RNN\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "\n",
    "        # Use the last RNN output for prediction\n",
    "        final_out = rnn_out[:, -1, :]  # (batch, rnn_hidden_dim)\n",
    "\n",
    "        # Decode the RNN output\n",
    "        output = self.decoder(final_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Training Loop with Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "def train_and_evaluate_autoencoder_rnn(model, train_loader, val_loader, epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # L2 Regularization\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Learning rate scheduler\n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'input_dim': [3],  # Number of input features (e.g., x, y, z)\n",
    "    'latent_dim': [16, 32],  # Latent space dimensionality\n",
    "    'seq_len': [100],  # Sequence length\n",
    "    'rnn_hidden_dim': [64, 128],  # RNN hidden size\n",
    "    'rnn_layers': [1, 2],  # Number of RNN layers\n",
    "    'output_dim': [3],  # Output size (e.g., x, y, z)\n",
    "    'learning_rate': [0.001, 0.0005],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing params: {params}\")\n",
    "    model = AutoencoderRNN(\n",
    "        input_dim=params['input_dim'],\n",
    "        latent_dim=params['latent_dim'],\n",
    "        seq_len=params['seq_len'],\n",
    "        rnn_hidden_dim=params['rnn_hidden_dim'],\n",
    "        rnn_layers=params['rnn_layers'],\n",
    "        output_dim=params['output_dim']\n",
    "    )\n",
    "    val_loss = train_and_evaluate_autoencoder_rnn(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=params['epochs'], learning_rate=params['learning_rate']\n",
    "    )\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Params: {best_params}, Best Validation Loss: {best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_compare(best_model, test_loader, dt):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "    all_model_predictions = []\n",
    "    all_ground_truth = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.cpu().numpy()\n",
    "            outputs = best_model(inputs).cpu().numpy()\n",
    "            \n",
    "            # Collect predictions and ground truth\n",
    "            all_model_predictions.append(outputs)\n",
    "            all_ground_truth.append(targets)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    all_model_predictions = np.concatenate(all_model_predictions, axis=0)\n",
    "    all_ground_truth = np.concatenate(all_ground_truth, axis=0)\n",
    "    all_sindy_predictions = np.concatenate(all_sindy_predictions, axis=0)\n",
    "\n",
    "    # Calculate errors\n",
    "    model_mae = mean_absolute_error(all_ground_truth, all_model_predictions)\n",
    "    sindy_mae = mean_absolute_error(all_ground_truth, all_sindy_predictions)\n",
    "    \n",
    "    print(f\"Model Mean Absolute Error: {model_mae:.4f}\")\n",
    "    print(f\"SINDy Mean Absolute Error: {sindy_mae:.4f}\")\n",
    "\n",
    "    # Visualize comparisons for a few samples\n",
    "    num_samples_to_plot = 5\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_samples_to_plot):\n",
    "        ax = fig.add_subplot(1, num_samples_to_plot, i + 1, projection='3d')\n",
    "\n",
    "        # Ground truth\n",
    "        ax.plot(\n",
    "            all_ground_truth[i, :, 0],\n",
    "            all_ground_truth[i, :, 1],\n",
    "            all_ground_truth[i, :, 2],\n",
    "            'g',\n",
    "            label=\"Ground Truth\"\n",
    "        )\n",
    "        # Model predictions\n",
    "        ax.plot(\n",
    "            all_model_predictions[i, :, 0],\n",
    "            all_model_predictions[i, :, 1],\n",
    "            all_model_predictions[i, :, 2],\n",
    "            'b',\n",
    "            label=\"Model Predictions\"\n",
    "        )\n",
    "        ax.set_title(f\"Sample {i + 1}\")\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.set_zlabel(\"Z\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Prepare the dataloader for the test dataset\n",
    "test_loader = prepare_dataloader(test_data, batch_size)\n",
    "\n",
    "# Compare the predictions of the best model and SINDy model\n",
    "test_and_compare(best_model, best_sindy_model, test_loader, dt=0.01)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
