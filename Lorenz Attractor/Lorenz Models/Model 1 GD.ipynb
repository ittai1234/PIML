{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.integrate import solve_ivp\n",
    "import pickle\n",
    "import pysindy as ps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import os\n",
    "from torchdiffeq import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.fft import fft, ifft\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import MultiTaskLassoCV, LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from typing import Tuple, Any, Iterable, Dict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lorenz_data(timesteps, dt, initial_conditions, sigma=10, beta=8/3, rho=28):\n",
    "    \"\"\"\n",
    "    Generate Lorenz system data.\n",
    "\n",
    "    :param timesteps: Number of timesteps to simulate.\n",
    "    :param dt: Time step size.\n",
    "    :param initial_conditions: Initial conditions (x0, y0, z0).\n",
    "    :param sigma: Lorenz system parameter.\n",
    "    :param beta: Lorenz system parameter.\n",
    "    :param rho: Lorenz system parameter.\n",
    "    :return: Numpy array of shape (timesteps, 3).\n",
    "    \"\"\"\n",
    "    def lorenz(t, state):\n",
    "        x, y, z = state\n",
    "        dxdt = sigma * (y - x)\n",
    "        dydt = x * (rho - z) - y\n",
    "        dzdt = x * y - beta * z\n",
    "        return [dxdt, dydt, dzdt]\n",
    "\n",
    "    t_eval = np.linspace(0, dt * timesteps, timesteps)\n",
    "    sol = solve_ivp(lorenz, [0, dt * timesteps], initial_conditions, t_eval=t_eval, method='RK45')\n",
    "    return sol.y.T  # Transpose to shape (timesteps, 3)\n",
    "\n",
    "# Generate dataset parameters\n",
    "timesteps = 7500  # Extended sequence length for better visualization\n",
    "n_samples = 20000  # Total number of sequences\n",
    "dt = 0.01  # Time step size\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "initial_conditions_list = np.random.uniform(-10, 10, size=(n_samples, 3))  # Initial conditions closer to attractor\n",
    "\n",
    "data = np.array([generate_lorenz_data(timesteps, dt, ic) for ic in initial_conditions_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_data = data[:16000]\n",
    "val_data = data[16000:18000]\n",
    "test_data = data[18000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExamples from the dataset:\")\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "for i in range(10):\n",
    "    example = data[i]\n",
    "    print(example)\n",
    "    ax = fig.add_subplot(2, 5, i + 1, projection='3d')\n",
    "    ax.plot(example[:, 0], example[:, 1], example[:, 2])\n",
    "    ax.set_title(f\"Example {i + 1}\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def prepare_dataloader(data, batch_size):\n",
    "    inputs = torch.tensor(data[:, :-1, :], dtype=torch.float32)  # All but last timestep as input\n",
    "    targets = torch.tensor(data[:, -1, :], dtype=torch.float32)  # Last timestep as target\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = prepare_dataloader(train_data, batch_size)\n",
    "val_loader = prepare_dataloader(val_data, batch_size)\n",
    "test_loader = prepare_dataloader(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedConvFNO(nn.Module):\n",
    "    def __init__(self, modes, width, conv_filters):\n",
    "        super(AdvancedConvFNO, self).__init__()\n",
    "        self.modes = modes\n",
    "        self.width = width\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv1d(3, conv_filters, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(conv_filters, conv_filters, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(conv_filters, self.width, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fourier Neural Operator\n",
    "        self.fc0 = nn.Linear(self.width, self.width)\n",
    "        self.fc1 = nn.Linear(self.width, self.width)\n",
    "        self.fc2 = nn.Linear(self.width, 3)  # Output (x, y, z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize, seq_len, channels = x.shape\n",
    "\n",
    "        # Apply convolutions\n",
    "        x = x.permute(0, 2, 1)  # Change to (batch, channels, seq_len)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch, seq_len, channels)\n",
    "\n",
    "        # Fourier Neural Operator\n",
    "        x = self.fc0(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x[:, -1, :])  # Use the last timestep\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SINDy for Post-Training Analysis\n",
    "def apply_sindy(train_data_flat, test_data_flat, dt):\n",
    "    x_train = train_data_flat[:-1]\n",
    "    dx_train = (train_data_flat[1:] - train_data_flat[:-1]) / dt\n",
    "    x_test = test_data_flat[:-1]\n",
    "    dx_test = (test_data_flat[1:] - test_data_flat[:-1]) / dt\n",
    "\n",
    "    # Define the SINDy model\n",
    "    feature_library = ps.PolynomialLibrary(degree=3)\n",
    "    optimizer = ps.STLSQ(threshold=0.1)\n",
    "    sindy_model = ps.SINDy(feature_library=feature_library, optimizer=optimizer)\n",
    "\n",
    "    # Fit and evaluate\n",
    "    sindy_model.fit(x_train, t=dt, x_dot=dx_train)\n",
    "    print(\"\\nSINDy Discovered Equations:\")\n",
    "    sindy_model.print()\n",
    "    print(f\"SINDy Test Score: {sindy_model.score(x_test, t=dt, x_dot=dx_test):.4f}\")\n",
    "\n",
    "# Accuracy Calculation\n",
    "def calculate_accuracy(outputs, targets, tolerance=0.1):\n",
    "    correct = torch.sum(torch.abs(outputs - targets) < tolerance).item()\n",
    "    total = targets.numel()\n",
    "    return correct / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Accuracy\n",
    "def train_and_evaluate(model, train_loader, val_loader, epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += torch.sum(torch.abs(outputs - targets) < 0.1).item()\n",
    "            train_total += targets.numel()\n",
    "\n",
    "        train_accuracy = train_correct / train_total * 100\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += torch.sum(torch.abs(outputs - targets) < 0.1).item()\n",
    "                val_total += targets.numel()\n",
    "\n",
    "        val_accuracy = val_correct / val_total * 100\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'modes': [16, 32],\n",
    "    'width': [64, 128],\n",
    "    'conv_filters': [16, 32],\n",
    "    'learning_rate': [0.001, 0.0005],\n",
    "    'epochs': [50]\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Grid Search for Hyperparameters\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing params: {params}\")\n",
    "    model = AdvancedConvFNO(modes=params['modes'], width=params['width'], conv_filters=params['conv_filters'])\n",
    "    val_loss = train_and_evaluate(model, train_loader, val_loader,\n",
    "                                  epochs=params['epochs'],\n",
    "                                  learning_rate=params['learning_rate'])\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Params: {best_params}, Best Validation Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_sindy(train_data_flat, test_data_flat, dt):\n",
    "    x_train = train_data_flat[:-1]\n",
    "    dx_train = (train_data_flat[1:] - train_data_flat[:-1]) / dt\n",
    "    x_test = test_data_flat[:-1]\n",
    "    dx_test = (test_data_flat[1:] - test_data_flat[:-1]) / dt\n",
    "\n",
    "    # Define multiple libraries to test\n",
    "    polynomial_degrees = [2, 3, 4, 5]  # Include higher polynomial degrees\n",
    "    thresholds = np.linspace(0.01, 0.2, 10)  # Finer threshold range\n",
    "    n_frequencies = [3, 5, 7]  # Test different numbers of Fourier frequencies\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -float(\"inf\")\n",
    "    best_params = {}\n",
    "\n",
    "    for degree in polynomial_degrees:\n",
    "        for threshold in thresholds:\n",
    "            for freq in n_frequencies:\n",
    "                print(f\"Testing SINDy with Polynomial Degree: {degree}, Threshold: {threshold}, Frequencies: {freq}\")\n",
    "\n",
    "                # Define the SINDy model\n",
    "                feature_library = ps.PolynomialLibrary(degree=degree) + ps.FourierLibrary(n_frequencies=freq)\n",
    "                optimizer = ps.STLSQ(threshold=threshold)\n",
    "                sindy_model = ps.SINDy(\n",
    "                    optimizer=optimizer,\n",
    "                    feature_library=feature_library,\n",
    "                    differentiation_method=ps.SmoothedFiniteDifference()  # Use smoother differentiation\n",
    "                )\n",
    "\n",
    "                # Fit the model\n",
    "                sindy_model.fit(x_train, t=dt, x_dot=dx_train)\n",
    "                score = sindy_model.score(x_test, t=dt, x_dot=dx_test)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_model = sindy_model\n",
    "                    best_score = score\n",
    "                    best_params = {\"degree\": degree, \"threshold\": threshold, \"frequencies\": freq}\n",
    "\n",
    "    # Print the best model's equations\n",
    "    print(\"\\nBest SINDy Model:\")\n",
    "    best_model.print()\n",
    "    print(f\"Best SINDy Test Score: {best_score:.4f}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_score = best_model.score(x_test, t=dt, x_dot=dx_test)\n",
    "    print(f\"Final SINDy Test Score: {test_score:.4f}\")\n",
    "\n",
    "    return best_model, best_params, test_score\n",
    "\n",
    "# Apply the updated SINDy\n",
    "best_sindy_model, best_params, test_score = advanced_sindy(train_data_flat, test_data_flat, dt=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_compare(best_model, best_sindy_model, test_loader, dt):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "    all_model_predictions = []\n",
    "    all_ground_truth = []\n",
    "    all_sindy_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.cpu().numpy()\n",
    "            outputs = best_model(inputs).cpu().numpy()\n",
    "            \n",
    "            # Collect predictions and ground truth\n",
    "            all_model_predictions.append(outputs)\n",
    "            all_ground_truth.append(targets)\n",
    "            \n",
    "            # SINDy predictions\n",
    "            test_inputs_flat = inputs.cpu().numpy().reshape(-1, inputs.shape[-1])\n",
    "            sindy_predictions = best_sindy_model.predict(test_inputs_flat)\n",
    "            all_sindy_predictions.append(sindy_predictions[-targets.shape[0]:])  # Use last part of the prediction\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    all_model_predictions = np.concatenate(all_model_predictions, axis=0)\n",
    "    all_ground_truth = np.concatenate(all_ground_truth, axis=0)\n",
    "    all_sindy_predictions = np.concatenate(all_sindy_predictions, axis=0)\n",
    "\n",
    "    # Calculate errors\n",
    "    model_mae = mean_absolute_error(all_ground_truth, all_model_predictions)\n",
    "    sindy_mae = mean_absolute_error(all_ground_truth, all_sindy_predictions)\n",
    "    \n",
    "    print(f\"Model Mean Absolute Error: {model_mae:.4f}\")\n",
    "    print(f\"SINDy Mean Absolute Error: {sindy_mae:.4f}\")\n",
    "\n",
    "    # Visualize comparisons for a few samples\n",
    "    num_samples_to_plot = 5\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_samples_to_plot):\n",
    "        ax = fig.add_subplot(1, num_samples_to_plot, i + 1, projection='3d')\n",
    "\n",
    "        # Ground truth\n",
    "        ax.plot(\n",
    "            all_ground_truth[i, :, 0],\n",
    "            all_ground_truth[i, :, 1],\n",
    "            all_ground_truth[i, :, 2],\n",
    "            'g',\n",
    "            label=\"Ground Truth\"\n",
    "        )\n",
    "        # Model predictions\n",
    "        ax.plot(\n",
    "            all_model_predictions[i, :, 0],\n",
    "            all_model_predictions[i, :, 1],\n",
    "            all_model_predictions[i, :, 2],\n",
    "            'b',\n",
    "            label=\"Model Predictions\"\n",
    "        )\n",
    "        # SINDy predictions\n",
    "        ax.plot(\n",
    "            all_sindy_predictions[i, :, 0],\n",
    "            all_sindy_predictions[i, :, 1],\n",
    "            all_sindy_predictions[i, :, 2],\n",
    "            'r',\n",
    "            label=\"SINDy Predictions\"\n",
    "        )\n",
    "        ax.set_title(f\"Sample {i + 1}\")\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.set_zlabel(\"Z\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Prepare the dataloader for the test dataset\n",
    "test_loader = prepare_dataloader(test_data, batch_size)\n",
    "\n",
    "# Compare the predictions of the best model and SINDy model\n",
    "test_and_compare(best_model, best_sindy_model, test_loader, dt=0.01)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
